natural language understanding andnatural language generation twofundamental related task buildingtask-oriented dialogue system oppositeobjectives tackle transformationfrom natural language formal representation whereas reverse keyto success either task parallel trainingdata expensive obtain largescale work propose generative model couple nlgthrough share latent variable approach allow explore space ofnatural language formal representation facilitate information share thelatent space eventually benefit andnlg model achieve state-of-the-art performance dialogue datasets bothflat tree-structured formal representations.we also show model trainedin semi-supervised fashion utilise unlabelled data boost performance.1 introductionnatural language understanding naturallanguage generation fundamentaltasks build task-oriented dialogue systems.in modern dialogue system module firstconverts user utterance provide automaticspeech recognition model formal representation representation consume adownstream dialogue state tracker update belief state represent aggregated user goal.based current belief state policy networkdecides formal representation system response finally module togenerate system response young observe opposite goal natural language∗work author intern apple.figure generation inference process ourmodel achieve andy denotes utterance formal representation respectively represent share latent variable andy.to formal representation generatesutterances semantics research literature well-studied separateproblems state-of-the-art system tackle thetask classification zhang wang oras structure prediction generation damonteet depend formal representation flat slot-value pair hendersonet first-order logical form zettlemoyerand collins structure query al.,2018 pasupat hand approach vary pipelined approachsubsuming content planning surface realisation stent recent end-to-end sequence generation dušek al.,2020 duality nlghas less explore fact task betreated translation problem converts1796natural language formal language nlgdoes reverse task require substantialamount utterance representation pair tosucceed data costly collect tothe complexity annotation involve althoughunannotated data either natural language formal representation easily obtain isless clear leverage twolanguages stand different space.in paper propose generative modelfor joint natural language understanding andgeneration couple nlgwith latent variable represent share intentbetween natural language formal representation learn association betweentwo discrete space continuous latentvariable facilitate information share task moreover trainedin semi-supervised fashion enable toexplore space natural language formal representation unlabelled data accessible examine model dialoguedatasets different formal representation thee2e dataset novikova semantics represent collection slot-valuepairs recent weather dataset balakrishnan formal representationsare tree-structured experimental result show thatour model improve standalone nlu/nlgmodels exist method task andthe performance boost utilisingunlabelled data.2 modelour assumption exist abstractlatent variable underlie pair utterance xand formal representation generativemodel abstract intent guide standard conditional generation either figure1a meanwhile infer either utterance formal representation figure mean perform require inferthe formal representationy generate condition figure vice-versa figure inthe follow explain model detail start nlg.2.1 nlgas mention task requiresus infer generate usingboth choose posterior distributionq gaussian task infer canthen recast compute mean standarddeviation gaussian distribution annlg encoder bi-directionallstm hochreiter schmidhuber encode formal representation linearisedand represent sequence symbol encode obtain list hidden vector witheach represent concatenation forward andbackward lstm state hidden vector arethen average-pooled pass feedforward neural network compute mean zand standard deviation vector posteriorq bi-lstm pooling wµh̄ bµσy wσh̄ represent neural network weightsand bias latent vector sample approximate posterior there-parameterisation trick kingma welling final step generate natural language xbased latent variable formal representationy lstm decoder rely andy attention mechanism bahdanau time step decoder computes lstm gxi−1 xi−1 attention softmax ci⊕gxi denote concatenation xi−1 wordvector input token corresponding decoder hidden state output tokendistribution time step i.2.2 nlunlu perform reverse procedure nlg.first encoder infers latent variable zfrom utterance encoder bi-directionallstm convert utterance list hiddenstates hidden state pool passedthrough feed-forward neural network computethe mean standard deviation theposterior procedure follow equation1 nlg.1797however note subtle difference betweennatural language formal language theformer ambiguous later preciselydefined make many-to-one mappingproblem one-to-many better reflectthe fact output require less variance decode choose latent vector innlu mean vector instead sample like equation latent vector obtain formalrepresentation predict decoder since space dependson formal language construct consider twocommon scenario dialogue system firstscenario represent slot-value pair e.g. food type=british area=north restaurantsearch domain mrkšić decoderhere consist several classifier slot predict corresponding values.2 classifier model feed-forward neuralnetwork take input softmax predicted value distribution ofslot s.in second scenario tree-structuredformal representation banarescu wethen generate linearised token sequence usingan lstm decoder rely viathe standard attention mechanism bahdanau al.,2014 decoding procedure follow exactlyequation model summaryone flexibility model come thefact infer share latentvariable either inferred zcan generation nextsection show share latent variableenables model explore unlabelled andy align learned meaning inside thelatent space.3 optimisationwe describe optimise witha pair §3.1 also unpaired or1note still necessary compute standard deviation since term need optimisation.see detail section slot correspond value plus specialone not_mention.y §3.2 specifically discuss prior choiceof objectives §3.3 combined objectivecan thus derive semi-supervised learning practical scenario small oflabelled data abundant unlabelled §3.4 optimising given pair utterance formal representation objective maximise loglikelihood joint probability log∫zp optimisation task directly tractable sinceit require marginalise latent variablez however solve follow thestandard practice neural variational inference kingma welling objective basedon variational bound derive aslx first term right side nlumodel second term reconstruction last term denote kullback−leibler divergence approximate posterior prior defer discussion ofprior section detailed derivation appendix.the symmetry utterance semanticsoffers alternative infer posteriorthrough approximation analogouslywe derive variational optimisation objective first term model secondterm reconstruction last termdenotes divergence.it observe model posterior inference path either alsotwo generation path path optimised.3.2 optimising additionally access unlabelledutterance formal representation optimisation objective marginal likelihoodp log∫y∫zp unobserved case.we develop objective base variational bound marginal first term auto-encoder reconstruction cascaded nlu-nlg path second term divergence regularizesthe approximate posterior distribution detailedderivations find appendix.when compute reconstruction term require first modelto obtain prediction runthrough reconstruct full information flow connectionscan draw recent work backtranslation augment training data machinetranslation sennrich back-translation presence latent variable model require sample alongthe nlu-nlg path introduced stochasticityallows model explore large area datamanifold.the describe objective wehave unlabelled derive similar objective leverage unlabelled first term auto-encoder reconstruction cascaded nlg-nlu path fullinformation flow y→z→x→z→y choice priorthe objective describe require usto match approximate posterior either prior reflect belief acommon choice research literatureis normal distribution kingma welling,2013 however note even wematch prior itdoes guarantee infer posteriorsare close desired property ofthe share latent space.to well address property propose anovel prior choice posterior inferred3this information flow require sample andy reconstruct since discrete sequence usereinforce williams pass gradient nlgto cascaded nlu-nlg path.from i.e. choose parameteriseddistribution prior belief similarly posterior infer i.e. freedom define tobe approach directly pull andq closer ensure shared latent space.finally note straightforward compute parallelx however access unlabelled data describe section canonly pseudo pair generate byour model match aninferred posterior pre-defined prior reflectingour belief share latent space.3.4 training summaryin general subsume following three train scenario experiment with.when fully label jugjointly optimise supervisedfashion objective follow lbasic denote labelled examples.additionally fully supervised setting jugcan train optimise andauto-encoding path correspond following objective lmarginal lbasic+∑ lx+ly furthermore additional unlabelled optimise semi-supervised jugobjective follow lsemi lbasic +∑x∼xlx +∑y∼yly denote utterance denote formal representations.4 experimentswe experiment dialogue datasets different formal representation test generalityof model first dataset novikovaet contain utterance annotatedwith flat slot-value pair semantic representation second dataset recent weatherdataset balakrishnan utterance semantics represent tree structure examples datasets provide intables language sousa offer british food price range.it family friendly star rating.you find near sunshine vegetarian cafe semantic representationrestaurant_name=sousa food=english price_range=cheap customer_rating=average family_friendly=yes near=sunshine vegetarian cafetable example dataset.natural language original __dg_yes__ __dg_inform__ __arg_date_time__ __arg_colloquial__ today forecast __arg_cloud_coverage__ mostly cloudy __arg_condition__ light rain shower natural language process remove tree annotation today forecast mostly cloudy light rain shower semantic representation __dg_yes__ __arg_task__ get_weather_attribute __dg_inform__ __arg_task__ get_forecast __arg_condition__ light rain shower __arg_cloud_coverage__ mostly cloudy __arg_date_time__ __arg_colloquial__ today table example weather dataset naturallanguage original dataset first train fair comparison exist methods.the process utterance second oursemi-supervised setting.4.1 training scenarioswe primarily evaluate model rawsplits original datasets enable usto fairly compare fully-supervised exist work nlg.4 statistics thetwo datasets find table addition experiment evaluatesemi-supervised vary amount labelled training data rest unlabelled note original test design purpose unseenslot-values test make difficult dušeket remove distribution biasby randomly re-splitting dataset thecontrary utterance weather dataset containsextra tree-structure annotation make thenlu task problem therefore removethese annotation make realistic asshown second table describe section optimiseour propose model various weinvestigate following approach jugbasic model jointly optimise nlu4following balakrishnan evaluation codehttps //github.com/tuetschek/e2e-metrics provide e2eorganizers calculate bleu nlg.dataset train valid teste2e number example datasetse2e f1dual supervise learning bleutgen dušek jurcicek juraska supervise learning bleus2s-constr balakrishnan comparison previous system twodatasets note previous system trainedfor weather dataset.and objective equation thisuses label data only.jugmarginal jointly optimise andauto-encoders label data equation12.jugsemi jointly optimise withlabelled data auto-encoders unlabelleddata equation baseline systemswe compare propose model exist method show table designedbaselines follow decoupled model aretrained separately supervised learning bothof individual model encoderdecoder structure however main difference shared latent variablebetween individual models.augmentation pre-train decoupledmodels generate pseudo label unlabelled corpus setup similar backtranslation sennrich pseudo dataand label data together fine-tunethe pre-trained models.among system experiment number unit lstm encoder/decoder dimension latent space is150 optimiser adam kingma learn rate batch size model fully train the1800model data decoupled augmentation∗ jugbasic jugmarginal jug∗semi table result dataset joint accuracy score bracket report varyingpercentage labelled training data models unlabelled data mark data decoupled augmentation∗ jugbasic jugmarginal jug∗semi table result dataset bleu semantic accuracy bracket report varyingpercentage labelled training data models unlabelled data mark data decoupled jugbasic table result exact match accuracy weather dataset.best model pick average andnlg result validation training.4.3 main resultswe start compare jugbasic performancewith exist work follow original split ofthe datasets result show table one2e dataset follow previous work f1of slot-values measurement andbleu-4 weather dataset isonly publish result observedthat jugbasic model outperform previousstate-of-the-art system e2edataset also weather dataset.the result prove effectiveness introducingthe share latent variable jointly train nluand study impact theshared section also evaluate three training scenario ofjug semi-supervised setting differentproportion labelled unlabelled data theresults present table wecomputed score joint accuracy mrkšićmodel data decoupled jugbasic table result bleu weather dataset.et slot-values solid nlumeasurement joint accuracy define proportion test example whose slot-value pair areall correctly predict bleu-4 andsemantic accuracy compute semantic accuracy measure proportion correctly generatedslot value produced utterance theresults observe decoupled improve technique generate pseudo data augmentation form strong baseline however model variant perform good baseline whenusing label data model jugmarginalcan surpass decoupled across four measurement gain mainly come factthat model auto-encoding objective tohelp learn share semantic space compared toaugmentation jugmarginal also builtin mechanism bootstrap pseudo data flyof training section extraunlabelled data model jugsemi furtherperformance boost outperforms baselinesby significant margin.with vary proportion unlabelled data in1801figure visualisation latent variable given pairof sample posterior denote blue orange respectively.the training unlabelled data helpful almost case moreover performancegain significant label datais less indicate propose model especially helpful resource setup thereis limited amount labelled training examplesbut available unlabelled ones.the result weather dataset present intable dataset likea semantic parsing task berant andwe exact match accuracy measurement.meanwhile measure bleu result reveal similar trend thegenerated example find appendix.4.4 analysisin section analyse impact ofthe share latent variable also impact ofutilising unlabelled data.4.4.1 visualisation latent spaceas mention section latent variable zcan sample either posterior approximation inspect latent spacein figure find well model learnsintent sharing plot dataset space t-sne projection maatenand hinton observe interesting property first data point value sampledfrom close other.this reveal meaning tiedin latent space second exist distinctclusters space inspect actual example within cluster wefound cluster represent similar meaning composition instance cluster cenmodel nlgjugbasic feed random comparative study evaluate contribution learned latent variable nlu/nlgdecoding models train whole weatherdataset.method nlgmi wrdecoupled error analysis dataset numbers ofmissing redundant wrong prediction slot-value pair report numbersof missing wrong generated slot value list fornlg lower number indicate good result bothmodels train training data.tered contain name foodtype price rating area near cluster center contain name eattype foodtype price indicate theshared latent serf conclusive global featurerepresentations nlg.4.4.2 impact latent variableone novelty model introduction ofshared latent variable natural language andformal representation common problem inneural variational model couple apowerful autogressive decoder decoder tend tolearn ignore solely rely generatethe data bowman chen goyal order examine whatextent model actually rely sharedvariable seek empirical answer compare jugbasic modelwith model variant random valueof sample normal distribution test table observe thatthere exist large performance drop assign random value suggest jugindeed rely greatly share variable produce good-quality y.we analyse various source errorsto understand case help improve.on dataset wrong prediction comesfrom either predict not_mention label forcertain slot ground truth semantics predictingarbitrary value slot present groundtruth semantics predict wrong value com1802e2e weathermethod nlgjugbasic comparison source unlabelled datafor semi-supervised learning utterance semantic representation model train train data.paring ground truth three type error refer missing redundant wrong table semantic error beeither miss generate wrong slot value inthe give semantics modelmakes mistake error sourcescomparing baseline decoupled believe cluster property learnedin latent space provide well feature representation global scale eventually benefit nluand nlg.4.4.3 impact unlabelled data sourcein section find performance ofour model enhance leveragingunlabelled data unlabelled utterance unlabelled semantic representationstogether unclear contribute performance gain answer question startwith jugbasic model experiment withadding unlabelled data unlabelled utterance semantic representation show table addingany uni-sourced unlabelled data modelis able improve certain extent however performance maximise datasources utilised strengthen argumentthat model leverage bi-sourced unlabelleddata effectively latent space share toimprove time.5 related worknatural language understanding refer tothe general task natural language toformal representation line research thedialogue community detect slot-valuepairs express user utterance classificationproblem henderson mrkšić vodolán anotherline work focus convert single-turn userutterances structured meaning representation semantic parsing task zettlemoyer andcollins liang dong lapata damonte comparison natural language generation scoped task generate naturalutterances formal representation thisis traditionally handle pipelined approach reiter dale content plan andsurface realisation walker stent al.,2004 recently formulate asan end-to-end learning problem text stringsare generate recurrent neural network condition formal representation al.,2015 dušek jurcicek dušek balakrishnan tseng recent work doesnlu jointly andcao explore duality semantic parsing former optimises twosequence-to-sequence model dual information maximisation latter introduces adual learning framework semantic parsing suet propose learning framework fordual supervise learning whereboth model optimise towardsa joint objective method bring benefit withannotated data supervised learning doesnot allow semi-supervised learning unlabelleddata contrast work propose generative model couple witha share latent variable focus exploringa couple representation space naturallanguage correspond semantic annotations.as prove experiment information sharinghelps model leverage unlabelled data forsemi-supervised learning eventually benefit nlg.6 conclusionwe propose generative model couplesnatural language formal representation viaa share latent variable since space iscoupled gain luxury exploit unpaired data source transfer acquire knowledge share meaning space eventuallybenefits especially lowresource scenario propose model alsosuitable translation task twomodalities.as final remark natural language rich andmore informal need handle ambiguous1803or erroneous user input however formal representation utilise system moreprecisely-defined future refine ourgenerative model good emphasise difference tasks.acknowledgmentsbo-hsiang tseng support cambridge trustand ministry education taiwan workhas perform resource provide bythe cambridge tier-2 system operate university cambridge research computing service http //www.hpc.cam.ac.uk fund epsrctier-2 capital grant ep/p020259/1
paper introduce novel methodology efficiently construct corpus question answering structure data introduce intermediate representationthat base logical query plan adatabase call operation trees thisrepresentation allow invert annotation process without lose flexibility thetypes query generate furthermore allow fine-grained alignment ofquery token operations.in method randomly generate otsfrom context-free grammar afterwards annotator write appropriate naturallanguage question represent theot finally annotator assign tokensto operation apply methodto create corpus otta operation treesand token assignment large semantic parsing corpus evaluate natural language interface database compare otta tospider lc-quad show ourmethodology triple annotationspeed maintain complexity thequeries finally train state-of-the-art semantic parsing model data showthat corpus challenging dataset andthat token alignment leverage toincrease performance significantly.1 introductionquestion answering structure data also call natural language interfaces todatabases nli2db text-to-sql taskin natural language processing semanticweb usually approach naturallanguage question question executablequeries formal representation logicalforms sparql sql.the state-of-the-art problem machinelearning technique learn mapping unfortunately construction label corpus trainand evaluate nli2db system time- costintensive slow progress thisarea particular usually require recruitingsql sparql expert write query natural language question instance spider yuet author recruit student writesql query work person-hours togenerate query correspond morethan minute question.as cost-effective alternative writingformal query manually author proposeto template generate automatically.for instance lc-quad dubey template base structure thetarget knowledge graph constructing template isalso time-consuming expressiveness theautomatically produce query limited.apart high cost generate query natural language question current datasetsdo necessarily cover whole range datapresent database spider coverageis limit creativity student inlc-quad templates.in paper propose procedure increase speed annotation process first introduce intermediate representationof structured query call operation trees figure followa context-free grammar base logicalquery plan easily sparqlor make system versatile addition show work abstracttree representation instead sequence yieldsbetter result recent work cheng show successful oftree-like abstraction intermediate representation parse text semantic representation reinforce choice operation tree themain representation language.our annotation process work follow first,898we context-free grammar sample randomots give database annotatorsin first round write corresponding question sampled second optional round annotator perform assignment token question operation theot additional annotation enrich information dataset show allow performance gain especially lowdata regimes.our approach produce datasets following advantage respect methodology previous work reduce timeneeded annotation less minute compare spider allow usto cover whole range data present thedatabase structure focus mostprominent example annotation procedureprovides alignment operation formal language word question arean additional source supervision training.we apply approach1 five datasets yield large corpus call otta2 consist of3,792 complex question plus corresponding well token assignment oneof domain besides adapt stateof-the-art system neubig workon operation tree include mechanismto profit token alignment annotation whentraining system yield well result upto point increase train aligned ots.2 related workin section first review related workin area natural language interfaces todatabases nli2db afterwards focus onthe data resource currently available toevaluate systems.natural language interfaces databases.there vast amount literature nli2db.a recent survey method technology isprovided affolter early systemsuse keyword-based approach inverted index query database simitsis blunschi bast haussmann approach able handle more1the annotation tool find http //github.zhaw.ch/semql/annotation_tool2the corpus find http //github.zhaw.ch/semql/semql-data/tree/master/annotated_tree_files/single_filescomplex question damljanovic zheng parsing-based approach usea natural language parser analyze reasonabout grammatical structure query andjagadish saha grammar-basedapproaches allow user formulate queriesaccording certain pre-defined rule thus focusprimarily increase precision answer song ferré recent system neural machine translation approachsimilar translate natural language fromfrench english iyer basik al.,2018 cheng guoet cheng resources review majordata resource recently forevaluating nli2db system resource aremainly create follow approach bothnl structure query manually create structure query automatically generate human create corresponding nlquestions.regarding fully manually created resource yuet provide spider dataset query database nlquestions annotate student somequestions manually paraphrase increasethe variability finegan-dollak release advising question university course advise query dahl al.,1994 create atis dataset user question flight-booking manually annotate withsql query modify iyer toreduce nesting zelle mooney createdgeoquery question geography annotate prolog convert sqlby popescu giordani moschitti also small datasets aboutrestaurants question tang mooney,2000 yelp website question andimdb question yaghmazadeh al.,2017 automatic step usually relyon generate structure query templatescreated expert zhong createdwikisql collection pair queriesand question make wikipedia however query relatively simple becauseeach database consist single table without foreign hence query donot contain join dubey developed899lc-quad complex questionsand sparql query dbpedia wikidata.they template generate sparql queriesfor seed entity relation lexicalizedautomatically template questionsof datasets create crowdsourcingworkers.all resource mention require alarge amount effort case annotatorsneed in-depth knowledge similarlystructured language approach simplify theprocess generate question-answering corporawhile ensure large coverage underlyingdatabase without forfeit complexity thequeries.on hand wang developeda method similar begin lexicon link natural utterance predicate thedatabase domain-specific grammarto create several canonical phrase associate withqueries finally crowdsourcing worker rewritethe canonical phrase create natural utterancesused train semantic parser similar ourapproach combine automatic method withcrowdsourcing worker however create lexicon grammar database method apply databasewithout create resources.3 operation treesin setting goal generate operationtree find correct answer givenquestion natural language binarytree closely relate logical query planin database engine compose sequence operation adatabase query language sparqlto retrieve proper result.example assume database aboutmovies want query natural language.in figure example depict forthe question star notebook inorder answer question table person andmovie select table movie filteredby movie title notebook next step thetables join bridge-table cast finally person.name column extracted.we enhance associate reasonable subset token question toeach operation tree instance token star could associate join operation tablescan person tablescan cast tablescan movie join person.id cast.person_id join movie.id cast.movie_id projection person.name select movie.title= notebook figure example operation tree forthe query star notebook thecorresponding database schema.as operation imply actor star amovie whereas token many could beassociated count operation mappingbetween tokens operation help later onto train machine learn algorithm generateots automatically natural language questionswith good quality.definition formally follow predefined context-free grammar current state theset operation include major operation fromthe relational algebra specific extension thefull grammar show figure isempty average count projection tablescan selection |distinct join union |intersection difference averageby |sumby countby table namea attributesop valuesfigure production rule contextfree grammar operation tree table namedenotes entity type database attribute denote attribute entity type value denote entry database.the non-terminal symbol denote startsymbol intermediate table result table respectively.900the represent query anyentity-relationship data paradigm instance insql databases entity type table theattributes column relationship arerepresented table well similar mapping ispossible paradigms.properties several feature question types different type question instance yes/noquestions isempty question list ofitems projection follow done questionsabout cardinality result count question aggregation types type result defined bythe entity type result instance aquestion list director thatsatisfy certain constraint e.g. director thatwere bear france case result typewould person type.constraints constraint represent filtersthat apply onto attribute entities.for instance director bear france aconstraint birth place attribute.entity types define entity type areinvolved query select entity typesare combine usually join operation forinstance figure entity type movieand person combine tablecast.aggregation types define reduction operation apply data include min/max operation attribute setoperations relation group byoperations.complexity order categorize define complexity score similar base number component inthe tree joins group operation aggregations filters query higherthe score like define fourcategories easy medium hard extra hard.4 corpus constructionthe evident construct corpus nlquestions corresponding querieswould consist main part first collect setof question create corresponding query question however thisapproach time-consuming majorissue essence question tend narrow scope i.e. necessarily cover thewhole range entity type attribute relationship present database moreover write corresponding query nlquestions require sufficient skill well mechanism verify statement actually correspond question.thus decide invert process first randomly sample abovedefined context-free grammar annotatorswrite corresponding question natural language.in last step annotator manually tokensof question operation several advantage procedure allow forcontrolling characteristic i.e. wecan control question type response type constraint entity type allowsthem create complex question bettercover variety underlying data theannotation process less time consuming theannotators build tree writequeries rather focus write question assigning token describe theprocess automatic sampling manual annotation detail.4.1 tree samplingthe tree sample procedure compose thefollowing step question type sample random orbe manually certain type desired.result type first entity type randomly sample specific attribute sampledfrom chosen entity type alternatively theresult type manually set.entity types entity type sample basedon graph structure entity relationship database schema samplefrom possible join-paths contain thetable result type also controllable specify length path wantto consider.constraints constraint filter argument sample first entity type arerandomly select constraint areto apply sample operation anda value random entity type eachattribute limit number overall constraint number maximum constraintsfor entity type.group group operation avgby sumby countby choose random a901group operation attribute need select group-attribute define whichattribute group aggregation-attribute define column apply aggregation instance could group genreand aggregate movie budget.tree structure tree structure sample asfollows first join operation apply onthe sampled entity type second operation union intersect diff insert third selection operation insert next theaggregation operation insert i.e. groupby operation finally operationsfor question type sample instance ifthe question type list entity usethe projection operation cardinalityquestion count operation.this procedure create tree make sensesemantically handle tree annotation phase describe furthermore make sure tree executable.for translate tree runthem database also omit tree return empty result lead confusionsduring evaluation different query thatboth return empty result would count asbeing equal.4.2 annotationthe annotation process i.e. write natural language question assign query token operation perform phase foreach phase develop graphical user interface facilitate annotation process moredetails appendix first phase annotator present automatically sampledas describe previous section task ofthe annotator formulate appropriate nlquestion sampled case thesampled tree contradict nonsensical constraint e.g. compute average year thesecases annotator either skip adapt theot change constraints.phase second phase annotator perform token assignment well quality control annotator present andthe question write differentannotator phase first check correct question assign tokensto operation order achieve consistentannotation result guideline howthe token assign information inthe appendix corpus ottawe apply corpus construction procedure five database produce corpus question correspond call otta order compare result withprevious work four database thespider corpus chinook college driving school formula extend dump imdb3 referto moviedata annotation employ engineer basic knowledge sqldatabases.5.1 corpus statisticstable summarize dataset number table database range number attribute range column perdatabase chinook moviedata ourcorpus annotated ithas around annotated threedatabases moviedata also performedthe token annotation procedure database compute average complexity score except moviedata hard otherdatabases medium average query complexity.the average time question annotation rangesfrom second average second token assignment question correction onthe hand take average second perot.5.2 corpus comparisonin order examine corpus compare itscharacteristics spider corpus lcquad corpus compare coverage ofthe query data complexity natural language question complexity corresponding sparql/sql queries.coverage table show major characteristicsof three corpus compare coverage ofthe database term ratio table andattributes appear queries.the average attribute coverage spider overall database equal however thanhalf database spider contain table orless thus also report coverage attributes3https //www.imdb.com/902moviedata chinook college driving school formula1 tables attributes queries annotation complexity hard medium medium medium mediumtable statistics corpus otta questions queries table/db table attr msttr tokens timespider sec.lc-quad otta sec.table comparison corpus otta spider lc-quad corpus note number ofdatabases lc-quad since open-domain knowledge base number table corresponds number different class numbers parenthesis consider database join group order nested aggregations booleanspider lc-quad comparison query complexity base ratio component query aggregation inlc-quad report number query count operation.only consider database morethan table spider cover attribute corpus otta contrast covers54.4 attribute furthermore divide become apparent consider databaseswith large amount table instance theformula-1 database corpus cover attribute contrast spider only22.1 attribute cover lc-quad cover properties4 attributesin correspond anextensive coverage consider high amountof properties.the table coverage show similar picture approach cover table thedatabases whereas spider cover thisnumber drop consider onlydatabases table thiseffect pronounced formula1 database cover table whereas spider cover showsthat method well scale large database relevant real-world application wheredatabases vast number table exist lcquad cover around approx make comparison hard impossible cover vast amount class with4 number class property wikidata weconsulted http //tools.wmflabs.org/sqid30k queries.query complexity order compare complexity query examine number ofoccurrences different component query table first observe corpus otta doesnot contain query order operatorsor nest query however could easilyadded grammar fill furthermore spider contain aggregation operation inparticular count average could easily adapt corpusby sample tree contain aggregation hand corpus standsout number join query averageotta join operation query contrastto spider join query fact query spider contain join whereas otta compose query contain least join operation furthermore around query contain joinsin contrast spider hand lc-quad contain average equivalent join relational database query nature graph database queriesthat optimize handle query rangeover multiple triple pattern however lc-quad2.0 lack complexity consider complex component e.g. group set-operation addition operation relational903algebra also support boolean question i.e. yes/no question make ourcorpus compare lc-quad complexity lexical complexity ofthe question measure term meansegmental token-type-ratio msttr covingtonand mcfall compute numberof different token type relation tokensin corpus msttr compute textsegments equal length order avoid biasesdue different length within corpus first note average length question allthree corpus approximately between10.6-13.6 token average table show thatour corpus contain much high lexical complexity question spider instead of0.52 thus approach seem avoid trivialor monotonous question also match withour impression manual inspection theother hand lexical complexity high lcquad open domain natureof dataset.examples table show example question otta compare question spider example show quality thequestions similar easy question bothdatasets often simple filtering question onone table medium complexity question includejoin operation filter hard question bothdatasets include join operation aggregationoperations find maximum compute average difference theextra complexity spider focus onsubqueries clause otta otherhand focus large join path aretypical real-world database query well asgroup-by operation aggregations.6 baseline systemsbaseline model baseline model fromnl question follow syntactic neuralmodel code generation neubig,2017 refer grammar-rnn5 thismodel base encoder-decoder architecturethat learn generate sequence productionrules arbitrary grammar turn produce query give question moredetailed discussion architecture refer5the ir-net also base thegrammar-rnn time write paper irnet rank second spider leader board.the reader neubig case learn generate rule define figure give question natural language based onthe generated list rule created.we train model phase pre-trainingphase supervised phase pre-trainingphase train grammar-autoencoder largeamounts randomly sample supervised phase replace grammar-encoder atext encoder train labelled dataset i.e. sample question correspondingot.encoder question standardgated-recurrent unit chung encode question denote representation i-th token question theencoder produce corresponding hidden state rn×h denote concatenation allhidden state produce question number token thesize hidden state.decoder decoder learn generate sequence production rule tree generate give encode question generation process formalize =t∏t=1p action take time actionstaken time parent action take encoded input question aretwo different type rule model appliesduring decoding current rule generatesa non-terminal symbol applyrule execute apply production rule current tree next symbol terminal thengentoken apply select tokenfrom vocabulary case different type token generate table-names attribute-names filter operation similar togrammar-rnn implement decoder arecurrent neural network internal stateis give at−1 h̃t−1 embedding current node type e.g.average union context vector iscomputed apply soft-attention inputhidden state ht−1 hidden vectorof last state contrast neubig,904hardness spider ottaeasyfind number album invoice total orsmaller issue average unit price track unit price track compose alfredellis/james brown find customer information state country belong postal code medium count number track part rock genre average length track grungeplaylist please show employee first names employee serve least customers.when sell track large byte find name artist make album balls tothe wall postal code sell track namedheadspace hard average duration millisecond track thatbelong latin genre many different playlist track biggerthan byte exist name artist release anyalbums album title track lowestlength millisecond genre name fantasy last name customer without invoicetotals exceed genre artist name scholarsbaroque ensemble extra name medium type least commonacross track whats total unit price sell customer emailhholy gmail.com argentina bill country count number artist release analbum.how many different genre track werebought customer live france album title album contain reggae rock genre track customer make least purchase excludingtitles chico science nacao zumbi album table example question otta spider group example hardness score theexamples chinook domain online music store database.2017 apply attention base luong al.,2015 h̃t−1 tanh ht−1 selection term fouroutput matrix wrencodes grammar rule nonterminal symbol encode thetable name attribute comparison operation respectively depending current frontiernode next output compute argmax softmax grammar encoder tree encoder weuse pre-training base gruarchitecture decoder hidden state foreach rule compute at−1 ht−1 contrast encoder context vectorct moreover ht−1 last hidden state compute output encoder sequence state rr×h rdenotes number rule encoded tree.token attention straight-forward method toinclude explicit token alignment create second annotation phase force theattention mechanism learn alignment forthis extra loss function computesthe binary cross entropy attention weight.more formally softmax ht−1he attention weight compute timestept pre-training phase replace byhr attention weight thei-th token token lossgi denotes token assignedto current node not.7 resultswe report result model detailsof experimental setup find appendixa experiment repeat five time different random seed table show precision ofthe grammar-rnn datasets otta theprecision define exact result matchingbetween gold standard query generatedquery furthermore table show averageprecision query complexity category thecolumn weighted avg. refers mean average precision query irrespective thequery complexity category.precision database exceptformula-1 model achieve precision formula-1 themodel achieve score couldbe explain fact formula-1database contain different attribute ourdata cover attribute furthermore attribute appear time perquery average contrast college905database attribute appear query average thus hard model learnattributes appear often trainingset database model cannothandle extra hard question often contain multiple join aggregation and/or group byoperators note without pre-training phase score drop large margin instance thescores moviedata drop precision.easy medium hard extra hard weighted avg.moviedata school precision query datasets accord query complexity weighted avg. refer tothe mean average precision query irrespectiveof query complexity category.benefit token assignments evaluate whether token assignment help totrain good model figure display learningcurves moviedata database andwithout token assignment model trainedwith data.the result show token assignmentincreases score around case of20 training data gain even high thus show model benefit theadditional information provide tokenassignments.0.13060.31550.35830.38190.47560.20090.3420.380.4180.49360.10.20.30.40.50.620 normal token assignmentfigure learning curve data database moviedata partof otta compare score training withand without token alignment.8 conclusionin paper introduce fast annotation procedure create query correspondingdatabase query case operation trees procedure triple velocity ofannotation comparison previous method ensure large variety different type ofqueries cover large part underlying database furthermore procedure allowsa fine-grained alignment token operations.we method generate otta anovel corpus semantic parsing base operation tree combination token assignments.generating corpus time- costefficient previous approach statistical analysis show corpus yield highercoverage attribute database morecomplex natural language question exist method furthermore implement abaseline system automatically generate otsfrom query baseline achieve score ofup precision already reasonablewhile also leave large potential improvementin future research finally show inclusion token alignment result increaseof precision result explore toleverage token assignment domain adaptionand few-shot learning also plan enhance theannotation process automatically generate proposal question token assignmentsand annotator perform corrections.we hope increase annotation efficiencyeven more.9 acknowledgementsthis work partially fund lihlith project support era-netchist-era swiss national science foundation agencia estatal deinvestigacin spain project pcin-2017-118and pcin-2017-085 inode project supportedby european unions horizon researchand innovation program grant agreement no863410
knowledge graph completion aimsat automatically predict miss link forlarge-scale knowledge graph vast number state-of-the-art technique havegot publish conference severalresearch field include data mining machine learning natural language processing however notice several recentpapers report high performance whichlargely outperform previous state-of-the-artmethods paper find beattributed inappropriate evaluation protocol propose simple evaluation protocol address problem propose protocol robust handle bias themodel substantially affect finalresults conduct extensive experiment andreport performance several exist method protocol reproducible codehas make publicly available.1 introductionreal-world knowledge base usually expressedas multi-relational graph collectionsof factual triplet triplet represent relation head entity atail entity however real-word knowledge basesare usually incomplete dong whichmotivates research automatically predictingmissing link popular approach knowledgegraph completion embed entity andrelations continuous vector matrix space well-designed score function tomeasure plausibility triplet mostof previous method translation distancebased bordes wang xiaoet semantic matching base nickel tresp yang al.,2014 nickel trouillon ∗equal contribution.liu score function easyto analyze.however recently vast number neuralnetwork-based method propose theyhave complex score function utilize blackbox neural network include convolutional neural networks cnns dettmers nguyen recurrent neural networks rnns wang graph neural networks gnns schlichtkrullet shang capsulenetworks nguyen ofthem report state-of-the-art performance severalbenchmark datasets competitive previousembedding-based approach considerable portion recent neural network-based paper reportvery high performance gain consistent across different datasets moreover mostof unusual behavior analyzed.such pattern become prominent mislead whole community.in paper investigate problem andfind attribute inappropriate evaluation protocol approach wedemonstrate evaluation protocol givesa perfect score model always output aconstant irrespective input lead toartificial inflation performance several model find simple evaluation protocolthat create fair comparison environment alltypes score function conduct extensive experiment re-examine recent method andfairly compare exist approach thesource code paper publicly available http //github.com/svjan5/kg-reeval.2 backgroundknowledge graph completion given knowledge graph de5517fb15k-237 wn18rrconve +4.0 +10.6 tucker +10.2 +9.3 convkb +21.8 capse +60.9 kbat +59.4 +2.3 transgate +24.3 table changes different method onfb15k-237 wn18rr datasets respect toconve show inconsistent improvements.note entity relation triplet fact task knowledge graph completion involve infer miss fact base onthe know fact exist method define embedding entity relation i.e. score functionf assign highscore valid triplet invalid ones.kgc evaluation evaluation forpredicting give triplet kgcmodel score triplet based score modelfirst sort triplet subsequently find therank valid triplet list amore relaxed call filtered setting theknown correct triplet train valid testtriplets remove except evaluate bordes triplet call negative samples.related work prior work kadlec cast doubt claim performance improvement several model architecturalchanges oppose hyperparameter tune ordifferent training objective work raisesimilar concern different angle byhighlighting issue evaluation procedureused several recent method chandrahas analyze geometry embeddingsand correlation task performance whilenayyeri examine effect different loss function performance however theiranalysis restrict non-neural approaches.0 graph entities0.20.40.60.81.0tripletscorefigure sorted score distribution convkb anexample valid triplet negative sample thescore normalize good dotted line indicate score valid triplet findthat example around negative sampledtriplets obtain exact score valid triplet.3 observationsin section first describe observationsand concern investigate reason behind.3.1 inconsistent improvements overbenchmark datasetsseveral recently propose method report highperformance gain particular dataset however performance another dataset notconsistently improve table reportchange score fb15k-237 toutanovaand chen wn18rr dettmers al.,2018 datasets respect conve dettmerset different method include rotate tucker balažević convkb nguyen capse nguyenet kbat nathani andtransgate yuan overall find thatfor recent base method inconsistent gain datasets instance convkb improvement overconve fb15k-237 degradation wn18rr surprising give methodis claim good conve otherhand method like rotate tucker give consistent improvement across benchmark datasets.3.2 observations score functionsscore distribution evaluate method give triplet ranking tgiven compute score tripletsof form of5518frequency01250250037505000number triplets score1-4 plot show frequency number ofnegative triplet assign score thevalid triplet evaluation fb15k-237 dataset.the result show method like convkb andcapse large number negative triplet samescore valid triplet whereas method likeconve occurrence rare.all entity invest recent base approach find unusual score distribution negatively sample tripletshave score valid triplet instance fb15k-237 dataset present figure1 negatively sample triplets,8,520 exact score valid triplet.statistics whole dataset figure wereport total number triplet exactsame score entire dataset convkb nguyen capse nguyen al.,2019 compare conve dettmerset suffer issue.we find convkb capse multipleoccurrences unusual score distribution onaverage convkb capse exactly score valid tripletover entire evaluation dataset fb15k-237 whereas conve around almostnegligible section demonstrate thisleads massive performance gain method likeconvkb capse.root problem investigate thecause behind unusual score distribution infigure plot ratio neuron becomingzero relu activation valid triplet vs.their normalized frequency fb15k-237 dataset.the result show convkb capse alarge fraction respectively ofthe neuron become zero apply relu0.0 neurons become zero051015202530normalizedfrequencyconvkbcapseconvefigure distribution ratio neuron becomingzero relu activation different method thevalid triplet fb15k-237 dataset find forconvkb capse unusually large fraction neuron become zero relu activation whereas thedoes hold conve.activation however conve count issubstantially less around thezeroing nearly neuron least forconvkb capse representationof several triplet become similar forward thus lead obtain exactsame score.4 evaluation protocols kgcin section present different evaluation protocol adopt knowledge graph completion show inappropriate evaluation protocol reason behind unusualbehavior recent nn-based methods.how deal score essentialaspect evaluation method decide howto break triplet score moreconcretely score candidate ifthere multiple triplet score fromthe model decide triplet pick.assuming triplet sort stablemanner design general evaluation schemefor consist following threedifferent protocol setting correct triplet insertedin beginning bottom correct triplet insert atthe random correct triplet placedrandomly ′.5519reported random bottommrr ↑conve +.164 +.106 capse +.361 +.229 kbat effect different evaluation protocol recent embed method fb15k-237 dataset fortop bottom report change performance respect random protocol please refer section5.4 detail kbat test data leakage original implementation experiments.discussion based definition threeevaluation protocol clear evaluationprotocol evaluate model rigorously itgives model bias provide thesame score different triplet inappropriateadvantage hand bottom evaluationprotocol unfair model inference time penalize model givingthe score multiple triplet i.e. manytriplets score correct triple correct triplet least rank possible.as result random best evaluationtechnique rigorous fair themodel line situation meet thereal world give several score candidate option select randomly.hence propose random evaluationscheme model performance comparisons.5 experimentsin section conduct extensive experimentsusing propose evaluation protocol makea fair comparison several exist methods.5.1 datasetswe evaluate propose protocol fb15k-237 toutanova chen dataset1 asubset fb15k bordes inverserelations delete prevent direct inference testtriples training.5.2 methods analyzedin experiment categorize exist kgcmethods following categories:1we also report result wn18rr dettmers al.,2018 dataset appendix.• non-affected include method whichgive consistent performance different evaluation protocol experiment paper consider three method conve rotate tucker.• affected category consist recently propose neural-network base method whose performance affect different evaluation protocol convkb capse transgate2 kbatare method category.5.3 evaluation metricsfor method code hyperparameters provide author respective paper model performance evaluate bymean reciprocal rank mean rank hits filtered setting bordes evaluation resultsto analyze effect different evaluation protocol describe section study performance variation model list section study effect bottom protocol compare random protocol intheir original paper conve rotate tuckeruse strategy similar propose randomprotocol convkb capse kbat usetop protocol also study random error inrandom protocol multiple wereport average standard deviation runswith different random seed result present tables find open-source implementationof transgate leave re-evaluation transgate ourfuture work.5520we observe non-affected method likeconve rotate tucker performance remain consistent across different evaluation protocol however affected method considerable variation performance specifically observe model performbest evaluate worst whenevaluated bottom3 finally find theproposed random protocol robust different random seed although theoretic upperand bound random score andbottom score respectively evaluateknowledge graph completion real-world largescale knowledge graph randomness taffect evaluation result much.6 conclusionin paper perform extensive reexamination study recent neural network basedkgc technique find many modelshave issue score function combinedwith inappropriate evaluation protocol method report inflated performance based ourobservations propose random evaluation protocol clearly distinguish theseaffected method others also stronglyencourage research community follow therandom evaluation protocol evaluation purposes.acknowledgementswe thank reviewer helpful comments.this work support part national science foundation grant iis-1546329and google fellowship
previous study bridge anaphoraresolution poesio al.,2013b pairwise model totackle problem assume goldmention information give paper cast bridge anaphora resolution question answer base context allowsus find antecedent give anaphorwithout know gold mention information except anaphor presenta question answer framework barqa task leverage power oftransfer learning furthermore proposea novel method generate large amountof quasi-bridging training data showthat model pre-trained dataset andfine-tuned small amount in-domaindataset achieves state-of-the-art result forbridging anaphora resolution bridgingcorpora isnotes markert andbashi rösiger introductionanaphora account text cohesion crucial text understanding anaphor nounphrase usually refer back different entity antecedent text anaphoraresolution task determine antecedentfor give anaphor direct anaphora resolution attract attention community recently winograd schema challenge rahman opitz frank kocijan indirect anaphora resolution orbridging anaphora resolution less well studied.in paper focus bridge anaphoraresolution bridging anaphor antecedent link various lexico-semantic frame encyclopedic relation following houet rösiger mainlyconsider referential bridging bridginganaphors truly anaphoric bridging relation context-dependent example building building substantial damage plausible antecedent candidate thebridging anaphor resident base lexical semantics order find antecedent buildingswith substantial damage take meaning broad discourse context account post-earthquake parlance build inspect building withsubstantial damage color-coded green allow resident re-enter yellow allow limited access allow resident last entryto gather everything could within minutes.most previous study bridge anaphora resolution poesio lassalle denis,2011 tackle theproblem pairwise model assume thatthe gold mention information give work poesio lassalle denis houet syntactic pattern measuresemantic relatedness head noun ananaphor antecedent proposesa simple deterministic algorithm also consider semantics modification head nouns.these approach however take broadercontext outside noun phrase i.e. anaphor andantecedent candidate account often failto resolve context-dependent bridging anaphor asdemonstrated example bridging anaphor require contextdependent text understanding recently gardneret argue question answering natural format model task require question understanding paper cast bridginganaphora resolution question answer based1all example specify otherwise isnotes markert bridging anaphor type boldface antecedent italic throughout paper.1429on context develop system barqa task base bert devlin context show example firstrephrase every anaphor question resident answer question thesystem identify span antecedentfrom context compared pairwise model system require gold systemmention information antecedent candidates.in addition framework allow integratecontext outside choose antecedentsfor bridge anaphor instance green damage color-coded among predict answer question.different coreference resolution areno large-scale corpus available referentialbridging resolution complexity thispaper propose method generate largeamount quasi-bridging training data fromthe automatically parse gigaword corpus parkeret napoles demonstratethat quasi-bridging training data betterpre-training choice bridge anaphora resolution compare squad corpus rajpurkaret moreover show modelpre-trained dataset fine-tuned smallamount in-domain dataset achieve state-ofthe-art result bridge anaphora resolution ontwo bridge corpus i.e. isnotes markert al.,2012 bashi rösiger summarize main contribution ourwork formalize bridge anaphora resolution question answer problem propose model solve task explore method generate large amountof quasi-bridging training dataset demonstrate value bridge anaphora resolution carefully carry series experiment referential bridge corpus provide error analysis verify effectivenessof model resolve context-dependentbridging anaphor isnotes release codeand experimental datasets http //github.com/ibm/bridging-resolution.2 related workbridging anaphora resolution since empirical corpus study relate bridginghave carry various genre different language fraurud poesio vieira,1998 poesio nissim gardentand manuélian nedoluzhko eckart markert rösiger,2018 poesio among datasets isnotes markert bashi rösiger,2018 arrau poesio recent three public english corpus containmedium- large-sized bridging annotation andhave evaluate system performanceon bridge anaphora recognition al.,2013a rösiger bridginganaphora resolution poesio lassalleand denis well full bridging resolution rösiger paper focusexclusively task antecedent selection.it worth note bridge definition inthe arrau corpus different usedin datasets rösiger point isnotes bashi contain referential bridging bridge anaphor aretruly anaphoric bridging relation contextdependent arrau bridge linksare purely lexical bridging pair notcontext-dependent e.g. europe spain tokyo –japan paper focus resolve referential bridging anaphors.regarding algorithm bridge anaphoraresolution previous work pairwisemodel task model assume gold system mention information give beforehand.it creates positive/negative training instance bypairing every anaphor precede mentionm usually antecedent candidate form window size.poesio lassalle denis train pairwise model resolve mereological bridging anaphor english gnome corpus2 french dede corpus gardent andmanuélian respectively exception ishou propose joint inference framework resolve bridge anaphor inisnotes framework build upon pairwise model predict semantically relatedbridging anaphor document together.recently generate word representation resource bridging i.e. embeddings bridge propose simple deterministic algorithm find antecedent bridginganaphors isnotes bashi word representation resource learn large corpus2the gnome corpus publicly available.1430input textbarqa…in post-earthquake parlance building afterbeing inspect building withsubstantial damage werecolor-coded green allowedresidents re-enter yellowallowed limited access redallowed resident lastentry gather everything theycould within minutes.…question context answers predicted spansresidents ofwhat post-earthquake parlance building inspect buildingswith substantial damage color-coded.green allowed resident re-enter yellowallowed limited access allowedresidents last entry gathereverything could within minute building withsubstantial damage building building withsubstantial damage building building damage green damage werecolor-coded…limited accessof post-earthquake parlance building inspect buildingswith substantial damage color-coded.green allowed resident re-enter yellowallowed limited access allowedresidents last entry gathereverything could within minute building withsubstantial damage building building withsubstantial damage building building substantialdamage green allowedresidents…… …figure resolving bridge anaphor example barqa.and capture common-sense knowledge i.e. semantic relatedness nps.different algorithm mention model require extracted orgold mention input predictsthe span antecedent bridging anaphordirectly.question answering reading comprehensionor question answer base context attacted much attention within community particular since rajpurkar release large-scale dataset squad consistingof question paragraph extract wikipedia article previous workhas cast traditional task questionanswering textual entailment mccannet entity–relation extraction al.,2019 coreference resolution unlike task largescale training datasets bridging result weform question task naturalway order leverage exist datasets e.g. squad require common-sense reasoning addition generate large-scale trainingdataset quasi-bridging demonstrate itis good pre-training corpus bridge anaphoraresolution.recently gardner argue weshould consider question answering formatinstead task perspective work specific probe task totest model ability understand bridginganaphora base context.winograd schema challenge bridginganaphora resolution share similarity withwinograd schema challenge specifically task understand contextto find antecedent anaphor however antecedent search space bridge anaphoraresolution much wsc.this anaphor pronoun itsantecedent usually samesentence bridge pair usually requirecross-sentence inference instance isnotes around anaphor antecedentsoccurring sentence ofanaphors antecedent twosentences away.recently kocijan heuristic generate large-scale wsc-like dataset andreport model pre-trained datasetachieves best result several datasetsafter fine-tune small in-domain dataset.we find similar pattern result bridginganaphora resolution section barqa system bridginganaphora resolutionin section describe system calledbarqa bridge anaphora resolution detail figure illustrate barqa predict antecedent bridge anaphor example problem definitionwe formulate bridging anaphora resolution acontext-based problem specifically give bridging anaphor surrounding1431context rephrase question thegoal predict text span isthe antecedent propose spanbased framework extract general ourbarqa system build vanilla bertqa framework devlin furthermodify inference algorithm guarantee thatthe answer span always appear beforethe bridge anaphor section moredetails devlin present theinput question context singlepacked sequence calculatethe probability every word startand answer span training objectiveis log-likelihood correct start endpositions.3.2 question generationin english preposition syntacticstructure encodes different associative relation noun phrase cover avariety bridge relation instance thechairman indicate professional function organization price stock indicate attribute object poesio also pattern estimate partof bridging relation pattern reflect howwe explain bridge anaphora human beings.it seem natural understandthe meaning bridging anaphor find theanswer question thesurrounding context a.as result order generate corresponding question bridging anaphor firstcreate remove word appear thehead concatenate form question point byhou premodifiers bridge anaphor areessential element understand bridge relations.for instance bridging anaphor painstakingly documented report base hundredsof interview randomly select refugee corresponding question painstakingly documented report answer generationfor bridge anaphor together correspond question context describedabove construct list answer contain antecedent occurring contextca.3 addition every antecedent froma following variation representthe main semantics answer list head e.g. last week earthquake create remove postmodifiers e.g. preliminary conclusionfrom survey downtown high-rise create remove postmodifiers determiner e.g. thetotal potential claim disaster worth note context notcontain antecedent bridging anaphora e.g. anaphor antecedentsoccurring small window size toconstruct answer answerlist a.3.4 inferencedifferent squad-style question answer specific requirement theposition predicted span bridge anaphoraresolution anaphor must appear antecedent therefore inference stage eachbridging anaphor first identify position context predict text spanswhich appear prune listof predict text span keep kspan candidate contain word andl empirically respectively wealso prune span prediction function word e.g. trainingduring training process first spanbert joshi initialize barqamodel show promise improvementson squad compare vanilla bert embeddings continue train model different pre-training fine-tuning strategies.section describes different training strategiesin detail.for every training strategy train barqafor five epoch learn rate abatch size training testing themaximum text length tokens.3in isnotes bashi gold coreference annotation ontonotes weischedel identify allpossible antecedent every bridging anaphor.4in general small learning rate i.e. and5e-5 small fine-tuning epoch common practice forfine-tuning bert model test combination these1432input textin search evidence obstruction justice thepresident republicans seek document concern severalfigures campaign fund-raising scandal.today hearing crime perjury attempt focusthe nation attention whether remove clinton fromoffice allegedly oath relationship withthe former white house intern obstruct justiceand tamper witness conceal it.generated quasi-bridging examplesentence today hearing crime perjury attemptto focus nation attention whether remove clintonfrom office allegedly oath relationshipwith former white house intern obstruct justiceand tamper witness conceal it.sentence search evidence obstruction bythe president republicans seek document concern severalfigures campaign fund-raising scandal.bridging pair………………figure examples generate quasi-bridging training data.4 generate quasi-bridging trainingdatabridging anaphora complex phenomenon andthere large-scale corpus available referential bridging section describe howwe generate large scale quasi-bridging dataset.hou explore syntactic prepositionaland possessive structure train wordembeddings bridging inspired work first structure identify bridginganaphors corresponding antecedent back discourse createbridging-like examples.more specifically give text first extractnps contain prepositional structure e.g. xpreposition possessive structure e.g. order high-quality automatically generate bridging annotation apply anadditional constraint i.e. andy contain node constituent tree instance consider npssuch political value impose sanctionsagainst south africa cost repair theregion transportation system illustrate generate bridgingannotation sentence pair araw text5 first extract obstruction ofjustice sentence identify inthis extract i.e. obstruction justice collect list sentence theparameters various train configuration small document isnotes corpus bashi corpus respectively corpus observe learningrate minimal impact result andfor learn rate result continue improve thebeginning epochs performance staysmore less epochs text gigaword corpus parker al.,2011 napoles text every sentence contains butdoes contain contain onesentence choose close tosi close sentence morelikely semantically relate finally generatethe sentence replace obstruction justice original sentence obstruction give quasi-bridging example twoadjacent sentence i.e. bridginglink i.e. justice obstruction result obtain large amount quasibridging training data i.e. around millionbridging pair apply method describedabove nyt19 section automaticallyparsed gigaword corpus.in order understand quality quasibridging training dataset randomly sample100 quasi-bridging sentence pair manuallycheck bridge annotation instance wescore bridge annotation scale mean bridging annotation correctand sentence pair sound natural indicatesthat example make sense soundnatural english denotes annotation unacceptable overall find instance instance scoreof respectively remain instance score zero general ournoisy quasi-bridging training dataset contain large number diverse bridge pairs.5 experiments5.1 datasetswe four datasets experiment firstdataset isnotes6 release markert al.6http //www.h-its.org/en/research/nlp/isnotes-corpus1433 dataset contain text bridge world streetjournal portion ontonotes corpus weischedel second dataset iscalled bashi rösiger contains459 bridge nps7 referential anaphorsfrom texts8 note bridge anaphorsin corpus limit definite npsas previous work poesio lassalle denis bridge relation arenot limit prototypical whole part relationor element relation consider twocorpora expert-annotated in-domain datasets.we assume reason skill e.g. world knowledge word relatedness require toanswer question squad also appliedfor bridge anaphora resolution therefore weinclude squad training data rajpurkaret training dataset another training dataset large scale quasi-bridging corpus quasibridging describe section summarizes four datasets mentionedabove note isnotes bashi number pair number bridginganaphors anaphor multiple antecedent e.g. coreferent mention thesame antecedent entity experimental setupfollowing accuracy thenumber bridge anaphor measure system performance resolve bridge anaphor onisnotes bashi calculate numberof correctly resolve bridging anaphor dividedby total number bridge anaphors.we measure type accuracy lenient accuracy strict accuracy strict accuracy original gold antecedent annotation arecounted correct answer lenient accuracy additional variation original antecedent annotation describe section3.3 correct answer list instance suppose gold antecedent annotation thefour seasons restaurant predicted span four seasons restaurant count predictionas incorrect prediction strict accuracy evaluation however correct prediction lenientaccuracy evaluation.7bashi considers comparative anaphora bridginganaphora exclude study.8note article different onesin isnotes.it worth note lenient accuracy corresponds exact match metric squad rajpurkar correct answer liststhat generate describe section canpartially address evaluation problem imperfect system mention prediction reportf1 score give partial credit aprediction capture main semanticsof original gold annotation fourseasons evaluation every bridging anaphor sentence contain firstsentence text previous sentence ofsa well form surround contextca line antecedentcandidate selection strategy.5.3 results isnotes bashi usingdifferent training strategiesin section carry experiment ourbarqa system different training strategies.for every bridging anaphor choose spanwith high confidence score contextca answer question thisspan predicted antecedent report resultson isnotes bashi lenient accuracy seetable result isnotes find thatbarqa train small number in-domaindataset bashi achieve accuracy isnotes well model trainedon large-scale datasets squad quasibridging however thesetwo datasets pre-train model fine-tuningit small in-domain dataset bashi bothsettings i.e. squad bashi quasibridging bashi achieve good result comparedto bashi training dataset thisverifies value pre-training fine-tuningstrategy i.e. pre-training model large scaleout-of-domain noisy dataset fine-tune itwith small in-domain dataset.particularly notice performance ofusing quasibridging alone oneusing squad however combine quasibridging bashi achieve best result onisnotes accuracy seem thatthe large-scale in-domain noisy training data quasibridging bring value large-scaleout-of-domain training data squad observe similar pattern result on1434corpus genre bridging type anaphors pairsisnotes news article referential bridge news article referential bridge train wikipedia paragraph news article quasi bridge four datasets experiments.barqa lenient accuracy isnotes lenient accuracy bashilarge-scale out-of-domain/noisy training datasquad in-domain training databashi isnotes in-domain fine-tuningsquad bashi quasibridging bashi squad isnotes isnotes results barqa isnotes bashi different training strategy indicates statisticallysignificant difference model two-sided pair approximate randomization test pre-training model quasibridgingthen fine-tune isnotes achieve bestresult accuracy furthermore evaluate bashi seem usingsquad pre-training dataset notbring additional value combine withisnotes.5.4 results isnotes bashicompared previous approachesprevious work bridge anaphora resolution onisnotes bashi gold/system mention asantecedent candidate report result strictaccuracy order fairly compare system every bridging anaphor first mapall span prediction system barqato gold/system mention choose thegold/system mention high confidencescore predicted antecedent specifically wemap predicted span mention sharethe head part createdby remove postmodifiers instance total potential claim themention total potential claim disaster predicted span anygold/system mention filter followinghou keep prediction whosesemantic type time time expression.the process equal gold/systemmentions semantic information furtherprune barqa span predictions.table table compare result oursystem barqa previous study bridge anaphora resolution isnotes bashi respectively datasets barqa modelis train best strategy report table2 pre-training quasibridging fine-tuningwith small in-domain data isnotes previously reportedthe best result prediction deterministic algorithm embeddings bridge nphead modifier additional feature intothe global inference model propose byhou deterministic algorithm isbased word embeddings bridging model meaning base head nounand modifications.our system barqa gold mention together semantic information tofurther prune span prediction achieve thenew state-of-the-art result isnotes strictaccuracy barqa gold mentions/semantics strict accuracy table how1435system gold mentions accuracymodels pairwise model model embeddings bridge head modifier model embeddings bridge head modifier workbarqa gold mentions/semantics strict accuracy without mention information strict accuracy without mention information lenient accuracy results different system bridge anaphora resolution isnotes bold indicate statisticallysignificant difference model two-sided pair approximate randomization test system mentions accuracymodel embeddings bridge head modifier workbarqa system mentions/semantics strict accuracy without mention information strict accuracy without mention information lenient accuracy results different system bridge anaphora resolution bashi bold indicate statisticallysignificant difference model two-sided pair approximate randomization test argue gold mention informationto construct antecedent candidate acontrolled experiment condition experiment setup barqa without mention information lenient accuracy realistic scenario practice.on bashi report accuracyof strict accuracy automaticallyextracted mention gold syntactic treeannotations system barqa without mention/semantic information achieve accuracyof strict accuracy evaluation result barqa improvedwith accuracy integratemention/semantic information model.note also adapt deterministic algorithm resolve lexical bridginganaphors arrau poesio andreported accuracy testdataset although paper focuson lexical bridging model barqa also beapplied resolve lexical bridging anaphor wefound barqa train train datasetalone around pair achieve accuracy test dataset.6 error analysisin order good understand model automatically label bridge anaphor isnotes aseither referential bridging/world-knowledge referential bridging/context-dependent thenanalyze performance barqa bestmodel categories.rösiger point althoughlexical referential bridging differentconcepts sometimes co-occur withinthe pair expression example employees anaphoric expression thesame time relation antecedententity mobil corp./the company thebridging anaphor employees correspond thecommon-sense world knowledge truewithout specific context call casesas referential bridging/world-knowledge differently call bridging anaphor referential bridging/context-dependent multipleequally plausible antecedent candidate accordingto common-sense world knowledge thenp pair analyze context tochoose antecedent example may1436 pair barqa embknow comparison percentage correctlyresolved anaphor barqa best modelfrom bridge categories.argue exploration production division employees example also validcommon-sense knowledge fact however consider less prominent company employees mobil corp. prepare slash size ofits workforce u.s. possibly soon nextmonth individual familiar company sstrategy size know butthey center exploration production division responsible locate oilreserves drill well pump crude andnatural employees notified.for bridging anaphor deterministic algorithm embeddings bridge word representation resource learn froma large corpus predict semanticallyrelated among candidate antecedent prediction system reflectthe common-sense world knowledge nppairs thus algorithm label bridge anaphor isnotes bridging anaphor iscorrectly resolve embeddings bridge label referential bridging/world-knowledge otherwise label referential bridging/contextdependent compare percentage correctlyresolved anaphor barqa gold mention best model mlnii bridging category notethat contain several context-levelfeatures e.g. document span verb pattern overall seem barqa model well atresolving context-dependent bridging anaphors.7 conclusionsin paper model bridge anaphora resolution question answer problem proposea system barqa solve task.we also propose method automaticallygenerate large scale quasi-bridging training data show system whentrained quasi-bridging training datasetand fine-tune small amount in-domaindataset achieve state-of-the-art result ontwo bridge corpora.compared previous system model issimple realistic practice notrequire gold annotation construct listof antecedent candidate moreover propose formulation model easilystrengthened span-based text understand corpus pre-training datasets.finally release experimental qadatasets squad json format bridginganaphora resolution isnotes bashi theycan test model ability understand text term bridge inference.acknowledgmentsthe author appreciate valuable feedback fromthe anonymous reviewer
open-domain question answering formulate phrase retrieval problem inwhich expect huge scalability andspeed benefit often suffer accuracy limitation exist phraserepresentation model paper aimto improve quality phrase embed augment contextualizedsparse representation sparc unlike previous sparse vector term-frequencybased e.g. tf-idf directly learn fewthousand dimension leverage rectifiedself-attention indirectly learn sparse vector n-gram vocabulary space augment previous phrase retrieval model seoet sparc show +improvement curatedtrec squadopen curatedtrec score even good best know retrieve read modelwith least fast inference speed.11 introductionopen-domain question answering taskof answer generic factoid question lookingup large knowledge source typically unstructuredtext corpus wikipedia find answer text segment chen widelyadopted strategy handle large corpus touse efficient document paragraph retrievaltechnique obtain relevant document andthen accurate expensive modelto read retrieved document find answer chen wang daset yang recently alternative approach formulate task end-to-end phrase retrieval problem encode index every possible textspan dense vector offline theapproach promise massive speed advantage with1code available http //github.com/jhyuklee/sparc.overview figurepassagebetween total area forest lose theamazon rise square kilometresquestionhow many square kilometre amazon forest waslost sparse representations amazon rise amazon sparse representations amazon rise amazon figure example sparse vector give context squad tf-idf high weight infrequent n-grams contextualized sparse representation sparc focus sematically relate n-grams.several order magnitude time complexity perform poorly entity-centric question often unable disambiguate similar differententities dense vector space alleviate issue concatenate term-frequency-based sparse vectorwith dense vector capture lexical information.however sparse vector identical acrossthe document paragraph mean everyword importance equally consider regardlessof context figure paper introduce method learn acontextualized sparse representation sparc foreach phrase show effectiveness opendomain phrase retrieval setup related previous work different task oftendirectly dense vector sparse vectorspace faruqui subramanian al.,2018 thousand dimension computational cost small gradient instead leverage rectified self-attentionweights neighboring n-grams scale itscardinality n-gram vocabulary space billion encode rich lexical information ineach sparse vector kernelize2 inner productspace train avoid explicit andobtain memory- computational efficiency.sparc improve previous phrase retrievalmodel denspi augmentingits phrase embed bothcuratedtrec squad-open fact curatedtrec result achieve state arteven compare previous retrieve readapproaches least fast speed.2 backgroundwe focus open-domain unstructured textwhere answer text span textual corpus e.g. wikipedia formally give kdocuments question taskis design model obtain answer byâ argmaxxki scoremodel learn phrase consist ofwords i-th j-th word k-thdocument pipeline-based method chen al.,2017 wang typicallyleverage document retriever reduce numberof document read suffer errorpropagation wrong document retrievedand still slow heavy reader model.phrase-indexed open-domain alternative introduce endto-end real-time open-domain approach directly encode phrase document agnostic ofthe question perform similarity search onthe encode phrase feasible decompose score function function argmaxxki query-agnostic phrase encoding question encoding denote afast inner product operation.seo propose encode phrase question concatenation densevector obtain deep contextualized wordrepresentation model devlin asparse vector obtain compute tf-idf ofthe document paragraph phrase belongsto argue inherent characteristic oftf-idf learn identical across thesame document limit representational power.2our method inspire kernel method insvms cortes vapnik goal paper propose good andlearned sparse representation model furtherimprove accuracy phrase retrievalsetup.3 sparse encoding phrasesour sparse model unlike pre-computed sparse embeddings tf-idf dynamically compute theweight n-gram depend context.3.1 need sparse representation answer question figure modelshould know target answer corresponds year confuse phrase corresponds year thedense phrase encoding likely difficultyin precisely differentiate need also encode several different kindsof information window-based tf-idf would nothelp year closer word distance example illustrate thestrong need create n-gram-based sparse encode highly syntax- context-aware.3.2 contextualized sparse representationsthe sparse representation phrase obtain concatenation start word sand word sparse embedding sstarti sendj similarly densephrase embedding obtain efficiently compute without explicitlyenumerating possible phrases.we obtain start/end sparse embed inthe unshared parameter wejust describe obtain start sparse embed omit superscript start given thecontextualized encoding document rn×d obtain start/end sparse encode rn×f relu rn×f rn×d query matrix obtain apply different linear transformation i.e. rn×d rn×d rn×f one-hot n-gram feature representation input document forinstance want encode unigram feature one-hot representation ofthe word equivalent vocabulary size intuitively contain weighted914bag-of-ngram representation n-gram isweighted relative importance startor word phrase note verylarge always exist efficient sparsematrix format e.g. explicitly create dense form since want tohandle several different size n-grams createthe sparse encode n-gram concatenate result sparse encoding practice weexperimentally find unigram bigram aresufficient cases.we compute sparse encoding questionside similar document side difference usethe token instead start word torepresent entire question share samebert linear transformation weight forthe phrase encoding.3.3 trainingas training phrase encoders wholewikipedia computationally prohibitive usetraining example extractive question answer dataset squad train encoders wealso improved negative sampling methodwhich make dense sparse representationsmore robust noisy texts.kernel function given pair question anda golden document paragraph case ofsquad first compute dense logit eachphrase phrase sparseembedding train need consider loss function define sparselogit phrase lsparsei =sstarti s′start sendj s′end brevity describe compute first term sstarti s′start correspond start word drop thesuperscript start second term compute way.sstarti s′start relu relu q′k′ rm×d rm×f denote thequestion side query n-gram feature matrix respectively output size prohibitively large efficiently compute lossby precomputing rn×m note consider apply kernel function entryis n-gram i-th positionof context equivalent j-th n-gram ofthe question efficiently compute aswell also think kernel trick inthe literature cortes vapnik allow compute loss without explicitmapping.the final loss minimize compute thenegative likelihood denseand sparse logits lsparsei∗ log∑i jexp lsparsei denote true start positionsof answer phrase want sacrifice quality dense representation isalso critical dense-first search explain insection dense-only loss omit thesparse logits original loss final loss case find weobtain higher-quality dense phrase representations.negative sampling learn robust phrase representation concatenate negative paragraphsto original squad paragraph paragraph concatenate paragraph xneg whichwas pair question whose dense representation h′neg similar original densequestion representation follow find tf-idf match scoreson word-level logits negative paragraphsfurther improve quality sparse representations.4 experiments4.1 experimental setupdatasets squad-open open-domain version squad rajpurkar use87,599 example golden evidence paragraph train encoders example test model suggestedby chen curatedtrec consist ofquestion-answer pair trec voorheeset curated baudiš šedivỳ test pair test model.we train squad test squadopen curatedtrec rely generalization ability model zero-shot curatedtrec.915model c.trec squad-openem s/qmodels dedicated search enginesdrqa paragraph ranker bertserini bert modelsorqa sparc trained distantly supervise training data trained multiple datasets† supervision target training data.table results open-domain datasets seeappendix computed.implementation details finetunebert-large encoders bert vocabulary unique token base onbyte pair encoding result uni-/bigram feature notfinetune word embed training wepre-compute store encode phrase representation document wikipedia than5 million document take hour toindex phrase wikipedia samestorage reduction search technique seoet search perform dense searchfirst rerank sparse score orperform sparse search first rerank densescores combination hybrid model dedicated searchengines show performance drqa chenet wang paragraph ranker multi-stepreasoner bertserini yanget multi-passage bert wanget end-to-end model notrely search engine result denspi al.,2019 orqa denspi +sparc evaluate denspi andours hybrid search strategy used.4.2 resultsopen-domain experiments table showsexperimental result open-domain question answer datasets compare methodwith previous pipeline end-to-end approaches.on datasets model contextualizedmodel squad1/100 squad1/10ours sparc −4.1 −3.2 doc./para tf-idf −1.9 −0.7 trigram sparc −2.0 −1.8 table ablations model show effect ofdifferent sparse representations.model f1originaldrqa chen devlin elmo sparc results squad developmentset lstm+sa+elmo query-agnostic baselinefrom representation denspi sparc largelyimproves performance phrase-indexingbaseline model denspi also method significantly faster model need heavy model theinference curatedtrec constructedfrom real user query model achieve stateof-the-art performance time submission.even though model train squad i.e. zero-shot outperform modelswhich either distant- semi-supervised withat least fast inference.on squad-open model outperformsbert-based pipeline approach bertserini yang twoorders magnitude faster multi-passage bert utilize dedicated document retriever outperform end-to-end model large margin squad-open main contribution improvement end-to-end alsonote retrieve correct document squadopen know often easily exploitable leeet open-domainappropriate test datasets curatedtrec fair comparison.ablation study table show effect contextualized sparse representation comparingdifferent variant method squad-open.we subset wikipedia dump and1/10 interestingly trigram feature insparc uni-/bigram representation call strong regularization for916q independence drqa independence film independence american science fiction denspi sparc independence india annually observe august national holiday india.q south korea drqa economy south korea south korean caput economy south korea south korean caput sparc developmental state south korea grow prediction sample drqa denspi denspi sparc sample show document title context predict answer.high-order n-gram feature appendix onhow sparc performs different search strategies.closing decomposability table performance denspi sparc thesquad v1.1 development single paragraph contain answer provide eachsample bert-large jointly encodesa passage question still high performance close f1score query-agnostic setting.qualitative analysis table show outputsof three openqa model drqa chen al.,2017 denspi denspi+ sparc model able retrieve various correct answer different document andit often correctly answer question specificdates number compare denspi showingthe effectiveness learned sparse representations.5 conclusionin paper demonstrate effectiveness ofcontextualized sparse representation sparc forencoding phrase rich lexical information inopen-domain question answering efficientlytrain sparse representation kernelizing thesparse inner product space experimental resultsshow fast open-domain model thataugments denspi sparc outperforms previous open-domain model include recentbert-based pipeline model order ofmagnitude fast inference time.acknowledgmentsthis research support national researchfoundation korea nrf-2017r1a2a1a17069645 nrf-2017m3c4a7065887 n0001418-1-2826 darpa n66001-19-2-403 allen distinguished investigator award sloan fellowship thank member korea university university washington naver clova andthe anonymous reviewer insightful comment
address challenge policy learn inopen-domain multi-turn conversation propose represent prior information dialog transition graph learn graphgrounded dialog policy foster amore coherent controllable dialog thisend first construct conversational graph dialog corpus arevertices represent howto edge represent natural transition message last utterance dialog context response thenpresent novel ground policy learningframework conduct dialog flow planningby graph traversal learn identify awhat-vertex how-vertex ateach turn guide response generation thisway effectively leverage facilitate policy learn follow enablesmore effective long-term reward design itprovides high-quality candidate action give control policy results benchmark corpus demonstratethe effectiveness framework.1 introductionhow effectively learn dialog strategy enduring challenge open-domain multi-turn conversation generation address challenge previous work investigate word-level policy model simultaneously learn dialog policy language generation dialog corpus al.,2016b zhang word-levelpolicy model often lead degeneration issuewhere utterance become ungrammatical orrepetitive lewis alleviate thisissue utterance-level policy model propose decouple policy learn responsegeneration focus incorporate∗this work baidu.†corresponding author wanxiang che.今天晚上要通宵加班i work overnight tonight.辛苦了，好辛苦，注意身体take care avery hard work.还不能打盹，领导也在i take leadersare also here.这么晚了，不 late sleepy 哈哈，那也会 吧ha-ha make sleepy.我以为你会犯困的，这么晚了i think sleepy late.context mechanisms responses犯困/sleepy+messageresponsefigure system understand user messageby link call linked vertex hitwhat-vertices green color select what-vertex sleepy how-vertex respond mechanismm3 network one-hop neighbor hitvertices generate coherent response twosub-steps firstly obtain response representation r̄using message representation amessage-encoder next produce response sleepy input notice howvertices rather completelyindependent other.high-level utterance representation e.g. latentvariables keywords facilitate policy learning zhao utterance-level method tend toproduce less coherent multi-turn dialog since itis quite challenge learn semantic transitionsin dialog flow merely dialog data withoutthe help prior information paper wepropose represent prior information dialogtransition message response graph optimize dialog policy base thegraph foster coherent dialog.to propose novel conversational graph ground policy learning frame1836work open-domain multi-turn conversationgeneration cg-policy consist keycomponents capture localappropriateness global-coherence information reinforcement learning base policymodel learn leverage foster amore coherent dialog figure give user message system select what-vertex sleepy how-vertex respond mechanism toproduce coherent response.1we first construct base dialog data.we vertex represent utterance content andedges represent dialog transition utterance specifically type vertex what-vertex contain keyword ahow-vertex contain respond mechanism multi-mapping base generator section3.1 capture rich variability expression wealso multi-mapping base method buildedges what-vertices capture thelocal-appropriateness keywordsas message response respectively beseen what-vertices highlyconnected region likely constitute coherent dialog.we present novel graph ground policymodel plan long-term success orient vertexsequence guide response generation specifically illustrate three pink line figure1 give user message cg-policy first link itskeywords obtain what-vertices next policy model learn select what-vertexfrom one-hop what-vertex neighbor whatvertices select how-vertex howvertex neighbor chosen what-vertex finally select vertex utilized guideresponse generation thus leverage priordialog-transition information graph edge tonarrow candidate response content moreeffective policy decision instead wholeset keywords candidate action moreover facilitate modeling long-term influenceof policy decision ongoing dialog firstpresent novel base reward well measure long-term influence select actions.we employ graph attention mechanism andgraph embed encode global structure information dialog state representation enable global information aware decisions.1each mechanism network model toexpress response content chen paper make following contribution work first attempt representsdialog transition graph conductsgraph ground policy learn supported policy learn framework cg-policy respond well termsof local appropriateness global coherence.• study show one-hop whatvertex neighbor what-vertices providelocally-appropriate diverse response content base reward supervise policy model promote globallycoherent dialog how-verticesin improve response diversity thecg help system succeed task oftarget-guided conversation indicate itgives control dialog policy.2 related workpolicy learn chitchat generation address degeneration issue word-level policymodels zhang previous work decouple policy learn response generation utterance-level latent variable zhao keywords yaoet action guide response generation work investigate priordialog-transition information facilitate dialogpolicy learning.knowledge aware conversation generationthere grow interest leverage knowledge base generation informative response dinan ghazvininejad al.,2018 moghe zhou liuet inthis work employ dialog-modeling orientedgraph build dialog corpus instead external knowledge base order facilitate multi-turnpolicy learning instead dialog informativenessimprovement.specifically motivate al.,2020 method theissue cross-domain transfer since rely onlabor-intensive knowledge graph ground multiturn dialog datasets model training comparedwith conversational graph automatically build dialog datasets introducesvery cost train data construction furthermore decouple conversation model intotwo part modeling to1837nlgcg-policy input outputmessage subgraphsmessage+ historykeywords keyword respondingmechanismdialog corpusresponsetransebasedgraphembeddingspolicyreward/ what-vertices '611 constructionconversational graph …figure architecture cg-policy thatconsists state/action policy wefirst construct conversational graph dialog corpus.then train cg-policy upper-right partshows detail input/output module.say modeling reasonable adjust what- part transfer different domainswhich reduce domain transfer cost.3 approachthe overview cg-policy present figure2 given user message obtain candidate action module attempt retrieve contextually relevant subgraphs state/actionmodule maintain candidate action history keywords select policy previous turn ormentioned user message policymodule learn select response keyword aresponding mechanism subgraphs.the module first encode message intoa representation message encoder theselected mechanism employ seq2bfmodel2 produce response2it decode response start input keyword generate remain previous future word subsequently keyword appear response.xmessage encodermlp respondingmechanism selectedby policymessageresponserepresentationrresponseseq2bf baseddecoderthe figure multi-mapping base generator nlgin seq2bf base model al.,2016 decoder.with representation select keyword input model construction/policy/nlg/reward train separately.3.1 background multi-mapping generatorfor nlgto address one-to-many semantic mapping problem conversation generation chenet propose end-to-end multi-mappingmodel respond mechanism amlp network model express responsecontent respond specific sentencefunction test procedure randomly selecta mechanism response generation.as show figure generator consist base message encoder respondingmechanisms decoder first give dialogmessage message-encoder represent avector second generator respond mechanism select policy convert xinto response representation finally akeyword select policy feed decoder response generation ensure thegiven keyword appear generated response introduce another seq2bf base decoder mouet replace original decoder.moreover generator train dataset withpairs message keyword extract aresponse response.33.2 constructiongiven dialog corpusd construct withthree step what-vertex construction how-vertexconstruction edge construction.3if multiple keywords extract response randomly choose keyword exist theresponse randomly sample word response toserve keyword construction extract contentwords fromd what-vertices rule-basedkeyword extractor obtain salient keywords fromutterances remove stop word weobtain keywords what-vertices.how-vertex construction obtain ofnr respond mechanism generator describe section howvertices notice how-vertices cgshare respond mechanisms.edge construction type edgesin join what-vertices theother join what-vertex how-vertex.to build first type edge first constructanother dataset consist keyword pair pair consist keywords extract message response respectively capture natural transition betweenkeywords train another multi-mapping basedmodel dataset.5 what-vertexvw find appropriate keywords responsesby select five keywords decode decodinglength respond mechanism andthen connect vertex keywords.to build second type edge message-keyword pair describedin section ground-truth responseto select suitable mechanism keyword give what-vertex select topfive mechanism frequently select forvw keyword build edge connect vwto rank how-vertices edgeslead respond mechanism suitable togenerate vw.3.3 nluto obtain subgraphs provide high-quality candidate action first extract keywords lastutterance context message sametool construction link keyword exact string matching toobtain multiple what-vertices retrievea subgraph keyword vertex exclude what-vertices subgraphs candidate action subgraph consist threeparts what-vertex one-hop neighboring4github.com/squareroot3/target-guided-conversation5we ever method edge construction e.g. finally find method canprovide diverse response keyword candidate pmitends provide high-frequency keyword candidate weuse base decoder replace seq2bf.0 prepare dataset pretrained embedding.1 construct what-vertex train multi-mapping base generator responding mechanism constitute how-vertex set.3 construct edge what-vertices what-vertex how-vertex train scoring model local relevance train transe base embedding pagerankscores what-vertices calculate short path distance betweenany what-vertices train original multi-mapping base rnndecoder user-simulator optimize policy reinforcement learning whereparameters module stay intact table training procedure cg-policy.what-vertices how-vertices connect tothe neighbor keywords beextracted message link reuse retrieved subgraphs last time.6thus leverage provide high-qualitycandidate action instead whole setof candidate previous work al.,2018 state/actionthis module maintains candidate action historykeywords select policy mentionedby user message moreover themessage-encoder section represent themessage vector theresponding mechanism section convert candidate response representation nrj=1 policy.3.5 policystate representation state representation stat t-th time step obtain concatenate amessage representation history keywordsrepresentation encode rnnencoders respectively formally enable global information aware policy decision employ graph attention mechanismand graph embed encode global structureinformation state representation.recall subgraph keyword message obtain hereeach subgraph consist what-vertex,6if encounter case first time step whatvertices what-vertices contain top-5 highfrequency keywords d.1839its what-vertex neighbor remove howvertices edge formally ngik=1 triple withτk headk relk tailk numberof triple keywords message null subgraph used.then calculate subgraph vector aweighted head vector tail vector inthe triples.gi =ngi∑k=1αk eheadk etailk =exp ∑ngim=1 etrelktanh wheheadk wtetailk represent pretrained graph embed transe bordes updatedduring training parameters.smt obtain recursively feed concatenated vector vanilla unit model parameter embeddingof keyword thus encode globalgraph structure information state representation enable global-information aware policymodel moreover calculate similar way.policy decision decision consist twosequential sub-decisions first what-policy select what-vertex candidate what-vertices how-policy select how-vertex fromhow-vertex neighbor select what-vertex.with state representation whatpolicy µwhat define µwhat =exp actl=1 model parameter different fromboth embedding j-th candidate what-vertices number ofcandidate what-vertices.the how-policy µhow define µhow ∑nrj=1 candidate response representation inthe state module mechanism mask isset i-th responding mechanism ofneighbors select what-vertex otherwise rewardsfollowing previous work consider theseutterance-level reward local relevance state-of-the-art multiturn response selection model dualencoder lowe calculate local relevance.repetition repetition penalty generate response share word withany contextual utterance otherwise similarity target-guided conversation calculate cosine similarity thechosen keyword target word pretrainedword embed space target similarity.7to leverage global graph structure information facilitate policy learning proposethe follow reward global coherence calculate average cosine distance choose what-vertex andone history what-vertices select mentionedpreviously transe base embed space alsoused equation coherence reward.sustainability reasonable promote whatvertices large number neighbor generate sustainable coherent diverse dialogs.for reward calculate pagerank score calculate full chosen whatvertex.shortest path distance target targetguided conversation choose what-vertex iscloser target what-vertex term shortestpath distance compare previouslychosen what-vertex reward ifthe distance change otherwise define final reward aweighted above-mentioned factor weight factor default.8 rewardscan fully leverage dialog transition information intraining data utterance base reward e.g. local relevance also graph basedrewards e.g. coherence sustainability policy optimizationto make train process stable employthe method sutton barto optimization moreover update policy pa7if keyword choose baseline model calculate target similarity word response select theclosest one.8we optimize value weibo dataset grid search.the weight third/sixth factor defaultbecause propose target-guided conversation.1840rameters parameter module stayintact training.3.8 nlgas describe section mechanismselected how-policy convert responserepresentation keyword theselected what-vertex seq2bf decoder response generation.4 experiments results94.1 datasetswe conduct experiment widely opendomain dialog corpora.weibo corpus shang alarge micro-blogging corpus data cleaning obtain million pair training validation pair test weuse publicly-available lexical analysis tools10 obtain feature dataset wefurther feature extract keywords utterance tencent embedding11forembedding initialization models.persona dialog corpus zhang crowd-sourced dialog corpus eachparticipant play part assigned persona.to evaluate policy controllability bring cgpolicy conduct experiment target-guidedconversation persona dataset tang training validationset test contain utterance respectively embeddings initializedwith glove pennington graph constructed onweibo corpus contain what-vertices and74,362 edge among what-vertices edge evaluate suitable chat bythree human annotators.12 constructed cgon persona corpus contain what-verticesand edge among what-vertices edge evaluate suitable chat threehuman annotators.4.2 methodswe carefully select three sota method focuson dialog policy learning baselines.9please supplemental material details.10ai.baidu.com/11ai.tencent.com/ailab/nlp/embedding.html12we randomly sample edge evaluation.larl latent variable drive dialog policymodel zhao releasedcodes choose multivariate categorical latent variable action since perform thebest target-guided conversation implement another model larl-target addthe target similarity factor reward andits weight grid search.chatmore implement keyword drivenpolicy model follow theiroriginal design target-guided conversation weimplement chatmore-target target similarity factor reward itsweight grid search.tgrm retrieval base model targetguided conversation keyword chosenat turn must move strictly closer embed space give target word tang al.,2019 target-guided conversation thecodes release original author denote astgrm-target kernel versionsince perform best.13 suit task ofopen-domain conversation weibo removethe unnecessary constraint keyword similaritywith target word denote tgrm.cg-policy system present section3 target-guided conversation implementanother system cg-policy-target usean additional feature short path distanceto target factor augment original whatvertex representation what-policy µwhat.formally v̄wj v̄wj theaugmented representation weight matrix embed distance value v̄wj size also usethis factor reward estimation weight isset grid search target similarity factor moreover samedialog corpus construct train user simulator reward function module forcg-policy.4.3 user simulatorwe user simulator training oflarl chatmore cg-policy user simulator original multi-mapping base generatorwith decoder pretrained dialog corpus update policy training.please refer chen details.during testing system share simulator.13github.com/squareroot3/target-guided-conversation18414.4 evaluation settingsconversation user simulator following previous work tang weuse user simulator play role human andlet model converse given arandomly select model randomly select anutterance utterance startingposition session test model tostart conversation moreover maximumallowed number turn experiment finally collect model-simulatordialogs evaluation single-turn level evaluation randomly sample message-responsepairs dialog model.conversation human following previouswork tang also perform humanevaluation reliable system comparison.given model evaluate randomly selecta dialogue test pick first utterancefor model start conversation human conversation continue till reach finally obtain dialogsfor evaluation single-turn level evaluation werandomly sample message-response pair fromthe dialog model.4.5 evaluation metricsmetrics bleu perplexity beenwidely dialog evaluation serban widely debate howwell automatic metric correlate withtrue response quality since theproposed system predict thehighest-probability response turn ratherthe long-term success dialog e.g. coherence employ bleu perplexity evaluation propose following metrics.4.5.1 multi-turn level metricsglobal coherence define incoherence problem follow inconsistent dialogs wherethe model contradicts e.g. modelsays driver adoctor one-side dialog modelignores user topic consecutive turn session containsmore three incoherence case asession contain case otherwise metric dist-i calculate ratio ofdistinct i-gram generated response al.,2016a dist-2 measure diversity ofgenerated responses.methods cohe dist-2 appr infor.larl results dialog simulator weibo.dialog-target success rate target-guidedconversation measure success rate generate target word within turns.4.5.2 single-turn level metricslocal appropriateness14 response inappropriate reply givenmessage otherwise response safe response know otherwise evaluation results4.6.1 settingwe three annotator judge quality ofeach dialog multi-turn level utterance pair single-turn level model notice thatmodel identifier mask evaluation.4.6.2 conversation simulatoras show table cg-policy significantly outperform sign test p-value baseline interms global coherence local appropriateness indicate effectively facilitate policy learning ablation study analysis larl single-turn responsequality model might beexplained latent variable finegrained enough provide sufficient information toguide response generation chatmore tend select high-frequency generic keywords resultingin performance term dist-2 tgrmperforms best term dist-2 informativeness indicate retrieval-based model canproduce diverse response generationbased model consistent conclusion previous work chen zhanget however tgrm perform worstin term coherence since tgrm userl framework indicate importance rlframework multi-turn dialog modeling herethe kappa value inter-annotater agreement isabove indicate moderate agreement.14we consider response appropriate forthe select respond mechanism.1842methods cohe dist-2 appr infor.larl results dialog human weibo.4.6.3 conversation humanas show table cg-policy outperform baseline term global coherence localappropriateness sign test p-value whichis consistent result table kappavalue indicate moderate agreement.4.6.4 ablation studywe conduct ablation study cg-policy onweibo corpus investigate cg-policy performs well first evaluate contributionof remove cg-policy denote cg-policy-nocg usegraph structure information action space pruning reward design moreover attempt touse without how-vertices augment thechatmore model action space pruning reward design denote chatmore-cg shownin table performance cg-policy-nocgdrops dramatically term coherence dist-2and appropriateness compare originalmodel moreover boost performanceof chatmore term metric indicate crucial superiorperformance cg-policy also helpother model e.g. chatmore second evaluatethe contribution action space prune orreward design respectively implement system variant what-vertices cgas action candidate turn denote cgpolicy-nocgact remove cg-basedfactors reward denote cg-policynocgrwd show table performanceof cg-policy-nocgact drop significantly termsof dist-2 tend select high-frequency keywords like chatmore indicate importance ofgraph path provide locally-appropriate anddiverse response keywords moreover performance cg-policy-nocgrwd drop significantlyin term coherence indicate base reward effectively guide cg-policy promotecoherent dialog third remove how-verticesfrom denote cg-policy-nocghow asshown table how-vertex removal hurt permethods cohe dist-2 appr infor.cg-policy ablation study cg-policy weibo.formance dist-2 indicate importance ofhow-vertices response diversity.4.7 task target-guided conversationbesides maintain coherence ground policy learning enable control dialogmodels important achieve certain goalsfor chatbot proactive lead certain chat topic keywords certain products.4.7.1 settingfollowing setting tang wherewe randomly sample keyword target wordfor session test procedure usea multi-mapping base user simulator train onthe persona dataset evaluation.methods succ cohe appr infor.larl-target results target-guided dialog persona.4.7.2 resultstable present result dialog eachmodel cg-policy-target significantly outperform baseline term dialogtarget success rate sign test p-value itcan cg-policy successfullylead dialog give target word learn towalk indicate graph givesus control policy larl-target andchatmore-target perform badly term success rate explain lack theability proactive dialog content planning.4.8 analysis responding mechanismsfigure provide representative word eachmechanism.15 example mech-1 keywords mainly subjective word think for15we select word occur frequently response guidedby mechanism rarely occur mechanisms.1843generation response respect personalopinion intention mech-2 tend respond specific type mood.mech-1 mech-2 mech-3 mech-4 mech-5以为think哈哈haha哪where漂亮beautiful别no想want哇wow什么what可爱cute还是or else信believe好吧alright ？萌cuddly没有nofigure representative word respond mechanisms.5 conclusionin paper present novel graph groundedpolicy learn framework open-domain multiturn conversation effectively leverageprior information dialog transition fostera coherent controllable dialog experimental result demonstrate effectiveness ofthis framework term local appropriateness global coherence dialog-target success rate inthe future investigate extend thecg support hierarchical topic management inconversational systems.acknowledgmentswe grateful support zeng atthe initial stage work also thank theanonymous reviewer helpful commentsand suggestion work support thenational research development projectof china no.2018aaa0101900 nationalnatural science foundation china nsfc viagrant
automatic dialogue response evaluator hasbeen propose alternative automatedmetrics human evaluation however exist automatic evaluator achieve moderate correlation human judgement andthey robust work propose build reference-free evaluator andexploit power semi-supervised training pretrained mask language model experimental result demonstrate thatthe propose evaluator achieve strongcorrelation human judgement generalize robustly diverse response corpus open-source thecode data http //github.com/zhaoting/dialog-processing.1 introductionevaluation conversational system beenone major obstacle dialogue research particularly open-domain dialogue automate metric show correlate poorly human judgement although human evaluation provide accurate assessment slow expensive alternative train evaluator learn predict human-like score lowe propose adem supervised regression model forautomatic response evaluation report spearman correlation withhuman judgement though good automatedmetrics score indicate moderate correlation another criticism point adem produce score oflow deviation lack robustness adversarial attack.an ideal evaluator precise thatits prediction strong correlation human judgement also robust suchthat generalize dialogue unseen training explore three method improve precision robustness responseevaluators propose build referencefree evaluator since reference-dependent metricscause problem deviation describe bysai also find referencedependent evaluator performance degrades significantly remove ground-truth responsesfrom test data propose unsupervised model ruber outperform supervise adem train next sentenceprediction task show rubercan improve supervised trainingon small amount annotate data wemake strong pretrained model asroberta obtain good textrepresentations combine three method reference-free semi-supervised robertabased evaluator good correlation robustness experimental result also show themodel maintain good performance crossdomain low-resource settings.2 related worksautomatic response evaluator first proposedby lowe mimic human annotator sassessment response appropriateness collect human annotation response quality for4,104 context-response pair train regression network adem supervisedly minimize squared error propose anunsupervised method ruber train automaticevaluators model optimize distinguish ground-truth response negativesampling response minimize margin rankloss process resemble next sentenceprediction task apply training ofbert devlin allow exploit27ing large amount conversation data hasbeen show outperform adem using ademand ruber baseline work willanalyze shortcoming develop solutionsto build precise robust evaluators.next sentence prediction predict whethera sentence true continuation give precede context positive sample theground-truth subsequent sentence negativesample different piece text benefit evaluation alsolanguage understanding devlin andlanguage generation bruni fernandez wolf response evaluation also improve well automate metric approximation response quality examples successful attempt improve automated metric include exploit multiple reference comparison gupta combine humanjudgement automated metric hashimotoet demonstrate thatsingle-turn human judgement reliable expect propose multi-turn human evaluation.ghandeharioun approximate sentiment semantic similarity engagement withnew automate metric hybrid metric multi-turn evaluation setting dziri show entailment also option approximate dialogue coherence quality.3 backgroundadem regression model take inputsa dialogue context vector hypothesis responsevector reference response vector itsoutput referenced metric anunreferenced metric ademref ademunref encoding vector produce pretrained encoders trainable parameters.ruber also combine metric compute differently ruberref r̂‖r‖ ‖r̂‖ ruberunref denote concatenation vectorsand multi-layer perceptron nonlinear activation function trainable parameters.besides difference metric computation different training strategy adem usessupervised training minimize mean squareerror prediction human score whileruber unsupervised training nsptask minimize margin ranking loss section combine advantage build good response evaluator.4 data collectionfor assess dialogue response evaluator wesample dialogue test split thedailydialog corpus contain open-domain human-written conversation expand extra responsehypotheses collect human annotation response quality.collection extra responses besides theground-truth response response different source dialogue context include negative-sampling response randomly select different dialogue responsesgenerated generative model train thetraining split combine generative model sutskever attentional hred serban vhred serbanet gpt2-sm gpt2-md wolfet decode method greedydecoding ancestral sampling nucleus sampling holtzman result response pool dialogue context contain various qualities.collection human annotations the2,000 dialogue-response pair select ofthem amazon mechanical turk workersto rate response appropriateness likert scale pair four worker afterremoving annotation outlier pair leyset remain data reach goodreliability regard inter-annotator agreementwith krippendorff krippendorff,2018 make split annotate data training validation test.figure show overall distribution score response appropriateness and1more detail inter-annotator agreement outlier removal provide appendix a.281 response overall score distribution.gt gpt2_md gpt2_sm hred vhred s2s_attn ns12345appropriateness plot score response source ground-truth negative-sampling.figure distributions human annotation response appropriateness test data excluding ground-truth response response pearson spearson pearson spearson sdademfull comparison reference metric unreferenced metric full test data ground-truthresponse-excluded test data §5.1 short standard deviation denote score p-values denotes score p-values show plot human score fordifferent response source distribution suggest create data consists diverse responses.5 methodology5.1 reference-free evaluationsai prove theoretically comparison reference response referencedmetric cause adem make conservative prediction score standarddeviation investigate effect removingreference computation experiment withthe full adem ruber well referenced unreferenced version show intable referenced metric adem ruber much standard deviation thanhuman score adem unreferenced metric haslow score correlation standard deviation full adem model heavily affect reference metric unreferenced metric fully utilized especially thedata include ground-truth responses.another important finding referencedmetrics correlation degrade significantly whenwe remove ground-truth response testdata suggest reference metric helpevaluators distinguish ground-truth responsefrom non-ground-truth response easily theycannot distinguish good response oneamong non-ground-truth responses.based result propose buildreference-free evaluator avoid direct comparison reference response improve robustness diversity.5.2 semi-supervised trainingadem supervised model rely humanannotations however expensive collectlarge-scale annotated data hand ruber show reach reasonable correlation score unsupervised training annsp task natural idea apply unsupervisedtraining first finetune evaluator a29model training datarubersup comparison original unsupervisedruber semi-supervised ruber §5.2 andspr short pearson correlation spearman scorrelation respectively.relatively small amount annotate data takingruber example finetuning ruber on720 annotate sample improve pearson scorrelation spearman correlation powerful text encoderall metric mention base encode vector powerful text encoder essential build good evaluator.adem ruber initialized pretrained response generator alternative pretrained mask language model asbert devlin roberta liuet powerful text encoderand benefit downstream task natural language processing huang lanet joshi shimanaka al.,2019 choose roberta-large build response evaluator.a roberta evaluator produce encodingvector give context response andthen finally calculate score asigmoid function rescale score matchannotator scale roberta roberta-eval ·mlp roberta parameter parameter optimize training.6 experimental evaluationstable show correlation score standard deviation four metric group firstgroup automate metric base ngram overlapping bleu-2 word embeddingsimilarities average extrema greedy thesecond group baseline adem ruber third group semi-supervised fullruber model semi-supervised unreferencedruber model roberta-based evaluatormodel sdautomated metricsbleu-2 evaluatoradem evaluatorrubersemi-sup judgementhuman performances automated metric baselineevaluators propose evaluator combine three propose method human score give final group semisupervised training yield improvement correlation abandon referenced metric makespredictions less conservative roberta evaluator outperform baseline large marginand much human-like score diversity.6.1 transferability studywe interested apply trained responseevaluator data different domain orstyles therefore carry experiment tostudy transferability roberta evaluator addition dailydialog corpus collect annotation responsesfrom personachat corpus zhang al.,2018 follow procedure section evaluator turn generalize corpus much well baseline ruber accord result table evaluator trainedon corpus achieves even high correlation score apply corpus however performance degradation observe whenapplying evaluator train corpus tothe corpus suggest makea careful choice training data plan toevaluate model different corpora.6.2 resource studyalthough annotated sample inthe experiment explore possibility30corpus correlationtrain test spr.roberta evaluatordd correlations roberta evaluator ruber training test data different corpus §6.1 amount0.500.550.600.65correlationpearsonspearmanfigure performance roberta evaluator w.r.tamount supervised training data §6.2 train even data figure showsthat around sample robertaevaluator reach performance close resultobtained entire samples.6.3 robustness evaluationin section address requirement towards robust evaluator.1 heavily influence referenceresponse propose evaluator entirely independent references.2 generalizing diverse response remove ground-truth test data theroberta evaluator still achieve pearson scorrelation spearman correlation evaluator achieve good performance diverse response different corpus see§6.1 sensitivity grammar relevanceof response also collect annotationsfor relevance grammatical correctness theroberta evaluator train appropriatenessannotations achieve pearson correlation relevance annotation correlation score grammatical correctness however understandable response perfect grammar still inappropriate certaincontext grammar highly correlatedwith appropriateness.24 robust fool attack unlikein find magicresponses fool evaluator outputhigh score constantly.7 conclusionautomatic dialogue response evaluator haveproblems robustness correlation human judgement investigate three method toalleviate reference-free metric apply semi-supervised training exploit powerful pretrained text encoders experimental result demonstrate propose evaluator achieve strong correlation humanjudgement show robustness deal withdiverse response domain alsobe train efficiently less annotatedsamples.acknowledgmentsthe author would like thank shinsuke morifrom kyoto university microsoft graham neubig anonymous reviewer constructive comments.this work support erato ishiguro symbiotic human-robot interaction program grant number jpmjer1401 japan
legal judgment prediction taskof automatically predict case judgment result give text describe fact excellent prospect judicial assistance system convenient service thepublic practice confuse charge frequent case applicable similarlaw article easily misjudge address issue exist method relies heavily domain expert hinder application different system paper present end-to-end model ladan tosolve task distinguish confusingcharges propose novel graph neural network automatically learn subtle differencesbetween confuse article design anovel attention mechanism fully exploit learned difference extract compellingdiscriminative feature fact description attentively experiments conduct realworld datasets demonstrate superiority ofour ladan.1 introductionexploiting artificial intelligence technique assist legal judgment become popular recentyears legal judgment prediction topredict case judgment result applicable article charge term penalty base fact description illustrate figure assist judiciary worker processing case offer legal consultancy service tothe public literature usually formulate text classification problem severalrule-based method al.,2012 neural-based method zhong beenproposed.the main drawback exist method thatthey solve confusing charge issue.∗corresponding authors.judgment resultsfact descriptionlaw articleschargesterms penaltyat october defendant zhao andzhang altercation zhao beat zhangand cause injury identification injury ofbilateral nasal bone fracture zhang minorinjuries grade ii……law article crime intentional injury whoeverintentionally injure another person shall sentence tofixed-term imprisonment three year criminal detention public surveillance……crime intentional injurya fixed-term imprisonment monthsfigure illustration generally judgeneeds conduct professional analysis reasoningon fact description case choose reasonable article charge term penalty toconvict offender.that high similarity several article corresponding case easilymisjudged example figure article385 article describe offense accept bribe subtle difference whetherthe guilty party state staff tosolving confusing charge issue capture essential rare feature distinguishingconfusing article define tendiscriminative attribute distinguish confusingcharges however method relies muchon expert hinder application largenumber practice desire methodthat automatically extract textual feature fromlaw article assist relevant exist work requirement attention mechanism extract feature fact description respect specific article show figure eachlaw article attention vector compute whichis extract feature fact descriptionof case predict whether article isapplicable case nevertheless weakness3087any state staff take advantage position demand money orproperty another person illegally accept another person'smoney property return secure benefit person shall beguilty acceptance bribes.article crime acceptance bribeswhoever order seek illegitimate benefit give state staff withmoney property shall crime briberyarticle crime offer bribeswhoever order seek illegitimate benefit give employee ofcompanies enterprise unit money property shall beguilty bribe non-state staffs.article crime offer bribe non-state staffthe employee company enterprise unit takingadvantage position demand money property anotherperson illegally accept another person money property returnfor secure benefit person shall guilty bribery crime nonstate staffs.article bribery crime non-state staffsfigure examples confuse charges.is learn article attention vectorindependently result similarattention vector learn semantically closelaw article hence ineffective distinguish confuse charges.to solve confusing charge issue propose end-to-end framework i.e. articledistillation base attention network ladan difference among similar article attentively extract feature case fact description effective indistinguishing confuse article improvethe performance obtain differenceamong similar article straightforward wayis remove duplicate text article leftover text attentionmechanism however find methodmay generate leftover text different article generate misleading informationto show remove duplicated phrase sentence article article i.e. text andbetween article article i.e. pinktext respectively article andarticle almost i.e. blue text design ladan base following observation usually easy distinguish dissimilar article sufficient distinction exist challenge discriminate similar articlesdue useful feature first group lawarticles different community article community highly similar toeach propose graph-based representation learn method automatically explore difference among article coma1a2an bfactdescriptionan-1an-2factdescriptionan-2anan-1 αnαn-1αn-2α1α2α3 a2a1a4 a3a3at-1atat+1community mcommunity m√βmcommunity matchingattention computationfigure fact-law attention model al.,2017 framework variables representthe encoded vector learn attentively extractingfeatures fact descriptions.pute attention vector community foran input case learn macro- microlevel feature macro-level feature forpredicting community include applicable article micro-level feature attentively extract attention vector selectedcommunity distinguish confuse article within community main contribution summarize follow develop end-to-end framework i.e. ladan solve task address theconfusing charge issue mine similarity fact description article well asthe distinction confuse article propose novel graph distillation operator extract discriminative feature foreffectively distinguish confuse article conduct extensive experiment realworld datasets result show modeloutperforms state-of-the-art methods.2 related workour work solve problem confusingcharge task refer calculation principle graph neural network section introduce related work aspects.2.1 legal judgment predictionexisting approach legal judgment prediction mainly divide three category inearly time work usually focus analyze exist legal case specific scenario mathematical statistical algorithm kort nagel keown lauderdale clark,2012 however method limit small datasets label later number of3088machine learning-based method sulea develop solve problem almost combine manually design feature linear classifier improve performance caseclassification shortcoming method rely heavily manual feature sufferfrom generalization problem.in recent year researcher tend exploit neural network solve task propose hierarchical attentional network capture relation fact description relevant article improve charge prediction.zhong model explicit dependency among subtasks scalable direct acyclicgraph form propose topological multi-tasklearning framework effectively solve thesesubtasks together yang refinethis framework backward dependenciesbetween prediction result subtasks thebest knowledge thefirst study problem discriminate confusing charge automatically predict applicable charge manually define discriminative attribute propose enhance representation case fact description learningthese attribute method rely much onexperts easily extend different system solve issue propose anovel attention framework automatically extract difference similar article toenhance representation fact description.2.2 graph neural networkdue excellent performance graph structure data attract significant attention kipf welling hamilton al.,2017 bonner general exist gnns focus propose different aggregation scheme fuse feature neighborhood node graph extract rich comprehensive information kipf propose graph convolution network mean pooling pool neighborhood information graphsage hamilton al.,2017 concatenate node feature appliesmean/max/lstm operator pool neighborhoodinformation inductively learn node embeddings mr-gnn aggregate themulti-resolution feature node exploit node information subgraph information global information together besides message passing neural networks gilmer furtherconsider edge information aggregation however aggregation scheme lead tothe over-smoothing issue graph neural network i.e. aggregated node representation would become indistinguishable whichis entirely contrary goal extract distinguishable information paper propose distillation operation base distillation strategy instead aggregation scheme toextract distinguishable feature similar articles.3 problem formulationin section introduce notation andterminologies formulate task.law cases case consist fact description several judgment result figure fact description represent atext document denote judgment result include applicable article charge term penalty assume kindsof judgment result i-th judgment resultis represent categorical variable whichtakes value case berepresented tuple articles case often analyze andadjudicated accord legislature statutorylaw also know write formally wedenote statutory articlesl number lawarticles similar fact description case wealso represent article document.legal judgment prediction paper weconsider three kind judgment result applicable article charge term penalty.given training datasetd qz=1of size train model canpredict judgment result test casewith fact description ftest i.e. ftest following zhong yang assume case applicable article.3089graph distillation operatorwy zxwy zxgraphconstructionlayerlaw-similarity graphs adjacency matricesgdo gdogdosubgraph selectiong2law elaw glaw flaw articlesfact re-encodemoduleconcatlaw distillationmoduley1y2y3law article predictioncharge predictonterm penaltypredictionmulti-tasklearningframeworkpooling factdescriptionflaw wlaw ylaw xlaw βmdistinction vector ablaw alaw claw blaw dbasic encodermodulefigure overview framework ladan take fact description case text definitionsof article input extract basic representation distinguish representation factdescriptions basic encoder re-encoder finally combine representation thedownstream prediction task distillation module module communize article distills thedistinguishable feature community attention calculation re-encoder.4 method4.1 overviewin framework ladan factdescription case represent part abasic representation denote distinguishable representation denote basic representation contain basic semantic information match group article thatmay apply case contrast distinguishable representation capture feature caneffectively distinguish confuse article theconcatenation feed subsequentclassifiers predict label task.as mention easy distinguish dissimilar article sufficient distinction exist difficulty solve confuse chargeslies extract distinguishable feature similar article obtain basic representation therefore popular document encode method e.g. encoder kim,2014 bi-rnn encoder yang learn distinguishable representation weuse distillation module first divide article several community ensure lawarticles community highly similar andthen extract community distinction vector distinguishable feature basic representation article community giventhe case fact description community distinction vector select relevant i.e. attentively extract thedistinguishable feature fact re-encodemodule follow elaborate distillation module fact re-encode module respectively.4.2 distilling articlesa case might misjudge high similarity article alleviate problem design distillation module extract distinguishable representative information article specifically firstuses graph construction layer dividelaw article different community eachlaw article community graph distillation layer isapplied learn discriminative representation hereinafter call distinction vector.4.2.1 graph construction layerto find probably confuse article firstconstruct fully-connected graph lawarticles weight edge betweena pair article define as3090the cosine similarity tf-idf term frequency-inverse document frequency representation since confusinglaw article usually semantically similar andthere exist sufficient information distinguishdissimilar article remove edge withweights less predefined threshold fromgraph appropriate obtain graph mi=1 compose several disconnect subgraphs community containsa specific community probably confuse article late experimental result demonstratethat easy-to-implement method effectively improve performance ladan.4.2.2 graph distillation layerto extract distinguishable information fromeach community straightforward todelete duplicate word sentence present inlaw article within community describedin addition introduce significanterrors simple method plug end-to-end neural architecture nondifferentiability overcome issue inspire popular graph convolution operator kipf welling hamiltonet veličković proposea graph distillation operator effectivelyextract distinguishable feature different compute message propagation neighbor aggregate message toenrich representation node graph thebasic idea behind learn effectivefeatures distinction remove similar feature nodes.specifically arbitrary article trainable weight matrix capturesimilar information neighborsin graph matrix extract effective semantic feature layer aggregation similar information andits neighbor remove representation li−∑lj∈niψ |ni|+ refers representation oflaw graph distillation layer refersto neighbor graph thebias rdl+1×dl rdl+1×2dlare trainable self weight matrix theneighbor similarity extract matrix respectively.note dimension feature vectorin graph distillation layer dimension basic representationsvbf similar also support multi-layer stacking.using layer output article representation last layer i.e. ∈rdh contain rich distinguishable featuresthat distinguish article articleswithin community improvelaw article distinguishable feature subgraph graph computeits distinction vector pool operatorsto aggregate distinguishable feature articlesin formally compute lj∈gi lj∈gi element-wisemax pooling element-wise pooling operator respectively.4.3 re-encoding fact distinguishableattentionto capture case distinguishable featuresfrom fact description firstly define thefollowing linear function predictits related community graph softmax wgvbf basic representation fact description rm×ds arethe trainable weight matrix bias respectively.each element reflect thecloseness fact description article community relevant communitygĉ compute asĉ maxi=1 mx̂i.then corresponding community distinction vector attentively extract distinguishable feature fact description yang attentively extract distinguishable feature base wordlevel sentence-level bi-directional gated recurrent units bi-grus specifically input sentence wi,1 fact description word-level bi-grus output hidden3091state sequence −−→gru ←−−gru represent word embed ofword wi.j based hidden state sequence distinction vector calculate attentive vector αi,1 evaluate discrimination ability word formally computedas =exp tanh wwhi wgwβĉ tanh wwhi wgwβĉ trainable weight matrix representation sentence sias =ni∑j=1αi denote word number sentence si.by word-level bi-grus weget sentence representation sequence vsnf refers number sentence fact description basedon sequence similarly build sentencelevel bi-grus calculate sentence-levelattentive vector reflect thediscrimination ability sentence andthen fact distinguishable representationvdf sentence-level bi-grus areformulated −−→gru ←−−gru =exp tanh wshi wgsβĉ tanh wshi wgsβĉ =∑iαihi.4.4 prediction trainingwe concatenate basic representation andthe distinguishable representation finalrepresentation fact description i.e. based generate corresponding feature vector ṽjf subtask mention i.e. articleprediction charge prediction term ofpenalty prediction obtain prediction foreach subtask linear classifier softmax wjpṽjf parameter specific tasktj training compute cross-entropy lossfunction subtask take loss ofall subtasks overall prediction loss −3∑j=1|yj |∑k=1yj denote number different class label task yj,1 yj,2 refers ground-truth vector task besides also consider loss article community prediction i.e. −λm∑j=1xj ground-truth vector community include correct article apply case summary finaloverall loss function experiments5.1 datasetsto evaluate performance method weuse publicly available datasets chineseai challenge cail2018 xiao al.,2018 cail-small exercise stage dataset andcail-big first stage dataset case sample datasets contain fact description applicable article charge term ofpenalty data processing first filter outsamples meaningful word tobe consistent state-of-the-art method filter case sample multiple applicablelaw article multiple charge meanwhile refer zhong keep thelaw article charge apply less than100 correspond case sample divide theterms penalty non-overlapping intervals.the detailed statistic datasets show intable baselines settingsbaselines compare ladan somebaselines including:1http //cail.cipsc.org.cn/index.html3092dataset cail-small cail-big training cases test cases articles charges term penalty statistics datasets.• cnn-based model withmultiple filter window width text classification.• harnn yang rnn-basedneural network hierarchical attentionmechanism document classification.• charge predictionmethod attention mechanism tocapture interaction fact description applicable laws.• few-shot discriminatingconfusing charge method extract feature predefined attribute factdescriptions enforce semantic information.• topjudge zhong topological multi-task learning framework forljp formalize explicit dependency subtasks directed acyclic graph.• mpbfn-wca yang multitask learning framework multiperspective forward prediction backward verification state-of-theart method.similar exist work zhong train baseline hlstm multi-task framework record select bestexperimental parameter accord rangeof parameter give original paper besides method ladan thesame multi-task framework i.e. landan+mtl ladan+topjudge ladan+mpbfn todemonstrate superiority feature extraction.experimental settings thulac tool word segmentation case sample chinese afterward skip-gram model mikolov pre-train word embeddings case document model embed size frequency threshold areset respectively meanwhile weset maximum document length word cnn-based model baseline themaximum sentence length word maximum document length sentence lstmbased model hyperparameters weset dimension latent state i.e. threshold inour method ladan graph distillation layer bi-gru randomly initialize attention vector adopt basicdocument encoder training learning rate adam optimizer batchsize train every model epoch choose best model validationset testing.25.3 experimental resultsto compare performance baselinesand method choose four metric thatare widely multi-classification task include accuracy macro-precision macro-recall macro-f1 sincethe problem confuse charge often occur category main metric thef1 score tables show experimental result datasets cail-small cail-big respectively method ladan perform thebest term evaluation metric becauseboth cail-small cail-big imbalanceddatasets focus compare f1-score objectively reflect effectivenessof ladan baseline comparedwith state-of-the-art mpbfn-wca ladanimproved f1-scores article prediction charge prediction term penalty prediction dataset cail-small and4.20 respectively and5.79 dataset cail-big meanwhile comparison multi-task framework i.e. topjudge mpbfn show ourladan extract effective feature fromfact description baseline meanwhile observe performance few-shoton charge prediction close ladan performance term penalty predictionis ideal predefinedattributes few-shot effective identify charge also prove robustness2our source code available http //github.com/prometheusxn/ladan3093tasks articles charges term penaltymetrics f1fla+mtl judgment prediction result cail-small.tasks articles charges term penaltymetrics f1fla+mtl judgment prediction result cail-big.of ladan high mr-scoresof ladan also demonstrate ability distinguish confuse article note method performance dataset cail-big well thanthat cail-small trainingset cail-big adequate.5.4 ablation experimentsto illustrate significance consideringthe difference article conduct ablation experiment model ladan+mtlwith dataset cail-small prove effectiveness graph construction layer webuild ladan model remove threshold i.e. table4 directly apply fullyconnected graph generate global distinction vector re-encoding fact description.to verify effectiveness graph distillationoperator build no-gdo ladanmodel i.e. table directly pool subgraph distinction vectorβi without gdos evaluate importance ofconsidering difference among article weremove ladan i.e. table i.e. article independently extract attentive feature fact description table wetasks charge penaltymetrics f1ladan+mtl ablation analysis cail-small.see effectively improvethe performance ladan critical limited performance article community obtain accurate remove accuracy ladandecreases harnn+mtl powerfully demonstrate effectiveness methodexploiting difference among similar articles.5.5 case studyto intuitively verify ladan effectively extract distinguishable feature visualize attention ladan encoders figure show case example article andarticle respectively darker theword high attention weight inthe correspond encoder i.e. information ismore important encoder basic encoder vital information thesetwo case similar contain the3094fact re-encoder basic encoder case example article crime non-state emploteesbasic encoder case example article acceptance bribesfact re-encoder figure attention visualization case example article article like position accept benefit accept cash therefore representation basic encoder predict acceptablelaw article charge term penalty case tend misjudge mentionedin distinction vector fact reencoder focus extract distinguishable feature like defendant identity information e.g. company manager work cadastralunit luocheng branch luohe city land andresources bureau example effectively distinguish applicable article andcharges cases.6 conclusionin paper present end-to-end model ladan solve issue confuse chargesin ladan novel attention mechanism propose extract feature distinguish confuse article attentively ourattention mechanism consider interaction fact description articlesbut also difference among similar article effectively extract graph neuralnetwork propose paper experimental result real-world datasets show ourladan raise f1-score state-of-the-art byup future plan study complicated situation case multipledefendants charges.acknowledgmentsthe research present paper supportedin part national program china shenzhen basic researchgrant jcyj20170816100819428 national natural science foundation china u1736205 moe-cmcc artificalintelligence project mcm20190701 nationalscience basic research plan shaanxi provinceof china national science basic research plan zhejiang province china lgg18f020016
goal-oriented dialogue system need tobe optimize track dialogue flowand carry effective conversation various situation meet user goal.the traditional approach building adialogue system take pipelined modular architecture module optimize individually however optimization scheme necessarily yieldan overall performance improvement thewhole system hand end-to-enddialogue system monolithic neural architecture often train input-outpututterances without take account entire annotation available corpus thisscheme make difficult goal-oriented dialogue system need integrate external system provide interpretable information systemgenerated particular response paper present end-to-end neural architecturefor dialogue system address challenge dialogue system achievedthe success rate language understand score responseappropriateness score human evaluation rank system position end-to-end multi-domain dialoguesystem task dialogue system technology challenge dstc8 introductionthe goal-oriented dialogue system help usersachieve goal request informationor execute command natural language conversation thus crucial dialogue systemto keep track dialogue flow carry aneffective conversation even user goalis complicate dialogue flow suddenlychanged.∗ equal contributionthe traditional approach build goaloriented dialogue system mostly adopt pipelinedmodular architecture natural language understanding module leeet first recognizes comprehendsuser intent extract value slot thedialogue state tracking module williamset track value slot thenthe dialogue policy module decidesthe system action finally natural language generation module generate utterance correspond tothe system action case multiple module combine together word-leveldst ramadan leeet dialogue history tothe dialogue state composite function nluand word-level budzianowskiet chen mehri zhao mapsthe previous utterance dialogue state system response composite function andnlg module usually optimize separately necessarily lead overall optimized performance successful task completion.on hand end-to-end neural model fordialogue system madotto al.,2018 enjoy straightforward training approach togenerating system response difficult forgoal-oriented dialogue system need tointeract external system generate explanation support system generate aparticular response.in paper present end-to-end neural architecture dialogue system address bothchallenges work base fine-tuninggpt-2 radford faithfully performthe follow essential dialogue management stepsin sequential manner single model restaurant restaurant serve proper british food town quite part town west possible three graffiti saint john chop house traveller give phone number address postcode graffiti phone number address hotel felix whitehouse lane hunthindon road post code cb30lx want book people please booking successful reference number name graffiti phone postcode cb30lx address hotel felix whitehouse lane hunthindon road food british area west name grafton hotel restaurant phone postcode cb580a address grafton hotel newmarket road ditton food british area east dialogue sng0689 goaldatabase restaurant dialogue turnsblue informable slot yello-green requestable slot name orange requestable slot valueinformable food british area west requestable phone address postcode book people single-domain example multiwoz dataset.dst predict dialogue state viapredicting system action retrieve appropriate record external database thedialogue state system action nlgvia predict system response result neural model generate systemresponse like end-to-end neural dialogue system also generate dialogue state systemactions intermediate output improve interpretability behavior dialogue system.in order achieve leverage annotationsof dialogue state system action provide inthe corpus multiwoz dataset budzianowskiet train system natural way.our model evaluate convlab leeet multi-domain end-to-end dialogsystem platform support various aspect thedevelopment evaluation dialogue system term automatic evaluation usersimulator human evaluation crowdworkers particularly human evaluation carry part dialogue system technology challenge dstc8 oursystem attain success rate thelanguage understand score theresponse appropriateness score rankingat place dstc8 also show thatour model competitive state-of-the-artmodels specialize sub-tasks dialogue management dialogue state trackingand dialogue-context-to-text generation task although model particularly tune forthose sub-tasks.the main characteristic model besummarized follow train followthe traditional dialogue management pipeline make monolithic neural model interpretableand easily integratable external system train end-to-end fashion simple gradient descent leverage gpt-2 apowerful pre-trained language model code isavailable github code repository.12 end-to-end multi-domaintask-completion taskbefore describe approach brieflyoverview end-to-end multi-domain taskcompletion task dstc8 wedeveloped dialogue system.2.1 multiwoz datasetthe multiwoz dataset large-scale fully annotate corpus natural human-human conversa1https //github.com/kaist-ailab/neuralpipeline_dstc8585restaurantinformname restaurant-name system actiondbquerycandidates query name frankie benny pricerange expensive area south food italian …database like find expensive place todine specifically serve italian food okay would like thecentre south part town would like south part townplease history restaurant pricerange expensivefood italianarea southdialogue statesystem action responsedialogue stateword decoder layertransformer decoder blocksdialogue history dialogue state system action responsegpt-2if empty query results ②③④response frankie bennys meet yourcriteria would like book query resultsreplacementresponse restaurant_name meet yourcriteria would like book ⑤⑥response restaurantmeets criteriacandidates queryno resultsrestaurant-nooffernone-nonesystem actionempty query results casenormal casefigure overview end-to-end neural dialogue model transformer fine-tuned gpt-2.the dash line represent information query invoke system actionneeds fetch actual value database.tions user tourist converse thesystem clerk across multiple domain eachdialogue rich annotation goal metadata dialog well user systemutterances annotation facilitate machine learning develop individual module adialogue system wordlevel word-level well end-toend dialogue system.figure show example single-domaindialogue multiwoz dataset dialogueconsists goal database dialogue turn goal define domain slots.the slot divide informable requestableand book slot informable slot represent userconstraints requestable slot hold additionalinformation user want obtain bookslots reserve place recommend bythe system.2.2 convlabfor evaluate dialogue system dstc8 convlab open-source platformthat support researcher train evaluate theirown dialogue system convlab contain implementation state-of-the-art model al.,2019b ramadan budzianowski end-to-end neural model dialogue system madotto whichare readily reusable build dialogue systemsusing various approaches.convlab also provide agenda-based usersimulator easily interact target dialoguesystem consist multi-intent language understanding milu rule-based policy template-based nlg.for dialogue goal randomly generatedthat conforms goal schema multiwoz dataset user simulator generate anagenda base goal interact with586⇒ dialogue state⇒ system action look place stay cheap price range type hotel okay specific area want stay metadata hotel semi name mention area mention park mention pricerange cheap star mention internet mention type hotel dialog_act hotel-request area need make sure cheap need park hotel name area park pricerange ⋯word-level input representation hotel-request area delimiter dialogue state domaindelimiter system action slot name-value pairssystem action intentfigure multiwoz dataset metadata treat dialogue state dialogue treatedas system action.the target dialogue system recognize systemdialogue decide user dialogue theagenda stack generate user response ateach turn system offer book theuser accepts system notify number reference number usedto verify whether booked place whatthe user informs convlab also provide automatic evaluator assess whether targetdialogue system trace user informs informs user request makesan appropriate booking external databasebased traced information although usersimulator evaluator highly sophisticated itis perfect human hence dialogue system submit dstc8 evaluate notonly user simulator also humancrowd-workers.3 end-to-end neural pipeline forgoal-oriented dialogue systemwe describe end-to-end neural pipeline forthe goal-oriented dialogue system base gpt-2.our system consist gpt-2 model finetuned delexicalized version multiwozdataset section database querymodule take pre-trained gpt-2 model andfine-tune follow step dialogue management pipeline figure illustrate overallarchitecture concrete example overviewof process follow model follows:1 predict recent domain corresponding dialogue state condition dialoguehistory.2 predict system action delexicalizedtokens condition dialogue historyand dialogue state.3 system action inform book need external information database query module2 retrieve candidatesand return them.4 update current system action detect empty query results section generate system response delexicalized token condition dialogue history,2convlab provide query module return candidate give domain dialogue state.587= token embedding okay hotel park hotelinform price cheap find okay hotel park hotelinform price cheap okay speaker embedding+ positional embeddingdialogue history dialogue state system action system responsefigure input representation fine-tuning gpt-2.dialogue state system action.6 update delexicalized token systemresponse query result.in figure number wrap circleindicate order process showshow system handle case dbquery return record all.3.1 input representationin multiwoz dataset metadata dialog correspond current dialogue stateand current system action respectively figure order gpt-2 need convertthe dialogue state system action wordtokens.figure show illustrative example asingle-turn dialogue representation ofthe dialogue state system action introduce delimiter token signal beginning sequence representation user utterance system response dialoguestate system action domain slotnames also represent additional special token special token thatindicate mention care complete input representation modelis illustrate figure similar radford wolf input embeddingcomprises token embedding speakerembedding positional embedding.3.2 delexicalizationeach dialogue multiwoz dataset generatedbased query result requestable slot value reference numbersand address color orange infigure valid particular dialogue instance hand modelshould able inform appropriate informationdepending dialogue context addressthis delexicalized value requestableslots reference number name postcode phonenumber address domain slotname hotel postcode hotel postcode thatappear corpus thus model learn togenerate delexicalized system response delexicalized token later string-replaced realinformation query small pieceof post-processing code.3.3 training objectivein order fine-tune gpt-2 optimize theweighted objective language modeling next-utterance classification follow radford usethe standard left-to-right objective bengioet follow =∑ilogp wi|w1 wi−1 objective calculate likelihood thenext word-token give previous wordtokens.for model need distinguish thegold response gold dialogue state+gold system action+gold system response distractor golddialogue state+gold system action+fake system response give dialogue history distractorsystem response randomly sample themultiwoz dataset linear classifier take thelast hidden state gpt-2 decoder block asinput compute class probability passingthrough softmax layer cross-entropy lossbetween class probability correct labelwas objective thus thegiven word sequence totalobjective become linear combination llmand hyper-parameters ltotal αlmllm αnclnc success rate return turns precision recall book rate ↑baseline greedy top-p p=0.8 top-k k=30 table results decode strategy automatic evaluation convlab evaluator baselinesystem provide convlab consists milu module rule-based andtemplate-based nlg.rank team success rate language response turns ↓understanding appropriateness baseline overall result human evaluation carry dstc8 organizer five team thebaseline result compared.3.4 decoding strategywhen generate system response thedialogue history final output probabilitydistribution word-tokens position usingthe distribution many decode methodsfor generate word-tokens significant impact quality output holtzman weston greedydecoding beam search common approach however since greedy decode consider token highestprobability position necessaryyield system response overall high probability addition holtzman evidence beam search decoding appropriate high-entropy natural language generationsuch dialogue sampling-based decode method top-k sampling top-p samplinghave show address problemsquite effectively dialogue task wolf al.,2019 budzianowski vulić evaluate performance model decode scheme mention select thebest human evaluation.3.5 handling empty query resultas mention gpt-2 invoke querymodule interact database however gpt-2 know many candidate satisfythe constraint a-priori therefore exist caseswhere candidate happen satisfy constraint refer empty-query-result.in case dialogue system generatethe system response correspond intentempty-query-result system monitor system action generate gpt-2 replace itby database query return emptyresult modify input gpt-2 togenerate system response simple solutionworked quite well practice.4 related worktransfertransfo wolf firstattempt incorporate large-scale pre-trained language model chit-chat dialogue system using backbone fine-tuning approachranked first automatic evaluation secondin human evaluation convai2 competition dinan model mainlyinspired work extend goal-orienteddialogues gpt-2.parallel independent work towardsdstc8 submission budzianowski vulić also demonstrate neural model goaloriented dialogue system fine-tuning gpt-2 onthe multiwoz dataset however handle dialogue-context-to-text task outputsthe system response give dialogue history theground-truth dialogue state database inour case oracle information relate database589 restaurant-request area restaurantnooffer food moderneuropean0.080.160.240.320.400.320.240.160.08 food moderneuropean pricerange name area attention weights system action attention weights responserestaurants system i＇msorry thereare nomoderneuropean.figure visualizing attention weight leave model attend dialogue state area generate system action restaurant-request area right model attend system action restaurant-nooffer generate response sorry modern european restaurant dialogue state provide dialogue history provide taking dialoguehistory input model operate complete dialogue system generate system response sequentially follow core step inthe dialogue management pipeline.5 experimental settings5.1 training detailswe develop model open-sourceimplementation wolf andthe gpt2-small parameter consist transformer decoder block andpre-trained weight wolf wetokenized sentence sub-word usinggpt2tokenizer4 sennrich fine-tuned gpt-2 batch size multiwoz training dataset themaximum history size dialogue to15 adam optimizer kingma ba,2015 learninglate coefficient thenc loss respectively.5.2 evaluation metricsthere evaluation criterion end-toend multi-domain dialog system task the3https //github.com/huggingface/transfer-learning-conv-ai4https //github.com/huggingface/transformersmulti-domain task-completion track dstc8 automatic evaluation user simulator success rate book rate return turns precision recall human evaluation crowd-workers success rate language understanding score response appropriateness score turnsin measure success rate dialogue isconsidered success requestableslots correctly fill book success needed.book success achieve reserve information informable slot measure book rate sub-evaluation.return reward signal obtain usersimulator dialogue complete return dialogue compute follow return turns ∗max turn task success ∗max turn otherwise.the turn indicate maximum limit turnsin conversation precision recall andf1 measure accuracy requestable slot filling.for human evaluation language understanding score response appropriateness scorewere metric natural response ofthe model point scale humanevaluation result report carry bythe dstc8 organizers.5906 results6.1 automatic evaluationtable show automatic evaluation result various decode strategy user simulatorprovided convlab propose model withgreedy decode strategy achieve success rateof return turnsof book rate precision of0.87 recall score automatic evaluation simulateddialogues model outperform baselinesystem fail perform best among submittedsystems mostly incorrect intent recognition user simulator believe canbe circumvent train model reinforcement learning train avoid systemresponses trigger intent recognition failure inthe simulator however main focus generate diverse system responses look naturalto human evaluators.6.2 human evaluationtable show final ranking competitionusing human evaluation.5 propose modelwith top-p sampling p=0.8 strategy rank thefirst place success rate average turn language understandingscore response appropriatenessscore compared model model show improvement successrate performance significant inhuman language metric point high model thelanguage understanding score responseappropriateness score.6.3 attention weightsfigure visualize attention weight thetransformer block model demonstratingthat model appropriately attend wordtoken generate previous module inthe dialogue management pipeline like apipelined dialogue system would generate intermediate output example ifthe user look modern europeanfood model generate dialogue state area mean area mentioned.then attention weight area dialogue state relatively higher5https //convlab.github.io/model joint slot acc.glad35.57 zhong gce36.27 nouri hosseini-asl sumbt46.64 trade greedy performance comparison state-ofthe-art model dialogue state tracking benchmarkof multiwoz dataset.model inform success bleubaseline71.29 budzianowski tokenmoe75.30 hdsa82.9 chen structured fusion82.70 mehri larl zhao greedy performance comparison state-ofthe-art model dialogue-context-to-text generationbenchmark multiwoz dataset.than token generate system action restaurant-request area asanother example change system actionas restaurant-nooffer model generate system response sorry nomodern european restaurant attend thetoken restaurant-nooffer multiwoz benchmarks performanceas ablation study test modular performance model multiwoz benchmarktasks budzianowski dialogue statetracking dialogue-context-to-text generation.6.4.1 dialogue state trackingtable compare dialogue state track accuracy model recent trackersin literature task measure jointaccuracy slot accuracy dialogue state track part model although train objective involves dialogue management task thandialogue state tracking model track perfor591mance competitive state-of-the-artmodels.6.4.2 dialogue-context-to-text generationdialogue-context-to-text generation look thecombined performance dialogue policy andthe system response generation module measure quality system response previous user utterance ground-truth dialogue state ground-truth database query result aregiven trained model straightforwardlyadapted perform task replace intermediate input ground-truth values.table show context-to-text generationbenchmark performance compare recentmodels propose literature modelwas competitive state-of-the-art model except bleu score factthat system large vocabulary gpt-2 make system response often contain diversewords dataset.7 conclusionin paper present end-to-end monolithic neural model goal-oriented dialogue thatlearns follow core step dialogue management pipeline since model output theintermediate result dialogue managementpipeline easy integrate external system interpret system generate aparticular response experimental result fromhuman evaluation show evidence approachcan provide natural human-level interactionfor goal-oriented dialogue advance stateof-the-art conversational agent alsodemonstrates power large-scale pre-trainedlanguage model adopt build end-toend goal-oriented dialogue systems.acknowledgementsthis work support nationalresearch foundation korea nrf2019r1a2c1087634 ministry science information communication technology msit korea iitp iitp2019-0-00075-001 iitp
existing automatic evaluation metric foropen-domain dialogue response generationsystems correlate poorly human evaluation focus evaluate response generation system response selection evaluate system properly response selection propose method construct response selection test well-chosen false candidate specifically propose constructtest filter type false candidate unrelated ground-truthresponse acceptable appropriate response experiment wedemonstrate evaluate system response selection test develop byour method correlate strongly human evaluation compare widely usedautomatic evaluation metric bleu.1 introductionautomatic evaluation open-domain dialoguegeneration system potential drive theirresearch development high reproducibility cost however exist automatic evaluation metric bleu papineniet correlate poorly human evaluation poor correlation arisesfrom nature dialogue manyacceptable response input context know asthe one-to-many problem zhao tackle problematic issue focus onevaluating response generation system response selection task system select anappropriate response give context aset response candidate candidate hasthe label indicate whether candidate isappropriate response give context traditionally response selection evaluate retrieval-based dialogue system lowe al.,2015 consider apply thistask drive research dialogue generationrepositorycontext ground-truth car.queryno car.i know.i cold.no car.i know.i cold.214questionretrieveutterancesgive score byhuman evaluationremovehigh-scoreutterancesno car.i know.i cold.214false candidatesfigure overview construction method ourtest first retrieve utterance relate tothe ground-truth response repository weremove acceptable utterance human evaluation.systems specifically consider responseselection pick promise system shouldbe evaluate precisely human among alot candidate system assume responseselection valid option preliminaryevaluation basis following assumption system generate appropriate responsescan also select appropriate response advantage evaluate generation system responseselection remedy one-to-manyproblem consider theappropriate response include setsof response candidate another advantage thatit enables simple clear comparison betweensystems accuracy.generally false response candidate randomly sample repository lowe al.,2015 gunasekara cause twoproblems unrelated false candidate acceptable utterance false first problemis randomly sample false candidate often ground-truth response considerthe case give context acar response candidate play tennis. ran594domly sample systems easily recognize thiscandidate false related content word excessiveeasiness preferable performancegap good inferior system tend tobe small second problem noguarantee randomly sample candidate arealways unacceptable example tknow. often sample false response becausethis phrase often occur open-domain dialogues.this phrase regard acceptable various context problem make generalresponse selection test unreliable.in work propose method constructresponse selection test well-chosen falsecandidates figure first retrieve utterances relate ground-truth response thenwe remove acceptable utterance human evaluation experiment demonstrate thatautomatic evaluation test develop byour method correlate strongly humanevaluation compare widely automaticevaluation metric bleu empiricalresults indicate response selection wellchosen false candidate valid option forevaluating response generation system willrelease test experiments.12 related workautomatic evaluation metric various metricshave propose automatic evaluation dialogue system bleu meteor banerjee lavie rouge greedymatching lintean vector extrema forgues metric evaluate quality response generate system however challenge oneto-many problem example adem metricproposed lowe easily fool byadversarial example response remedy one-to-many problem focus evaluate system response selection.response selection test human labelsone popular test response selection isdouban conversation corpus chinese al.,2017 test response candidate hasa manually annotate label indicate whetheror candidate appropriate givencontext although test similar ours,1the test available http //github.com/cl-tohoku/eval-via-selection.there difference purposesand procedure test design purpose ofcreating test simulate evaluateretrieval-based dialogue system thus candidate corpus retrieve thecontext query retrieval-based system do.in paper develop english response selection test human label evaluate dialoguegeneration system salient differencesfrom douban conversation corpus procedureof retrieve false candidate retrieve false candidate ground-truth response thismethod certainly collect false candidate relate ground-truth response andfacilitate error analysis describe section test construction3.1 construction methodfor context ground-truth response rtrue construct false response candidatesrfalse rfalse retrieve utterance anutterance repository mentionedin section want filter type ofutterance unrelated ground-truthresponse acceptable appropriateresponses filter utterance follows:1 retrieve utterance relatedto ground-truth response rtrue theutterance repository remove acceptable retrievedutterances human evaluation.1 retrieve utterance relate groundtruth response assume utterance relate ground-truth response share similar content word retrievethe related utterance basis similarity content word process make itdifficult system distinguish groundtruth false candidate compare thecontent words.2 remove acceptable utterance coincidentally retrieved utterance acceptable appropriate response removesuch utterance human annotator evaluate retrieved utterance specifically instruct five annotator candidate score eachretrieved candidate five-point scale score mean utterance clearly beregarded appropriate response given595context whereas score mean cannotbe regard appropriate additionto score also instruct annotator givea score ungrammatical utterance remove utterance give score orhigher three annotator theseutterances high score acceptable inaddition remove utterance givena score three annotator becausethese likely ungrammatical alsoinstruct annotator score ground-truth response combine retrieved utterance remove question score ground-truthresponse i.e. three annotator givea score intend ensure thatground-truth response certainly appropriate forthe give context.3.2 overview constructed test setsettings test construction retrieve question repository andremove acceptable follow method describe section crowdsourcing2 toscore retrieved utterance remove acceptable utterance question thathave available false candidate fromthese question develop question thesame context different candidate groundtruth response false candidate regardone acceptable utterance remove humanevaluation ground-truth response newquestions.we dialogue data dailydialog liet construct test extractthe four begin turn dialogue samplefrom dailydialog regard fourth utteranceas ground-truth response extract utterance opensubtitles2018 lison toconstruct repository retrieve false candidate note repository containthe utterance dialogue data trainresponse generation system section test develop testset consist question candidate ground-truth false candidate show basic statistic test set.the fleiss kappa fleiss annotator score scale note we2https //www.mturk.com/3we calculate fleiss kappa base scale thescores categorical.total question question turn question scoring class scoring class basic statistic test setcontext excuse could please take pictureof camera sure button press shoot one.candidates:1 could focus ninja focus.3 lose focus focus ground-truth table example test three false candidate contain content word focus relatedto context topic scoring binary classification scoreshigher regard appropriate response others fleiss kappa scoringis high douban conversationcorpus test table show example test false response candidatesshare content word focus relate thetopic camera experiment conduct simple experiment investigate whether system take content word account canrecognize false response candidate test set.for model tf-idf model loweet simply compare content word give context candidate.as result accuracy comparison also replace false candidate inour test randomly sample utterance theaccuracy tf-idf model increase to0.671 result indicate difficult torecognize false candidate test bycomparing content words.4 experimentswe test whether automatic evaluation response generation system test correlateswith human evaluation.5964.1 experimental procedurewe train multiple response generation system andrank basis human automaticevaluation score compare system ranking human score ranking byeach automatic score verify correlations.4.1.1 response generation modelswe train different response generation systemsto rank experiment architecturesare seq2seq seq2seq lstm hochreiter schmidhuber transformer vaswani system architecture differenthyper-parameters.4we train model opensubtitles2018 thetraining data consists sample validation data consists sample whichis four-turns dialogue.4.1.2 evaluation procedureground-truth system rank human scoresthe trained system generate response rgen foreach input context five human annotator response score generate response rgen five-point scale ascore mean response clearly beregarded appropriate response givencontext whereas score mean regard appropriate aresult obtain five score foreach response rgen average smean =mean also average smeanacross question test yieldthe final score sfinal system based thisscore make ranking system regardit ground-truth ranking.although develop test consistsof question costly evaluate allthe system response question byhumans thus give context randomlysampled question test system inputs c.system ranking response selection accuracywe rank system response selection accuracy well-chosen false candidate chosen trained response generation systemscompute softmax cross-entropy loss eachresponse candidate regard candidate loss system selection:4we describe model setting appendix b.metrics spearman p-valuebleu-1 −0.36 chosen correlations ground-truth systemranking ranking automatic evaluation.r̂ argminr∈r prediction calculateaccuracy make ranking system basedon accuracy comparison also make aranking response selection accuracy randomly sample false candidate random wecompute accuracy chosen randomusing question test set.system ranking evaluation metricsfor comparison also make ranking system three exist automatic evaluation metric bleu meteor rouge-l. first thetrained system generate response input context compute score compare generate response ground-truthresponses.these score compute automaticallywithout false candidate thus compute themusing available four-turns dialogue sample dailydialog regard fourth utterance ground-truth responses.4.2 resultswe compare ranking spearman rank correlation coefficient show table first weyielded human upper bound evaluate thecorrelation ranking make different annotator human randomly dividedhuman evaluation group make tworankings correlation coefficient thetwo ranking second find thatthe ranking make exist automatic evaluation metric correlate poorly ground-truthranking bleu often evaluate generationsystems correlate human evaluation exception rouge-l. however,5we compute coefficient random averagingthe coefficient different trials.597figure plot spearman rank correlation coefficient ground-truth ranking ranking random blue indicate correlation coefficient chosen.context peter enough computer game godo homework now.b play stop playing computer game candidates ground-truth finish soon.random thats problem small towns.chosen finish soon.table examples randomly sample wellchosen candidates.its correlation coefficient whichmeans reasonable correlation third find thatthe rank make test reasonablycorrelates ground-truth ranking comparedwith metric correlation coefficient chosen high discussioninstability evaluation random samplingthe correlation coefficient ranking response selection randomly sample false candidate random high bleuand slightly chosen however serious problem observe instability make test consistsof different false candidate random samplingwith different seed test make asystem ranking compute coefficient figure2 show plot spearman rank correlation coefficient trial range thecoefficients wide resultmeans quality evaluation randomlysampled false candidate strongly depend thesampled candidate uncontrollablefactor stemming randomness.interpretable error analysis automaticevaluation well-chosen false candidate bringsanother benefit interpretable error analysis table show example question testset well-chosen false candidate chosen issimilar ground-truth response however thegrammatical subject chosen sentence completely mismatch context.thus system select false candidate theymay lack ability determine correctly subject sentence test enablesus analyze system prediction variousmeaningful perspective case study design error label indicateswhy false candidate false assign to50 false candidate test succeed inassigning label candidates.6limitation test design evaluateopen-domain dialogue generation system thus itis suitable evaluate type dialoguesystem task-oriented contrast exist automatic evaluation metric bleu type restriction.5 conclusionin paper focus evaluate responsegeneration system response selection evaluate system properly response selection weproposed method construct response selectiontest well-chosen false candidate specifically propose construct test filter outsome type false candidate unrelatedto ground-truth response acceptable appropriate response demonstratedthat evaluate system response selection withthe test develop method correlatesmore strongly human evaluation comparedwith widely metric bleu.in future provide label indicate candidate false false candidatesin test easily detect weakpoints system error analysis.acknowledgmentsthis work partially support jsps kakenhi grant number jp19h04162 would liketo thank laboratory member give advice reviewer work insightful comments.6we show example appendix c.598
algorithmic approach interpret machine learning model proliferate recent year carry human subject teststhat first kind isolate effect algorithmic explanation aspect model interpretability simulatability avoid important confounding experimental factor model simulatable whena person predict behavior input kind simulation test involve text tabular data evaluate fiveexplanations method lime anchor decision boundary prototype model composite approach combinesexplanations method clear evidence method effectiveness find invery case lime improve simulatability tabular classification prototypemethod effective counterfactual simulation test also collect subjective ratingsof explanation find rating predictive helpful explanationsare result provide first reliable andcomprehensive estimate explanationsinfluence simulatability across variety explanation method data domain showthat need careful metricswe evaluate explanation method significant room improvementin current methods.11 introductioninterpretable machine learning widelydiscussed topic rudin doshi-velez andkim lipton gilpin survey paper converge definition explainable interpretable thereare common thread discourse commentators observe interpretability useful for1we make support code data model publicly available http //github.com/peterbhase/interpretablenlp-acl2020achieving model desideratum include build user trust identify influenceof certain variable understand modelwill behave give input ensure thatmodels fair unbiased.in review doshi-velez outline approach measure interpretability.they describe human-subject task testfor particularly useful property simulatability.a model simulatable person predictits behavior input property especially useful since indicate person understand model produce output does.the first task term forward simulation give input explanation usersmust predict model would output thegiven input second counterfactual simulation user give input model output forthat input explanation output andthen must predict model output give perturbation original input explanation algorithmically generate method interpret explaininga model simulation test carry outbefore study date isolate effectof explanation simulatability ribeiro al.,2018 chandrasekaran nguyen bang carry simulation test first toincorporate following design choice separating explain instance test instance explanation give away answer evaluate effect explanation abaseline unexplained example balancingdata model correctness user succeed guess true label forcinguser prediction input performance isnot bias toward overly specific explanation wedisplay study design figure provide result high-quality human5541 post prediction phaselearning phase explanation learning phase prediction phasesimulationforwardsimulationcounterfactual prediction phase human simulation model prediction explanation counterfactual input counterfactual model prediction post prediction phaseexplanationeffectpost sim.accuracypre sim.accuracyfigure forward counterfactual simulation test procedure measure human user ability predictmodel behavior isolate effect explanation first measure baseline accuracy measure accuracy user give access explanation model behavior forward test explained example aredistinct test instance counterfactual test test instance counterfactual version modelinput explanation pertain original inputs.user test response includeboth forward counterfactual simulation tasks.through test measure explanation effectiveness five method across text tabularclassification task evaluation include twoexisting explanation technique lime anchor ribeiro translatetwo explanation method image recognition model work textual tabularsetups first latent space traversal method term decision boundaryapproach joshi samangouei al.,2018 second case-based reasoning method term prototype method chen final method novelcomposite approach combine complementary explanation method lastly wealso collect subjective numerical user rating ofexplanation quality finding are:1 lime improve forward counterfactualsimulatability tabular classification task.2 prototype improve counterfactual simulatability across textual tabular data domains.3 method definitively improve forward andcounterfactual simulatability together thetext task though prototype compositemethods perform best average.4 appear user quality rating explanation predictive helpful explanation counterfactual simulation.5 user rate composite explanation asamong best quality combine explanation overtly improve simulatability either data domain.2 background related work2.1 interpretable mean survey paper term vary ways.rudin draw distinction interpretability explainability suggest amodel interpretable perform computationsthat directly understandable post-hoc explanation hand potentially misleading approximation true computations.gilpin also distinguish thetwo concept though define differently.in paper distinguish betweeninterpretability explainability rather weadopt conceptual framework doshi-velezand consider interpretability interms downstream desideratum assessmodels respect terminology follow explanation method mayimprove interpretability model thesense interpretable model simulatable.2.2 explanation methodsseveral taxonomy propose categorize method interpretability organizemethods category feature importance estimation case-based reasoning latent space traversal.feature importance estimation feature importance estimate provide information howthe model certain feature prominentamong method gradient-based approach first introduce vision simonyanet show may5542be translate text data approach since demonstrate sometimes behave counterintuitive adebayoet number alternative method propose quantify feature importance across data domain kimet lundberg sundararajan study choose evaluate domain-agnostic approach lime andanchor ribeiro method simple model sparse linear modelsand rule list approximate complex model behavior locally around input show estimated effect directly interpretable feature onthe model output method local input define domain-specificmanner perturbation distribution center onthat input.case-based reasoning prototype model classify instance base similarity toother know case work prototype model computer vision introduce neural modelsthat learn prototype correspond part image chen hase theseprototypes produce classifier featuresthat intend directly interpretable.latent space traversal method traversethe latent space model order show howthe model behaves input change classification setting cross decision boundarymay reveal necessary condition model prediction original input several method exist vision model joshi samangouei knowledge approach exists discriminative model text andtabular data develop simple method forthese kind model describe section evaluating interpretabilityhere discuss work involve automatic andhuman evaluation interpretability well ashow improve past simulation test design.while human evaluation useful evaluate many aspect interpretability restrict ourdiscussion work measure simulatability.improving forward test design forward simulation task implement many different form serious need consensus proper procedure doshi-velez andkim originally propose user predictmodel behavior give input explanation.with many explanation method trivial task explanation directly revealthe output example lime give predictedprobability indicate model behavior withhigh likelihood make number experimental design choice give reliable estimate method effectiveness past study separate explained instance thetest instance prevent explanation givingaway answer three study datapoints explanation prediction item nguyen chandrasekaran al.,2018 bang evaluate effect explanation baseline userssee example data point without explanation prior evaluation include control choice distinguish test fromthat ribeiro balance data bymodel correctness user succeed simply guess true label force userpredictions every input metric notfavor overly niche explanations.counterfactual simulatability counterfactualsimulatability knowledge never beenmeasured machine learning model whiledoshi-velez propose usersto edit input order change model output instead user predict model behavior onedited version data point approach ismore scalable solicit creative responses.relation automatic tests prior work haveproposed automatic metric feature importanceestimates nguyen hooker deyoung typically operateby check model behavior follow reasonable pattern counterfactual input constructedusing explanation e.g. mask important feature check class scoredrops whereas automatic metric define appropriate model behavior advance counterfactual instance generate fixed schema wepresent random counterfactual human andelicit prediction model behavior instance allow human validation modelbehavior broad range input scenario thanan automatic procedure human expectation give response diverse concreteexamples rather dictate advance.subjective ratings hutton measure user judgment whether word importancemeasures explain model behavior text classi5543lime0 wordsbaselineest probabilitynegativepositivedespite modest aspiration occasional charm dismissed.input label model outputstep modest impressiveevidence margin +0.32decision boundaryevidence margin occasional rareevidence margin impressive aspiration rarecharms dismissed.anchorprototypemost similar prototype important word none select similarity score rather silly.figure explanation method apply input test movie reviews.fication setting rating task thus similar totheirs change evaluate likert scale rather force ranking explanation technique neural model rather wordimportance estimate naive bayes classifier another study user judge image classification explanation likert scale range explanation concise explanation banget whereas scale focus conciseness user rate explanation reveal reason model behavior.3 explanation methodsin section describe explanation method example explanation test movie review show figure limit discussion lime anchor since detail thesemethods find original paper notethat lime anchor decision boundarymethod arbitrary blackbox model prototype method neural modelthat also produce explanation.3.1 limeribeiro present lime local linear approximation model behavior userspecified feature space linear model theblackbox output sample distributionaround input number featuresto take class probability ourmodel output show lime explanationsto user give select feature withestimated weight model intercept ofmodel weight predicted model output.3.2 anchorribeiro introduce method learn rule list predict model behavior withhigh confidence sample distribution around input learn approach obtain rule list rule applyto input high probability receive prediction original feature space rule list specify user.as original work individual tokensfor text data learning parameter anchor explanation.3.3 prototype modelprototype model previously forinterpretable computer vision chen hase develop prototype modelfor text tabular classification tasks.in model neural network input alatent space score class maxpk∈pca similarity function vector thelatent space protoype vectorsfor class choose gaussian kernel forour similarity function e−||zi−pk||2.the model predict input belong sameclass prototype closest latent space unlike chen takethe activation obtain concise explanations.in lieu image heatmaps provide feature importance score distinguish thesescores standard feature importanceestimates score prototype-specific rather class-specific choose featureomission approach estimation text data omission straightforward give token wetake difference function output theoriginal input input token embed zero tabular domain however variable never take meaningless value circumvent problem take difference function value original5544input expect function value particular feature miss expectation compute distribution possible value fora miss feature provide multinomial logistic regression condition remain covariates.when present prototype explanation weprovide user predicted class score mostsimilar prototype feature importancescores provide score magnitude meet asmall threshold explanation figure score meet threshold size ofpc text classification task forour tabular classification task trainingand feature importance detail appendix.3.4 decision boundaryjoshi samangouei introduce technique traverse latent spacesof generative image model method provide path start input data point crossa classifier decision boundary methodsmay help user necessary condition forthe model prediction.we provide simple method traverse thelatent space discriminative classifier example figure algorithm first samplesaround original input instance crossthe decision boundary counterfactual inputis choose take instance withthe edit feature token variable break euclidean distancebetween latent representation lastly provide path input greedily pick theedit remain edits least change themodel evidence margin differencebetween positive negative class score theexplanations present user include input step counterfactual input evidencemargin step path longer thanfour step show last four.3.5 composite approachwe hypothesize explanation provide complementary information since takedistinct approach explain model behavior.hence test composite method combineslime anchor decision boundary andprototype explanation make adjustmentsto method combine first showonly last step decision boundary explanation i.e. change flip prediction second train prototype modelwith feature extraction layer initialize fromthe neural task model thereafter doso since interested explain taskmodel behavior tactic yield prototypesthat reflect characteristic task model.4 experimental designin section describe datasets task model user pool experimental design.4.1 data task modelswe perform experiment classification taskswith text tabular data first dataset consist movie review excerpt pang dataset include review binarysentiment label split partition of70 train validation andtest respectively neuralarchitecture yang limited touse single sentence second dataset isthe tabular adult data repository graff dataset contain record individual label iswhether annual income data processing scheme neural network architecture ribeiro accuracy give appendix.4.2 user poolwe gather response in-persontests trained undergraduate hadtaken least course computer science orstatistics.2 user randomly assign toone condition correspond ourdataset-method pair condition atleast full test collect allocate remainingparticipants composite method order toensure high quality data employ screening test check user understanding theirexplanation method test procedure participant screen score wealso exclude data user whose task completion time extremely user hour user test witha dataset explanation method give total user test user exitthe experiment finish task require advance background explanation rely conditional probability approximation ofprobabilities quantitative concepts.5545text tabularmethod change change puser lime −1.93 change user accuracy give explanation model behavior relative baselineperformance data group domain give confidence interval calculate bootstrapusing user response bold result significant level lime improve simulatabilitywith tabular data method definitively improve simulatability either domain.forward simulation counterfactual simulationmethod change change puser lime −2.64 −0.92 −2.07 change user accuracy give explanation model behavior relative baselineperformance data group simulation test type give confidence interval calculate bybootstrap user response bold result significant level prototype explanationsimprove counterfactual simulatability method definitively improve simulatability test.for data analysis purpose consider taskitems answer post test phases.4.3 simulation testswe collect forward test counterfactual test response total.forward simulation test represent infigure test split four phase learning phase prediction phase learning phasewith explanation post prediction phase.to begin user give example thevalidation label model predictionsbut explanation must predict themodel output either input withthe number choose base user time constraints.users allow reference learningdata prediction phase next returnto learning example explanation include finally predict model behavior instance firstprediction round design improvement inuser performance post prediction phase isattributable addition explanations.we show screenshot user test interfacein appendix.counterfactual simulation represented figure test require user predict amodel behave perturbation givendata point test consist postprediction round difference addition explanation inboth round provide user test dataset timeconstraints ground truth label model sprediction perturbation input seethe appendix description perturbationgeneration algorithm users predict modelbehavior perturbation post round user give data alsoequipped explanation model prediction original input therefore improvement performance attributable addition explanations.data balancing critical aspect experimental design data balancing toprevent user succeed test simplyby guess true label every instance todo ensure true positive false positive true negative false negative equally represent input likewise counterfactual test sample perturbation forany instance chance pertur5546text ratings tabular ratingsmethod σlime user simulatability rating data domain scale mean standard deviation forratings give confidence interval mean give calculate bootstrap.bation receive prediction originalinput confirm user understanding databalancing screening test.data matching within data domain allusers receive data point throughout theexperiment design control difference data across condition user though reduce information byeach test make confidence interval relatively wide give sample size alsomatch data across prediction round order tocontrol influence particular data pointson user accuracy post phases.4.4 subjective simulatability ratingsusers explanation phase test second learning phase forward test andthe post phase counterfactual test thesestages user give subjective judgmentsof explanation rate method point likert scale response question explanation show systemthought explain user shouldgive high rating explanation showsthe reason model prediction regardless ofwhether prediction correct.5 resultswe report data total response from39 user test test method datadomain pair contain either taskitems missingness user exit study early result follow weuse term change refer estimate ofexplanation effectiveness difference useraccuracy across prediction phase simulationtests perform two-sided hypothesis test forthis quantity block bootstrap resampling bothusers unique task item within condition efron tibshirani addition sinceusers complete first prediction round eithersimulation test without access explanation weestimate mean accuracy methodwith random effect model allow toshare information across method yield moreprecise estimate test performance.below analyze experimental result andanswer three question explanation helpusers user rate explanation canusers predict explanation effectiveness explanation help user show simulation test result tables table group result data domain andin table group result test type.our principal finding follows:1 lime tabular data settingwhere definitive improvement forward counterfactual simulatability withno method data domain finda definitive improvement across tests.2 even combined explanation composite method observe definitive effect model simulatability.3 interestingly prototype method reliably well counterfactual simulation test inboth data domain though forward test itmay explanation helpful onlywhen show side side inputs.these result suggest many explanationmethods noticeably help user understandhow model behave method successful domain might work equally wellin another combine information explanation result overt improvementsin simulatability give wide confidenceintervals result consider cautiously also method infact improve simulatability precisely estimate example prototypeand composite method best averagewith text data though confident thatthey improve simulatability.note estimate explanation effectiveness5547could influence user simply regress tothe mean accuracy prediction round wefind primary result skew thisphenomenon high estimate change ineach data domain test type come condition mean test performance eitherabove overall mean case within point potential problem mitigate random effect model pretest performance pull test meanstoward overall mean.5.2 user rate explanation seem intend user explanationsbased quality rather model correctness observe significant difference ratingsgrouped model correctness table appendix table show user rating methodand data domain.we observe rating generally higherfor tabular data relative text data composite lime method receive high rating domain variance explanation rating quite high relative scale.5.3 users predict explanationeffectiveness answer question measure explanation rating relate user correctness postphase counterfactual simulation test thisphase user rate explanation model prediction original input predict model behavior perturbation input ratingsof explanation quality good indicator theireffectiveness would expect higherratings associate user correctness.we find evidence explanation ratingsare predictive user correctness estimate therelationship logistic regression user correctness rating test model absolute rating rating normalize within user since rating lack absolute scale users.with text data point estimate confidence move rating isassociated −2.9 percentagepoint change expect user correctness usingnormalized rating find move themean explanation rating first standard deviation associate −3.9 point change tabular datapoints estimate change rating associate −2.6 percentage point change expect user correctness.of course show association important note isno relationship user rating simulatability simply query human explanation quality provide good indicationof true explanation effectiveness.6 qualitative analysiswhen explanation succeed improve useraccuracy fail present example counterfactual test item analyze explanation havepointed reason model behavior.6.1 explanation success examplefor example post test responsesfor prototype lime correct themodel output change counterfactual test.original pretty much suck afunny moment two. counterfactual mostly bother look funny moment two. lime identifies funny moment positive word weight include baseline notable negative wordis suck −.23 change similar word bother together lime suggeststhe prediction would stay since positive word unaffected importantnegative word similar substitute.the prototype model give activatedprototype murders numbers greatmovie perfectly acceptable widget. itidentifies funny important wordsfor prototype activation counterfactualis still similar prototype suggest prediction would change.6.2 explanation failure examplefor item response werecorrect explanation methodimproving correctness relative test accuracy users need predict model prediction change negative counterfactual.original bittersweet film simple inform rich human events. counterfactual teary film simplein form vibrant devoid events. give word condition original positive prediction bittersweet. whathappens bittersweet change teary anchor explanation actually applyto counterfactual scenario probabilisticdescription model behavior condition onthe word bittersweet present.lime give five word smallweights baseline suggest lime fail identify feature input necessary modeloutput among five word three thatchanged sentence would suspect weight change make inthe counterfactual would flip model output.decision boundary give counterfactual input negative prediction sappy film simple link unique human events. however difficult tell whether counterfactual sentence similar decision-relevant waysto propose counterfactual sentence.the prototype model give activated prototype original prediction watstein handily direct edit around screenplay sappy element sustain hook buildupwith remarkable assuredness first-timer. noimportant word select left withouta clear sense similar prototype circumstance would lead themodel output changing.these example reveal area improvementin explanation better method need distinguish sufficient necessary factorsin model behavior clearly point waysin examples share decision-relevant characteristic input must soin appropriate feature space problem athand especially model complex data.7 discussionforward tests stretch user memory showusers example learn phase donot allow reference learning data prediction phase reasonably user report difficult retain insight fromthe learn phase late prediction rounds.generating counterfactual inputs bedifficult algorithmically construct counterfactual input match true data distribution especially seek change model prediction text counterfactuals regularly outof data distribution sense realmovie review would exhibit word choice theydo still consider input interest reason model handle inputsin manner assess possiblemodel behavior analysis.fair comparison explanation methods inour forward simulation treatment phase provide user explained instance allowthem read pace control forthe number data point method butone could instead control user exposure time orcomputation time explanation generation lime anchor approachesfor efficiently cover space input alimited budget example ribeiro since applicable decision boundary prototypemethods lack similar notion coverage clear whether approach areuseful text data suchapproaches lime anchor perform good onforward simulation tasks.8 conclusionsimulatability metric give quantitative measureof interpretability capture intuition explanation improve person understanding model produce output inthis paper evaluate five explanation methodsthrough simulation test text tabular data.these first experiment fully isolate theeffect algorithmic explanation simulatability find clear improvement simulatabilityonly lime tabular data prototypemethod counterfactual test also appear thatsubjective user rating explanation quality arenot predictive explanation effectiveness simulation test result suggest must becareful metric evaluate explanation method significant roomfor improvement current methods.acknowledgmentswe thank reviewer helpful feedbackand study user work supportedby nsf-career award darpa mcsgrant n66001-19-2-4031 royster society phdfellowship google cloud computeawards view contain article arethose author funding agency.5549
keyphrase generation summarize main idea document setof keyphrases setting recently introduce problem givena document model need predict aset keyphrases simultaneously determine appropriate number keyphrases toproduce previous work setting employ sequential decoding process generate keyphrases however decodingmethod ignore intrinsic hierarchical compositionality exist keyphrase adocument moreover previous work tends togenerate duplicate keyphrases wastestime computing resource overcomethese limitation propose exclusive hierarchical decoding framework includesa hierarchical decode process either asoft hard exclusion mechanism thehierarchical decoding process explicitlymodel hierarchical compositionality akeyphrase soft hard exclusion mechanism keep track previouslypredicted keyphrases within window sizeto enhance diversity generatedkeyphrases extensive experiment multiple benchmark datasets demonstrate effectiveness method generate less duplicated accurate keyphrases1.1 introductionkeyphrases short phrase indicate coreinformation document show figure keyphrase generation problem focus onautomatically produce keyphrase ofkeyphrases give document ofthe condensed expression keyphrases benefitvarious downstream application include opinionmining berend wilson doc1our code available http //github.com/chen-wang-cuhk/exhird-dkg.input document noninvasive diagnostic device develop toassess vascular origin severity penile dysfunction wasdesigned study mathematical model penilehemodynamics preliminary experiment healthy young volunteers.… simulations mathematical model show device iscapable differentiate arterial insufficiency venous leakand indicate severity …keyphrases erectile dysfunction arterial insufficiency venous leak veno-occlusivemechanism mathematical model hemodynamics figure example input document expected keyphrase output keyphrase generation problem present keyphrases appear documentare underlined.ument clustering hulth megyesi andtext summarization wang cardie document categorizedinto group present keyphrase appearsin document absent keyphrase doesnot appear document recent generativemethods apply attentional encoderdecoder framework luong bahdanauet copy mechanism al.,2016 predict presentand absent keyphrases generate multiplekeyphrases input document methodsfirst beam search generate huge numberof keyphrases e.g. pick nranked keyphrases final prediction thus inother word method predict fixednumber keyphrases documents.however practical situation appropriate number keyphrases varies accord thecontent input document simultaneouslypredict keyphrases determine suitable number keyphrases yuan adopt sequential decoding method greedy search togenerate sequence consisting predictedkeyphrases separator example produce sequence hemodynamics erectile dysfunction sep1096arator produce token decode process terminates final keyphrase prediction obtain split sequence byseparators however drawback tothis method first sequential decoding methodignores hierarchical compositionality existingin keyphrase keyphrase composed ofmultiple keyphrases keyphrase consist ofmultiple word work examine hypothesis generative model predict moreaccurate keyphrases incorporate knowledge hierarchical compositionality thedecoder architecture second sequential decoding method tend generate duplicated keyphrases.it simple design specific post-processing rulesto remove repeated keyphrases generatingand remove repeat keyphrases waste timeand compute resource address twolimitations propose novel exclusive hierarchical decoding framework includesa hierarchical decoding process exclusionmechanism.our hierarchical decoding process design toexplicitly model hierarchical compositionalityof keyphrase compose phrase-leveldecoding word-level decoding step determines aspect documentto summarize base document content aspect summarize previouslygenerated keyphrases hidden representationof captured aspect employ initialize thewd process process conductedunder step generate keyphraseword word repeat meet stop condition method pdand attend document content gather contextual information moreover attention scoreof step rescale correspondingpd attention score purpose attentionrescaling indicate aspect focus onby current step.we also propose kind exclusion mechanism i.e. soft hard avoidgenerating duplicated keyphrases either softone hard hierarchicaldecoding process thewd process hierarchical decoding besides collect previously-generated kkeyphrases predefined window size.the soft exclusion mechanism incorporate inthe training stage exclusive loss employ encourage model generate different first word current keyphrase firstwords collected keyphrases however thehard exclusion mechanism inferencestage exclusive search forcewd produce different first word firstwords collected keyphrases motivation statistical observation document large benchmark keyphrases individual document havedifferent first word moreover since keyphraseis usually compose three word thepredicted first word significantly affect prediction following keyphrase word thus ourexclusion mechanism boost diversity ofthe generated keyphrases addition generatingfewer duplication also improve chanceto produce correct keyphrases beenpredicted yet.we conduct extensive experiment four popular real-world benchmark empirical resultsdemonstrate effectiveness hierarchicaldecoding process besides soft thehard exclusion mechanism significantly reducethe number duplicated keyphrases furthermore employ hard exclusion mechanism model consistently outperform sotasequential decoding baseline four benchmarks.we summarize main contribution follow best knowledge first designa hierarchical decoding process keyphrasegeneration problem propose novel exclusion mechanism avoid generate duplicatedkeyphrases well improve generation accuracy method consistently outperformsall sota sequential decode method multiple benchmark setting.2 related work2.1 keyphrase extractionmost traditional extractive method wittenet mihalcea tarau focus onextracting present keyphrases input document follow two-step framework firstextract plenty keyphrase candidate handcraft rule medelyan theyscore rank candidate base eitherunsupervised method mihalcea tarau supervise learning method nguyen kan,2007 hulth recently neural-based se1097quence label method gollapalli luan zhang alsoexplored keyphrase extraction problem however extractive method predict absent keyphrase also essential part akeyphrase set.2.2 keyphrase generationto produce present absent keyphrases meng introduce generative model copyrnn base attentionalencoder-decoder framework bahdanau al.,2014 incorporate copy mechanism guet wide range extension copyrnn recently propose chen wang chen zhao zhang rely beamsearch over-generate keyphrases withlarge beam size select e.g. fiveor rank final prediction thatmeans over-generated method alwayspredict keyphrases input documents.nevertheless real situation keyphrase number determine document contentand vary among different documents.to yuan introduce newsetting model predict multiplekeyphrases simultaneously decide suitablekeyphrase number give document twomodels sequential decoding process catseqand catseqd propose yuan catseq also attentional encoder-decodermodel bahdanau copy mechanism adopt trainingand inference setup setting catseqd extension catseq orthogonalregularization bousmalis targetencoding lately chan propose areinforcement learn base fine-tuning method fine-tune pre-trained model adaptive reward generate sufficient accurate keyphrases follow settingwith yuan propose exclusivehierarchical decoding method problem.to best knowledge first timethe hierarchical decoding explore kgproblem different hierarchical decodingin area yarats lewis,2018 chen zhuge werescale attention score step thecorresponding attention score provide aspectguidance generate keyphrases moreover either soft hard exclusion mechanism innovatively incorporate decoding process toimprove generation diversity.3 notations problem definitionwe denote vector matrix bold lowercase uppercase letter respectively sets aredenoted calligraphy letter torepresent parameter matrix.we define keyphrase generation problem asfollows input document output akeyphrase thekeyphrase number yiare sequence word i.e. andyi yilyi wordnumbers correspondingly.4 methodologywe first encode word document intoa hidden state employ exclusive hierarchical decode show figure producekeyphrases give document hierarchical decoding process consist phrase-level decoding word-level decoding pdstep decide appropriate aspect summarizebased context document theaspects summarize previous step hidden representation captured aspect isemployed initialize process generatea keyphrase word word processterminates produce eowd token ifthe process output eopd token wholehierarchical decoding process stop andwd attend document content attention score re-weight attentionscore provide aspect guidance improve thediversity predicted keyphrases incorporate either exclusive loss training i.e. thesoft exclusion mechanism exclusive searchmechanism inference i.e. hard exclusionmechanism sequential encoderto obtain context-aware representation eachdocument word employ two-layered bidirectional document encoder bigru −→mk−1 ←−mk+1 wherek embed vectorof dimension −→mk ←−mk rd1098𝐡1,0𝐡1,1𝐡1,2𝐡1,3𝐡2,0𝐡2,1𝐡2,2𝐡4,0𝐡3,0𝐡3,1𝐡3,2𝐡3,3 eowd eopd eowd eowd neopd neopd neopd y11y21y12 y13y23𝐡1 𝐡4𝐡3𝐡0phrase-leveldecoding 𝐡3𝐡3,0𝐡3,1𝐡3,2 neopd y13wd-attentionpd-attentionሚ𝐡2,2𝐡2𝜷3 𝛽3,1 𝑦13el/esሚ𝐡3,1…ሚ𝐡3,0ሚ𝐡3,1 …word-leveldecoding simplified framework exclusive hierarchical decoding intermediate step predict 𝑦13el/es el/es el/esfigure illustration exclusive hierarchical decoding hidden state i-th step thecorresponding j-th hidden state neopd token mean eowd token meanswd terminates eopd token mean whole decoding process finish represent encoded hidden state document pd-attention wd-attention attentionmechanisms respectively attention score i-th step attentionalvector el/es indicate either exclusive loss exclusive search incorporated.is encoded context-aware representation xk.here mean concatenation.4.2 hierarchical decoderour hierarchical decoding process control bythe hierarchical decoder utilize phraselevel decoder word-level decoder handlethe process process respectively.we present hierarchical decoder first thenintroduce exclusion mechanism decoder hidden state attentional vectorsare d-dimensional vectors.4.2.1 phrase-level decoderwe adopt unidirectional layer phraselevel decoder process lastpd step finish phrase-level decoder willupdate hidden state follow =−−→gru1 h̃i−1 hi−1 h̃i−1 attentional vector step step e.g. h̃2,2in figure regard hidden representation captured aspect i-th step.h0 initialize document representation −→mlx ←−m1 initialize zeros.in pd-attention process attentionalscore βi,1 βi,2 compute fromthe follow attention mechanism employ hias query vector /lx∑n=1exp tw1mn word-level decoderwe choose another unidirectional layer toconduct word-level decoding i-th pdstep word-level decoder update hiddenstate first =−−→gru2 eyij−1 attentional vector step eyij−1 de-dimensionalembedding vector yij−1 token definehi,0 =−−→gru2 currenthidden state phrase-level decoder zerovector embedding start token.then attentional vector compute tanh =lx∑k=1ᾱ k∑lxn=1 original attention scorewhich compute similar except a1099new parameter matrix employedas query vector purpose rescalingoperation indicate focused aspectof current step step.finally utilize predict probability distribution current keyword copymechanism gijp sigmoid copygate softmax w3h̃i r|v| isthe probability distribution predefined vocabulary xk=yijᾱ isthe copying probability distribution whichis word appear document r|v∪x final predict probability distribution finally greedy search apply toproduce current token.the process terminates produce eowd token whole hierarchical decodingprocess word-level decoder produce eopd token step i.e. predictedas eopd traininga standard negative log-likelihood loss employedas generation loss train hierarchical decoding model −|ȳ|∑i=1lȳi∑j=0logp ȳij ȳi−1 ȳij−1 ȳi−1 ȳi−1 targetkeyphrases previously-finished step andȳij−1 ȳi0 ȳij−1 target keyphrase wordsof previous step i-th step whentraining original target keyphrase extendedwith neopd token eowd token i.e. neopd yilyi eowd besides eopd token also incorporate target indicate whole decoding process teacher forcing employ training.4.4 soft hard exclusion mechanismsto alleviate duplication generation problem wepropose soft hard exclusion mechanisms.either incorporate hierarchical decoding process form kind ofexclusive hierarchical decoding method.soft exclusion mechanism exclusive loss introduce training stage shownalgorithm training exclusive lossrequire window size target keyphrases ȳ|ȳ| predicted probability distribution j-th step i-th stepwhere |ȳ| lȳi firstly exclusive loss j-th step thei-th step compute follows.2 then4 =∑i−1idx=i−kel ȳidxj ȳidxj else6 secondly exclusive loss whole decoding process calculate jel.9 finally joint loss +lel employ trainthe model.algorithm inference exclusive searchrequire window size first word ofpreviously-predicted keyphrases yi−11 current step index predicted probability distribution current step.1 then3 i−kes i−kes yidxj for6 return predicted word forcurrent step.in algorithm line mean thecurrent step predict first word akeyphrase short exclusive loss punishesthe model tendency generate samefirst word current keyphrase firstwords previously-generated keyphrases withinthe window size kel.hard exclusion mechanism exclusivesearch introduce inference stageas show algorithm exclusive searchmechanism force word-level decoding predict different first word first word ofpreviously-predicted keyphrases within windowsize keyphrase usually threewords first word significantly affect prediction following word therefore boththe soft hard exclusion mechanism canimprove diversity generated keyphrases.5 experiment setupour model implementation base theopennmt system klein pytorch paszke experiments all1100models repeat three different randomseeds averaged result reported.5.1 datasetswe employ four scientific article benchmarkdatasets evaluate model includingkp20k meng inspec hulth,2003 krapivin krapivin semeval following previouswork yuan chen weuse training kp20k train models.after remove duplicate data maintain509,818 data sample training validation testing set.after training test model test datasets four benchmark datasetstatistics show table total validation testinginspec statistic validation test datasets.5.2 baselineswe focus comparison state-of-the-artdecoding method choose following generation model setting baseline transformer vaswani atransformer-based sequence sequence modelincorporating copy mechanism.• catseq yuan rnn-based attentional encoder-decoder model copy mechanism encoding decoding sequential.• catseqd yuan extension ofcatseq incorporate orthogonal regularization bousmalis target encodinginto sequential decoding process improvethe generation diversity accuracy.• catseqcorr chan another extension catseq incorporate sequentialdecoding coverage andreview mechanism boost generation diversity accuracy method adjustedfrom chen setting.in paper propose novel model thatare denote follow exhird-s. exclusive hierarchicaldecoding model soft exclusion mechanism experiment window size isselected tune kp20k validation dataset.• exhird-h. exclusive hierarchicaldecoding model hard exclusion mechanism experiment value windowsize select inspec krapivin semeval kp20k respectively aftertuning corresponding validation datasets.we choose bilinear attention luonget copy mechanism seeet models.5.3 evaluation metricswe engage recently proposedin yuan evaluation metric compare predict keyphrasesby model ground-truth keyphrases whichmeans fixed cutoff prediction therefore consider number ofpredictions.we also another evaluation metric.when number prediction less five randomly append incorrect keyphrases itobtains five prediction instead directly usingthe original prediction adopt anappending operation become samewith prediction number lessthan five.the macro-averaged scoresare report determine whether twokeyphrases identical keyphrases arestemmed first besides duplicatedkeyphrases remove stemming.5.4 implementation detailsfollowing previous work meng yuanet chen chan al.,2019 lowercase character tokenize thesequences replace digit digit token similar yuan training present keyphrase target sort accord order first occurrence thedocument absent keyphrase target areput sorted present keyphrase target start start the1101model inspec krapivin semeval kp20kf1 present keyphrase prediction result model datasets best result bold tablesof paper subscript represent corresponding standard deviation e.g. indicate inspec krapivin semeval kp20kf1 absent keyphrase prediction result model datasets best result bold. neopd token present absent keyphrasesrespectively employ eowd tokenfor present absent keyphrases isused eopd token.the vocabulary token share encoder decoder hidden state encoderlayers initialize zero training stage randomly initialize trainable parametersincluding embed uniform distribution −0.1 batch size maxgradient norm initial learn rate as0.001 dropout adam kingmaand optimizer learning rate decay half perplexity kp20kvalidation stop decrease early stop isapplied training inference theminimum phrase-level decode step themaximum results analysis6.1 present absent keyphrasepredictionswe show present absent keyphrase prediction result table table correspondingly indicate table theexhird-s model exhird-h outperformthe state-of-the-art baseline metric demonstrate effectiveness ourexclusive hierarchical decoding method besides exhird-h model consistently achieve bestresults present absent keyphrase premodel inspec krapivin semeval kp20ktransformer average dupratios predictedkeyphrases datasets score thebetter performance.diction datasets2.6.2 duplication ratio predictedkeyphrasesin section study model capability ofavoiding produce duplicated keyphrases duplication ratio denote dupratio definedas follow dupratio duplication prediction mean number instance thedupratio report average dupratio documentin table table observe thatour exhird-s exhird-h consistently andsignificantly reduce duplication ratio alldatasets moreover also find exhirdh model achieve duplication ratio onall datasets.2we also simultaneously incorporate soft andthe hard exclusion mechanism hierarchical decodingmodel still underperform exhird-h.1102model inspec krapivin semeval kp20k akoracle results average number predictedunique keyphrases document number present absent keyphrases respectively oracle gold average keyphrase number.the closest value oracle bold.model present absent dupratiof1 akexhird-h ablation study exhird-h model onsemeval dataset mean hierarchicaldecoder replace sequential decoder theexclusive search still incorporate represent hierarchical decoding model without utilizingexclusive search mechanism.6.3 number predicted keyphraseswe also study average number uniquekeyphrase prediction document duplicatedkeyphrases remove result shownin table main finding themodels generate insufficient number uniquekeyphrases datasets especially predict absent keyphrases also observe thatour method improve number uniquekeyphrases large margin extremelybeneficial solve problem insufficient generation correspondingly also lead overgenerate keyphrases ground-truthfor case problem suchas present keyphrase prediction krapivinand kp20k datasets leave solve overgeneration present keyphrases krapivin andkp20k future work.6.4 exhird-h ablation studysince exhird-h model achieve best performance almost metric select itas final model probe subtly thefollowing section order understand effect component exhird-h conductan ablation study report result thesemeval dataset table observe hierarchical decodingprocess exclusive search mechanism helpkespresent absent dupratiof1 akoracle results exhird-h kp20k differentwindow size exhird-h equalsto mean take first wordsof previously-predicted keyphrases consideration dupratio average dupratio perdocument show average number groundtruth keyphrases oracle row.ful generate accurate present absentkeyphrases besides also find significant performance margin duplication ratioand keyphrase number mainly theexclusive search mechanism.6.5 exhird-h window size exclusivesearchfor comprehensive understanding exclusive search mechanism exhird-h model also study effect window size conduct experiment kp20k dataset andlist result table note large window size lead dupratio anticipate becausethe exclusive search observe previouslygenerated keyphrases avoid generate duplicated keyphrases large kesis dupratio absolute zero becausewe stem keyphrases determine whetherthey duplicate besides also find thatlarger lead well score reason score append incorrectkeyphrases obtain five prediction number prediction less five large kesleads predict unique keyphrases appendless absolutely incorrect keyphrases improvethe chance output accurate keyphrases.however generate unique keyphrases mayalso lead incorrect prediction willdegrade score since considersall unique prediction without fixed cutoff.1103model present absent dupratiof1 akoracle transformer results apply exclusive search toother baseline kp20k mean exclusive search applied.6.6 exhird-h incorporate baselines withexclusive searchour exclusive search general method canbe easily apply model section study effect exclusive search onother baseline model show experimentalresults kp20k dataset table table note effect exclusive search baseline similar effect hierarchical decoding also seeour exhird-h still achieve best performanceon metric even baseline alsoincorporated exclusive search exhibitsthe superiority hierarchical decoding again.6.7 exhird-h case studywe display prediction example figure exhird-h model generate accuratekeyphrases document compare fourbaselines besides also observe much less repeated keyphrases generate exhirdh instance baseline produce thekeyphrase debug least three time however exhird-h generate whichdemonstrates propose method morepowerful avoid duplicate keyphrases.7 conclusion future workin paper propose exclusive hierarchical decoding framework keyphrase generation.unlike previous sequential decoding method ourhierarchical decoding consist phrase-leveldecoding process capture current aspect tosummarize word-level decoding process togenerate keyphrases base captured aspect.besides also propose soft hard exclusion mechanism enhance diversity thegenerated keyphrases extensive experimental result demonstrate effectiveness methsoc hw/sw co-verification base debugging technique purpose –increasingly complex sophisticated vlsi design couple withshrinking design cycle require short verification time andefficient debug method hw/sw co-verification techniqueseems draw balance design test still residesin fpga remain hard debug purpose paperis study run-time debug methodology fpgabased co-verification system …targets computer hardware computer software co-verification debug transformer co-verification debugging fpga catseq debugging logic programming catseqd debugging design verification catseqcorr debugging computer design exhird-h verification debugging simulation co-verification computer software logic testing figure example generate keyphrases baseline exhird-h correct prediction arebold present keyphrases underlined thedigit parenthesis represent frequency thecorresponding keyphrase generate model e.g. debugging mean keyphrase debug generate three time model interesting future direction explorewhether beam search helpful model.acknowledgmentsthe work describe paper partiallysupported research grants council thehong kong special administrative region china cuhk collaborative research fund c5026-18gf would like thank ourcolleagues comment
neural-based end-to-end approach naturallanguage generation structureddata knowledge data-hungry makingtheir adoption real-world application difficult limited data work propose task few-shot natural languagegeneration motivated human tend tosummarize tabular data propose simpleyet effective approach show onlydemonstrates strong performance also provide good generalization across domain thedesign model architecture base ontwo aspect content selection input dataand language model compose coherentsentences acquire priorknowledge training example across multiple domain show approach achieve reasonable performancesand outperform strong baseline anaverage bleu point improvement.our code data find http //github.com/czyssrs/few-shot-nlg1 introductionnatural language generation structure data knowledge gatt krahmer,2018 important research problem various application example taskoriented dialog question answering al.,2017 ghazvininejad saha interdisciplinary application medicine hasanand farri cawsey healthcare hasan farri dimarco great potential automatic system wide range real-life application recently deep neural network base systemshave develop thee2e challenge novikova weathergov liang well complexones wikibio rotowire wiseman compared totraditional slot-filling pipeline approach suchneural-based system greatly reduce feature engineering effort improve text diversity wellas fluency.although achieve good performance onbenchmarks challenge novikovaet wikibio lebret performance depend large trainingdatasets e.g. table-text training pair forwikibio lebret single domain.such data-hungry nature make neural-based nlgsystems difficult widely adopt real-worldapplications significant manual datacuration overhead lead formulate aninteresting research question:1 significantly reduce humanannotation effort achieve reasonableperformance neural model make best generativepre-training prior knowledge generate text structure data motivated propose task fewshot natural language generation give ahandful labeled instance e.g. training instance system require producesatisfactory text output e.g. bleu tothe best knowledge problem nlgcommunity still remain under-explored herein propose simple effective approachthat generalize across different domains.in general describe information table need skill compose coherent faithful sentence skill select copy factual content table learnedquickly read handful table otheris compose grammatically correct sentence thatbring fact together skill re184input tableattribute value name walter extranationality germanoccupation aircraft designerand manufacturer table encoderattention weightswalter extra pre-trained language modelwalter extra germanname name nationaltilytable valuesattribute namesposition informationwalter extra swicthpolicyname name matchingfigure overview approach base framework switch policy pre-trained language model serve asthe generator follow encoder architecture simple term implementationand parameter space need learn scratch large give few-shot learning setting.stricted domain think latent switch help alternate twoskills produce factually correct coherentsentences pre-trained language model chelba radford al.,2019 innate language skill providesstrong prior knowledge compose fluent coherent sentence ability switchand select/copy table learn successfully training instance freeingthe neural model data-intensive training previous best perform method base onlarge train data whichdoes apply switch mechanism trainsa strong domain-specific language model performvery poorly few-shot setting.since operate highly datarestricted few-shot regime strive simplicityof model architecture simplicity also impliesbetter generalizability reproducibility realworld application crawl multi-domain tableto-text data wikipedia training/testinstances training instance ourmethod achieve reasonable performance.in nutshell contribution summarizedas following propose research problem fewshot great potential benefita wide range real-world applications.• study different algorithm proposedproblem create multi-domain table-totext dataset.• propose algorithm make theexternal resource prior knowledge significantly decrease human annotation effortand improve baseline performance anaverage bleu various domains.2 related work2.1 structured dataas core objective many application natural language generation structure data/knowledge study many years.early traditional system follow pipelineparadigm explicitly divide generation intocontent selection macro/micro planning surface realization reiter dale apipeline paradigm largely rely template andhand-engineered feature many work beenproposed tackle individual module suchas liang walker al.,2009 later work konstas lapata investigate model context selection andsurface realization unified framework.most recently success deep neuralnetworks data-driven neural base approacheshave include end-to-end method jointly model context selection surface realization wiseman al.,2018 puduppully data-drivenapproaches achieve good performance severalbenchmarks like challenge novikova al.,2017 webnlg challenge gardent wikibio lebret however theyrely massive amount training data elsaharet propose zero-shot learning question generation knowledge graph theirwork applies transfer learn forunseen knowledge base type base onesand textual context still require largein-domain training dataset different fromour few-shot learning setting propose low-resource table-to-text generation with1851,000 pair example large-scale target-sideexamples contrast setting tens tohundreds paired training example require meanwhile without need target example especially important real-world usecases large target-side gold referencesare mostly hard obtain therefore task ismore challenging close real-world settings.2.2 large scale pre-trained modelsmany current best-performing method forvarious task adopt combination pretraining follow supervised fine-tuning usingtask-specific data different level pre-traininginclude word embeddings mikolov pennington peters sentence embeddings mikolov kiroset recently language model base pre-training like bert devlin al.,2018 gpt-2 radford suchmodels pre-trained large-scale open-domaincorpora provide down-streaming task withrich prior knowledge boost performance paper adopt idea employ pre-trained language model endowin-domain model language modelingability well learn shottraining instances.3 method3.1 problem formulationwe provide semi-structured data tableof attribute-value pair ni=1 riand either string/number phrase asentence value represent sequenceof word mj=1 word haveits correspond attribute name positioninformation word value sequence thetarget generate natural language descriptionbased semi-structured data provide withonly handful train instances.3.2 base framework switch policywe start field-gated dual attention modelproposed achievesstate-of-the-art performance bleu wikibiodataset method lstm decoder withdual attention weight first apply switch policy decouple framework table contentselection/copying language model base generation inspired pointer generator al.,2017 time step maintain soft switchpcopy choose generate softmaxover vocabulary copying input table value attention weight probabilitydistribution.pcopy sigmoid wcct +wsst +wxxt aithi encoder hidden state decoder input stateand attention weight respectively time step t.wc trainable parameters.the pointer generator learn alternate betweencopying generate base large trainingdata show advantage copy out-ofvocabulary word input task training data limited many tablevalues need explicitly teach model copy generate.therefore provide model accurate guidanceof behavior switch match targettext input table value position ofwhere copy position maximize thecopy probability pcopy additional loss term.our loss function λ∑wj∈mm∈ pjcopy original loss model output target text target token position input table value list definedin section mean matched phrase hyperparameter weight copy lossterm also concatenate decoder input withits match attribute name position informationin input table calculate pcopy pre-trained generatorwe pre-trained language model generator serve innate language skill dueto vocabulary limitation training instance leave pre-trained word embeddingfixed fine-tuning parameter pretrained language model generalizewith token unseen training.figure show model architecture usethe pre-trained language model gpt-21 proposedin radford transformer final hidden state transformeris calculate attention weight copy1https //github.com/openai/gpt-2186domain humans books songs training instance base-original switch switch lm-scratch switch bleu-4 result three domain base-original original method base applies pre-trainedword embed base+switch switch policy base+switch+lm-scratch make architecture method train model scratch without pre-trained weight generator template manually craft templatesswitch pcopy first embed attributevalue list serve context generation inthis architecture generator fine-tuned frompre-trained parameter encoder attention part learn scratch initial geometry side different therefore weneed apply large weight copy loss pcopy give model strong signal teach tocopy fact input table.4 experiment4.1 datasets experiment setupthe original wikibio dataset lebret contain english wikipedia article wellknown human wiki infobox serve asinput structure data first sentence thearticle serve target text demonstrate generalizability collect datasets domain books songs crawl wikipediapages filter cleanup with23,651 instance books domain instance songs domain2 together humans domain original wikibio dataset forall three domains conduct experiment vary training dataset size and500 rest data validation test weight copy loss term isset parameter setting foundin appendix deal vocabulary limitationof few-shot training model adopt thebyte pair encoding sennrich subword vocabulary radford compare propose method otherapproaches investigate section serve asthe baseline base-original original model2note target text sometimes contain information infobox scope fewshot generation work therefore filter thedatasets remove rare word infobox.check dhingra related study issueon wikibio datasetin base architecture addition applies pre-trainedword embedding training base+ switch switch policy base switch+ lm-scratch make architecture asour method except train model scratchinstead pre-trained weight generator.template template-based non-neural approach manually craft domain.4.2 results analysisfollowing previous work wefirst conduct automatic evaluation bleu4 show table rouge-4 f-measure result follow trend bleu-4 result show appendix b.as original model baseoriginal obtain stateof-the-art result wikibio full performsvery poorly few-shot setting generatesall token softmax vocabulary result severe overfitting limited training data result behind template-basedbaseline switch policy base+switch firstbrings improvement average point indicate content selection ability easy learn handful training instance however form verylimited fluent sentence augmentation pre-trained language model modelbase+switch+lm bring significant improvement average bleu points.we provide sample output method using200 train instance table show effect copy switch losspcopy introduce section give model astronger signal learn copy input table.ma propose pivot model forlow-resource paired example andlarge-scale target-side example compare our187attribute value attribute valuename andri fullname andri ibobirth date april birth place sentani jayapura indonesiaheight currentclub persipura jayapuraposition defender gold reference andri igbo bear april indonesian footballer currently play persipura jayapura indonesia superleague text different methodsbase vasco emanuel freitas born december kong kong ahong kussian football player currently play hong kong first divisionleague side pegasus andri igbo andri igbo april international cricketer andri igbo bear april isan indonesian football defender currently play forpersipura jayapura sample input table generate summary fromthe test humans domain training instance training instance switch copy loss pcopy ablation study effect copy loss term onhumans domain measure bleu-4 loss term bringsan average improvement bleu points.method pivot model table note thathere train evaluate model original wikibio dataset work orderto maintain size target side examples fortheir settings. paired training instance comparison pivot model method additional large-scale targetside example method require additional target sidedata achieve good performance.human evaluationwe also conduct human evaluation study usingamazon mechanical turk base aspect factual correctness language naturalness weevaluate sample evaluation unit assign worker eliminate human variance.the first study attempt evaluate well thegenerated text correctly convey information thetable count number fact textsupported table contradict ormissing table columnsof table show average number supportingand contradict fact method comparingto strong baseline gold reference.the second study evaluate whether generatedtext grammatically correct fluent regardless factual correctness conduct pairwisecomparison among method calculate theaverage time method choose betterthan another show column table method bring significant improvement overthe strong baseline tukey hsdtest measure copy loss term furtheralleviates produce incorrect fact languagenaturalness result method without copyloss slightly well evaluation doesnot consider factual correctness thus generatedtexts wrong fact still high score.see appendix detail evaluationprocedure. supp cont scoregold reference switch switch copy loss pcopy human evaluation result average number support fact column large good contradictingfacts column small good language naturalness score column large good conclusionin paper propose research problemof few-shot natural language generation approach simple easy implement achieve strong performance various domain ourbasic idea acquire language model priorcan potentially extend broad scope ofgeneration task base various input structureddata knowledge graph query etc.the deduction manual data curation effort forsuch task great potential importance formany real-world applications.acknowledgmentwe thank anonymous reviewer theirthoughtful comment thank shuming forreleasing process data code pivotmodel research support intelai faculty research grant author solelyresponsible content paper theopinions express publication reflectthose funding agencies.188
question answer important aspect open-domain conversational agent garner specific research focus conversational convqa subtask notable limitation recent convqa effort theresponse answer span extraction fromthe target corpus thus ignore natural language generation aspect high-qualityconversational agent work propose method situate responseswithin seq2seq approach generate fluent grammatical answer response whilemaintaining correctness technical perspective data augmentation generate training data end-to-end system.specifically develop syntactic transformations produce question-specific candidate answer response rank usinga bert-based classifier devlin evaluation squad data rajpurkar demonstrate propose model outperform baseline coqa andquac model generate conversational response show model scalability conduct test coqa dataset.11 introductionfactoid question answer recently enjoy rapid progress increase availability large crowdsourced datasets e.g. squad rajpurkar marco bajaj al.,2016 natural questions kwiatkowski al.,2019 train neural model significant advance pre-training contextualized representation massive text corpus e.g. elmo peters bert devlin success recent work examines conversational convqa system capable interact user multiple turns.1the code data available athttps //github.com/abaheti95/qadialogsystem.large crowdsourced convqa datasets e.g. coqa reddy quac choi consist dialogue crowd worker whoare prompt answer sequence question regard source document although theseconvqa datasets support multi-turn interaction response mostly limited toextracting text span source document anddo readily support abstractive answer yatskar,2019a response copy directly awikipedia article provide correct answer user question sound natural aconversational setting address challenge develop seq2seq model generate fluentand informative answer response conversationalquestions.to obtain data need train model rather construct yet-another crowdsourcedqa dataset transform answer exist dataset fluent response data augmentation specifically synthetically generatesupervised training data convert questionsand associate extractive answer squadlike dataset fluent response syntactictransformations over-generatea large candidate response abert-based classifier select best response asshown half figure over-generation selection generatesfluent response many case brittleness ofthe off-the-shelf parser syntatic transformation rule prevent direct case notwell-covered mitigate limitation generate augmented training dataset thebest response classifier train end-toend response generation model base pointergenerator networks andpre-trained transformers large amount ofdialogue data dialogpt d-gpt zhang al.,2019 §3.2 §3.3 empirically demon192q hizb uttahrir fail pull abloodless coup in1974 egyptparser +syntacticrulesr1 fail egypt pull bloodless coupr2 fail pull bloodless coup in1974 egypt…rm fail pull bloodless coup egyptbestresponseclassifierrm fail pulloff bloodless coupegyptq hizb uttahrir fail pull abloodless coup in1974 egyptsequence-to-sequencer fail pulloff bloodless coupin egyptaugment training data end-to-end setupover-generate select best response end-to-end response generation pointer generator network dialogpt d-gpt figure overview method generate conversational response give first method syntactic transformations over-generate list response good question parsetree best response classifier select suitable response list second method thispipeline augment training data train seq2seq network d-gpt §3.1 final seq2seqmodel end-to-end scalable easy train performs good first method exclusively.strate propose model capableof generate fluent abstractive answer bothsquad coqa.2 generating fluent responsesin section describe approach construct corpus question answer thatsupports fluent answer generation half figure framework overgenerateand rank previously context question generation heilman smith wefirst overgenerate answer response pairsusing §2.1 rank responsesfrom best worst response classificationmodels describe §2.2 later describehow augment exist datasets fluentanswer response best responseclassifier augment dataset fortraining transformer models.2.1 syntactic transformations first step apply syntactic transformations question parse tree alongwith expert answer phrase produce multiplecandidate response work effectively accurate question essential weuse stanford english lexparser2 klein manning train section questionbank judge amongst othercorpora however parser still fail recognize∼ question neither sbarq sqtag assign erroneous parse tree wesimply output expert answer phrase single2https //nlp.stanford.edu/software/parser-faq.html zresponse remain question processedvia following transformation over-generatea list candidate answer verb modification change tense main verb base onthe auxiliary verb simplenlg gatt reiter pronoun replacement substitutethe noun phrase pronoun fixed list fixing preposition determiner find thepreposition determiner question parsetree connect answer phrase addall possible preposition determiner miss response generation using tregex andtsurgeon levy andrew compile response combine component previoussteps answer phrase case thereare multiple option step number option explode bestresponse classifier describe winnow.an example process show figure response classification baselinesa classification model select best responsefrom list st-generated candidate giventhe training dataset describe §2.3 nquestion-answer tuples list ofcorresponding response rimi thegoal classify response good.the probability response good laterused ranking experiment differentmodel objective describe logistic assume response eachqi independent model classify response separately assign good response qi.the logistic loss give by∑ni=1∑mij=1 +193sbarqwhnp sqnp vpppr1 netherlands rise philip verbtransformationinserting missingpreposition anddetermineranswer phraseplacementr2 rise withpronounoptional ppremoval…q year netherlands rise philip figure example syntactic transformations inaction question year netherlands riseup philip answer using question parse tree modify verb rise basedon auxiliary verb miss preposition determiner blue combine thesubject component answer phrase green generate candidate another candidate swap subject pronoun purple transformation also optionally removeprepositional-phrases show orange inthe figure show candidate realitythe transformation generate many different candidate include many implausible ones.e−yij∗f label discuss §2.3 annotator expect miss good responsessince good answer often similar differ single preposition orpronoun therefore explore ranking objective calculate error base marginwith incorrect response rank abovecorrect collins withoutloss generality assume betterthan response since themodel rank high otherresponses margin error define softmaxloss as∑ni=1 +∑mij=2 e−mij experiment following feature basedand neural model loss function language model baseline response areranked normalized probability a3-gram train gigaword corpus withmodified kneser-ney smoothing.3 responsewith high score classify othersas model linear classifier featuresinspired heilman smith wanet implement similar linearmodels sentence pair classification tasks.specifically following features:3http //www.keithv.com/software/giga/• length features word length questionqi answer-phrase response rij• wh-word features feat whose orhow present negation features feat ornone present n-gram features normalized probability perplexity rij• grammar features node count qiand syntactic parse trees• word overlap features three featuresbased fraction word overlap qiand precision =overlap |qi| recall =overlap |rij harmonic meandecomposable attention sentencepair classifier parikh referredas model find attention base wordalignment input pair premise hypothesis case question response aggregate feedforward network apart fromstandard vector embeddings also experimentwith contextualized elmo peters embed model version implement allennlp gardner lastly bert-base uncasedmodel devlin sentence pair classification model take question responserij separate special token predicts response suitable unsuitable.in case number response generate question could high as5000+ therefore train modelwith pre-trained contextualized embeddings suchas elmo bert model softmax losssetting backpropagation require compute andstoring hidden state different responses.to mitigate issue strided negativesampling training first separate thesuitable response remain unsuitable response divide responsesfor small batch responses.each batch comprise suitable response randomly choose sample unsuitable response ensure unsuitableresponses least training shuffle create small batchesby take stride size da+elmo bert train softmax loss test time com194pute logits normalize across allresponses.2.3 training data response classificationin section describe detail training validation test data develop thebest response classifier model create supervised data choose sample train-setof squad dataset rajpurkar contains human-generated questionsand answer span select wikipedia paragraph sample remove qapairs answer span word theytend non-factoid question complete sentence typically question also filter question cannotbe handle parser obvious parser error filtering take asample question generate list ofresponses total response develop annotation task amazon mechanical turk select best responsesfor question question theannotators select response list response correctly answer question soundsnatural seem human-like since list ofresponses question long annotator review select best hence implement searchfeature within response list annotator type partial response searchbox narrow option selection.to make easier also sort responsesby length encourage annotator selectrelatively short response find bebeneficial would prefer automatic qasystem terse verify annotatorsdidn cheat annotation design selectingthe first/shortest option also test shortestresponse baseline another baseline responseclassifier model first/shortest response inthe list select suitable.each question assign annotator therefore unique annotate response question decrease recallof gold truth data since than5 good correctly respond question hand annotator choose aunique suboptimal/incorrect response whichdecreases precision gold truth.after annotate question fromsquad sample randomly split data rtrain statistics training validation andtest curated squad training data.q denote question answer thesquad sample denote response generate mean number question denote number response whichare label respectively human annotation process.into train validation test question refer squad gold annotate data increase train dataprecision assign label response thatare mark best least different annotator hard constraint questionsfrom training data remove annotator mark unique response otherhand increase recall test validation retain annotations.4 assignlabel remain response even ofthem plausible result data split issummarized table response mark zero moreannotators least annotator selectthe response list consider amatch compute annotator agreement score divide number match total number annotation annotator using thisformula find average annotator agreement be0.665 annotator agreement score isweighted number annotated questions.2.4 evaluation response classificationas previously mention §2.3 datadoesn contain true positive since exhaustively find annotate goodresponses response list long additionally large class imbalance betweengood response make standard evaluation metric precision recall score andaccuracy potentially misleading gather additional insight regard well model rankscorrect response incorrect calculate4we find annotator high affinity ofchoosing first short response notthe best choice list reduce annotation errorswe another constraint short response beselected least different annotators.195classifier loss max-f1 pr-aucshortresp langmodel linear soft soft soft soft best response classifier result testdata shortresp stand shortest response baseline langmodel stand language model baseline linear stand linear model log. soft. loss column stand logistic softmaxloss respectively refers decomposable attention model parikh +elmo refers toadding pre-trained elmo embeddings model.precision f1,6 area underthe precision-recall curve pr-auc train allclassifier model training evaluate test data result evaluationis present table result show short response baseline shortresp perform model absolute difference depend model verify annotationis dominate presentation bias annotator select short first thelist response question languagemodel baseline langmodel perform even absolute difference demonstratingthat task unlikely trivial solution.the feature-based linear model show good performance train softmax loss beatingmany neural model term pr-aucand max-f1 inspect weight vector wefind grammar feature specifically number preposition determiner theresponse feature high weights.this probably imply important challenge task find right prepositionsand determiner response importantfeatures response length response s3-gram probability ostensible limitationof feature-based model fail recognize correct pronoun unfamiliar name entity thequestions.due small size train vanilla5p time correct response rank first6max maximum model achieve bychoosing optimal threshold curvedecomposable attention model unable tolearn good representation accordingly performs linear feature-basedmodel addition elmo embeddings appearsto help cope find damodel elmo embeddings well able topredict right pronoun named entity presumably pre-trained representation thebest neural model term bertmodel fine-tuned softmax loss last rowof table data-augmentation generationseq2seq model effective generationtasks however label question response pair train table areinsufficient train large neural model hand create large-scaledataset support fluent answer generation bycrowdsourcing inefficient expensive therefore augment squad response fromthe sts+bert classifier table create synthetic training dataset seq2seq model wetake pair squad train-setwhich handle question parser andsts rank candidate response thebert response classifier probability train withsoftmax loss ranking loss collins koo,2005 therefore question select thetop rank responses7 threshold theprobabilities obtain bert model werefer result dataset squad-synthetic consist instances.to increase size train data takethe pair natural questions kwiatkowskiet harvestingqa8 cardie,2018 instance samests+bert classifier technique pairscombined result dataset instance refer dataset.3.1 d-gpt variants baselinesusing result datasets trainpointer generator network al.,2017 dialogpt d-gpt zhang andtheir variant produce fluent answer response7at three response question8harvestingqa dataset contain pairsgenerated top-ranking wikipedia article thisdataset noisy question automatically generatedusing lstm base encoder-decoder model makesuse coreference information answer extractedusing candidate answer extraction module.196generator input generation model isthe question answer phrase theresponse corresponding generation target.pgn pgns widely seq2seq modelsequipped copy-attention mechanism capableof copy word input directly intothe generate output make well equippedto handle rare word name entity present inquestions answer phrase train bi-lstm opennmt toolkit klein data weadditionally explore pgns pre-training information initialize embed layer withglove vector pennington pretraining pair questions-onlysubset opensubtitles corpus9 tiedemann,2009 corpus contain questionresponse pair training pair inthe validation name pre-trained pgnmodel pgn-pre also fine-tune pgn-preon data generate additionalvariants.d-gpt dialogpt dialogue generative pretrained transformer zhang recently release large tunable automatic conversation model train reddit conversationlike exchange gpt-2 model architecture radford fine-tune d-gpton task datasets forcomparison also train gpt-2 datasetsfrom scratch without pre-training finally assess impact pre-training datasets pre-train gpt-2 question fromquestions-only subset opensubtitles data similar pgn-pre model gpt-2-premodel gpt-2-pre later fine-tune ssand datasets correspond variants.coqa baseline conversational questionanswering coqa reddy alarge-scale convqa dataset creatingmodels answer question pose conversational setting since generatingconversational response system issensible compare convqa systems.we pick best perform bert-basedcoqa model smrctoolkit al.,2019 baseline.10 refer model thecoqa baseline.quac baseline question answering context9http //forum.opennmt.net/t/english-chatbot-model-withopennmt/18410one performing model available code.is another convqa dataset modifiedversion bidaf model present choi al.,2018 second baseline instead seq2seqgeneration select span passage whichacts response version modelimplemented allennlp gardner refer model quac baseline.sts+bert baseline also compare generation model technique create thess train datasets responsesgenerated rank bert responseclassifier validate seq2seq model human annotate data table evaluation squad setto fair unbiased comparison createa question sample squad squad-dev-test unseen allthe model baseline sample contains∼ question handle bythe parser error question wedefault output answer-phrase response sts+bert baseline coqabaseline quac baseline model passage correspond question squad-dev-test responses.to demonstrate model operatein fully automate like coqa baseline quac baseline generate response answer span select bertbased squad model instead gold answerspan squad-dev-test automatic evaluation compute validationperplexity seq2seq generation model onsg data column table however validation perplexity weak evaluator generationmodels also lack human-generatedreferences squad-dev-test othertypical generation base automatic metric therefore amazon mechanical turk humanevaluation response judge annotator annotator identify response conversational answer questioncorrectly output answer-phrase allquestions trivially correct style responsegeneration seem robotic unnatural prolonged conversation therefore also theannotators judge response completesentence indiana sentencefragment indiana question andresponse pair show annotator five options197model data ppla correct answer7 complete-sentence- grammaticalitycoqa human evaluation result model baseline sample squad-dev-test firstthree stand baseline last stand oracle column stand validationperplexity value percentage response model belong specific option select annotators.based three property correctness grammaticality complete-sentence five option show table header.the best response complete-sentence whichis grammatical answer question correctly option option give insightsinto different model behavior response assign majority option select annotator aggregate judgment buckets.we present evaluation table compute inter-annotator agreement bycalculating cohen kappa cohen betweenindividual annotator assignment aggregated majority option average cohen kappa weight number annotation everyannotator substantial agreement result reveal coqa baseline theworst term option main reason thatis response generate fromthis baseline exact answer span therefore observe well option i.e.correct answer complete-sentence thequac baseline correctly select span-based informative response∼ time time however often select span passagewhich relate topic contain thecorrect answer option another problemwith baseline restrict inputpassage many always able find validspan answer question sts+bertbaseline well term option comparedto baseline limit stsand parser error mention earlier time baseline directly copy answerphrase response explain highpercentage option b.almost model perform good trainedwith data show additional datafrom natural questions harvestingqa help except model train data variant perform good sts+bertbaseline term option gpt-2 modeltrained data scratch performvery well small size trainingdata pretraining opensubtitiles question boost performance option gpt-2pre model variant option gpt-2 modelvariants best model however d-gpt whenfinetuned dataset retain thecorrect answer make less grammatical error option compare othermodels furthermore oracle answer perform even good last table showsthat d-gpt generate good quality responseswith accurate answer provide sampleresponses different model appendix a.3.3 evaluation coqain section test model ability generate conversational answer coqa coqa baseline predict answer thecoqa dataset consist passage seven different domain wikipedia conversational question answer those198model ecoqa human evaluation result d-gpt model train dataset coqa model sample of100 question answer filter coqa stand oracle answer options explainedin table header.passages conversational nature ofthis dataset question word like question out-of-domain model theyrequire entire context multiple turn ofthe conversation develop response otherout-of-domain question include unanswerable ∼0.8 yes/no question alsodon consider question answer word typically non-factoid question take random sample theremaining question sample contain question diverse domain outside thewikipedia model train thisincludes question take middle conversation example meet unfamiliar model performa human evaluation similar §3.2 sample compare coqa d-gpt trainedon dataset coqa prediction inputas answer-phrases result show table4.this evaluation reveals d-gpt modelis able successfully convert coqa answerspans conversational response thetime option d-gpt wrong answer18 time option input answer predict coqa baseline alsoincorrect time however oracleanswers able generate correct responses77 time option weighted average cohen kappa cohen score allannotators evaluation substantialagreement result demonstrate ability ourmodel generalize different domain generate good conversational response questionswhen provide correct answer spans.4 related workquestion generation well study problem community many machinelearning base solution heilmanand smith labutov al.,2015 serban reddy duet cardie comparison work explore opposite direction generate conversational humanlike answersgiven question feng also tosolve fluent answer response generation task restrict setting movie relate question with115 question pattern contrast generationmodels deal human generate questionsfrom domain.learning rank formulation answerselection system common practice frequently rely pointwise ranking model severyn moschitti garg discriminative re-ranking collins andkoo softmax loss closer learn pairwise ranking maximize multiclass margin correct incorrect answer joachims burges köppelet important distinction fromtrec-style answer selection st-generatedcandidate response semantic syntactic lexical variance make pointwise methodsless effective.question answering using crowd-sourcingmethods create datasets rajpurkar al.,2016 bajaj rajpurkar conversational datasets dinan andconvqa datasets choi reddyet elgohary saha al.,2018 largely drive recent methodological advance however model train convqadatasets typically select exact answer span insteadof generate yatskar instead create another crowd-sourced dataset task augment exist datasets include suchconversational answer response +bert train softmax loss.5 conclusionin work study problem generatingfluent responses context building fluent conversational agent proposean over-generate rank data augmentation procedure base syntactic transformations bestresponse classifier method modifythe squad dataset include conversational answer train seq2seqbased generation model human evaluation onsquad-dev-test show model generate199significantly well conversational response compare baseline coqa quac models.furthermore d-gpt model oracle answer able generate conversational responseson coqa time showcasingthe model scalability.acknowledgmentswe would like thank reviewer providingvaluble feedback early draft paper.this material base part research sponsor iis-1845670 odni iarpavia better program darpa w911nf-17-c-0095 addition amazon research award viewsand conclusion contain herein theauthors interpret necessarilyrepresenting official policy either expressedor imply odni iarpa darpaor u.s. government
crucial challenge questionanswering scarcity label data since costly obtain question-answer pair target text domain human annotation alternative approach totackle problem automatically generate pair either problem context large amount unstructured text wikipedia work propose hierarchical conditional variational autoencoder hcvae generate pair give unstructured text context maximizingthe mutual information generate qapairs ensure consistency validateour information maximizing hierarchicalconditional variational autoencoder infohcvae several benchmark datasets byevaluating performance model bert-base generate qapairs qa-based evaluation boththe generate human-labeled pair semisupervised learning training stateof-the-art baseline model result showthat model obtain impressive performance gain baseline task fraction data training introductionextractive question answering themost fundamental important task naturallanguage understanding thanks increasedcomplexity deep neural network ofknowledge transfer language model pretrained large-scale corpus peters devlin dong stateof-the-art model achieve human-levelperformance several benchmark datasets rajpurkar however also equal contribution1the generate pair code find athttps //github.com/seanie12/info-hcvaeparagraph input philadelphia mural thanany u.s. city thanks part creationof department recreation mural program program fund muralsq1 city mural city philadelphiaq2 philadelphia mural creation department recreation smural programq3 department recreation muralarts program start many mural fund graffiti programby department recreation example pair generate framework paragraph extract wikipedia providedby cardie example please seeappendix d.crucial success recent data-driven model availability large-scale datasets todeploy state-of-the-art model real-worldapplications need construct high-qualitydatasets large volume pair trainthem however costly require massive amount human effort time.question generation question-answerpair generation popular approach toovercome data scarcity challenge ofthe recent work resort semi-supervised learning leverage large amount unlabeled text e.g.wikipedia generate synthetic pair withthe help system tang yanget tang sachan xing,2018 however exist system overlook important point generate pairsfrom context consisting unstructured text isessentially one-to-many problem sequence-tosequence model know generate genericsequences zhao without much variety train maximum likelihoodestimation highly suboptimal qag209since context give model often contain rich information could exploit togenerate multiple pairs.to tackle issue propose novelprobabilistic deep generative model pairgeneration specifically model hierarchicalconditional variational autoencoder hcvae withtwo separate latent space question answerconditioned context answer latentspace additionally condition questionlatent space generation hierarchicalconditional first generate answer give acontext generate question give boththe answer context sample bothlatent space probabilistic approach allowsthe model generate diverse pair focusingon different part context time.another crucial challenge task toensure consistency question itscorresponding answer since semantically dependent question answerable give answer thecontext paper tackle consistencyissue maximize mutual information belghazi hjelm andchen generated pair weempirically validate propose mutual information maximization significantly improve theqa-pair consistency combining hierarchical cvae infomax regularizer together propose novel probabilistic generative model refer informationmaximizing hierarchical conditional variationalautoencoder info-hcvae info-hcvaegenerates diverse consistent pair evenfrom short context table quantitatively measure thequality generated pair popular evaluation metric bleu papineni rouge hovy meteor banerjee lavie text generation tellhow similar generated pair groundtruth pair directly correlate actual quality nema khapra,2018 zhang bansal therefore usethe qa-based evaluation metric proposedby zhang bansal measure howwell generated pair match distributionof pair semi-supervised learningsetting already label neednovel pair different pairsfor additional pair truly effective.thus propose novel metric reverse r-qae generated pairsare novel diverse.we experimentally validate modelon squad v1.1 rajpurkar naturalquestions kwiatkowski triviaqa joshi datasets qaeand r-qae bert-base devlin model model obtain highqae r-qae largely outperform stateof-the-art baseline significantly smallernumber context experimental resultsfor semi-supervised three datasets usingthe squad labeled dataset show ourmodel achieves significant improvement thestate-of-the-art baseline +2.12 squad +5.67on +1.18 trivia contribution threefold propose novel hierarchical variationalframework generate diverse pair froma single context knowledge thefirst probabilistic generative model questionanswer pair generation propose infomax regularizer effectively enforce consistency thegenerated pair maximize mutualinformation novel approach resolve consistency pair qag.• evaluate framework several benchmark datasets either train modelentirely generate pair qa-basedevaluation ground-truth generate pair semi-supervised modelachieves impressive performance task largely outperform exist baselines.2 related workquestion question-answer pair generation early work question generation mostly resort rule-based approach heilmanand smith lindberg labutovet however recently encoder-decoderbased neural architecture zhouet gain popularity outperform rule-based method useparagraph-level information cardie song zhao additional information reinforcement learning popular210approach train neural model thereward define evaluation metric songet kumar accuracy/likelihood yuan hosking andriedel zhang bansal state-ofthe-art model alberti dong al.,2019 chan pre-trained language model question-answer pair generation context main target relatively less explored topic tackle afew recent work cardie albertiet dong best ourknowledge first propose probabilistic generative model end-to-end tackleqag moreover effectively resolve qapair consistency issue maximize mutualinformation infomax regularizer belghaziet hjelm chen,2019 another contribution work.semi-supervised help ofqg model possible train modelsin semi-supervised learning manner obtain improved performance tang apply duallearning jointly train unlabeleddataset yang tang train framework goodfellow sachan xing proposea curriculum learn supervise modelto gradually generate difficult question qamodel dhingra introduce cloze-styleqag method pretrain model zhang andbansal propose filter low-quality synthetic question answer likelihood whilewe focus answerable setting paper recent work tackle unanswerable settings.zhu neural network edit answerable question unanswerable andperform semi-supervised alberti dong convert generate questionsinto unanswerable heuristic filteror replace correspond answer base orf1.variational autoencoders variational autoencoders vaes kingma welling areprobabilistic generative model varietyof natural language understanding task includinglanguage modeling bowman dialogue generation serban zhao al.,2017b park al.,2019 machine translation zhang deng work propose novel hierarchical conditional vaeframework infomax regularization generate pair sample high consistency.3 methodour goal generate diverse consistent qapairs tackle data scarcity challenge extractive task formally give context whichcontains token want togenerate pair question contain token correspond answer contain token tackle task bylearning conditional joint distribution thequestion answer give context sample pair estimate probabilistic deepgenerative model describe next.3.1 hierarchical conditional vaewe propose approximate unknown conditional joint distribution variational autoencoder framework kingma andwelling however instead directly learn common latent space question andanswer model hierarchical conditional framework separate latent spacefor question answer follow =∫zx∑zypθ x|zx zy|zx zx|c dzxwhere latent variable questionand answer respectively zx|c andpψ zy|zx conditional prior followingan isotropic gaussian distribution categoricaldistribution figure decompose latent space question answer since answeris always finite span context bemodeled well categorical distribution whilea continuous latent space appropriatechoice question since could unlimitedvalid question single context moreover design bi-directional dependency flow ofjoint distribution leverage hierarchical structure enforce answer latent variables211figure conceptual illustration propose hcvae model encode decode question correspondinganswer jointly dashed line refers generative process hcvae.figure directed graphical model hcvae thegray white node denote observe latent variables.to dependent question latent variable inpψ zy|zx achieve reverse dependencyby sample question x|zx thenuse variational posterior maximize theevidence lower bound elbo follow thecomplete derivation provide appendix ezx∼qφ zx|x x|zx ezy∼qφ zy|zx y|zy zy|zx ||pψ zy|zx zx|x ||pψ zx|c lhcvaewhere parameter generation posterior prior network respectively werefer model hierarchical conditionalvariational autoencoder hcvae framework.figure show directed graphical model ofour hcvae generative process follows:1 sample question sample answer generate answer generate question embedding pre-trained word embed network bert devlin forposterior prior network whereas wholebert contextualized word embeddingmodel generative network answerencoding binary token type bert.specifically encode context tokens except token part answer span highlighted word context figure encode thesequence word token token type andposition embed layer encode theanswer-aware context embeddinglayers hcvae training.prior networks different conditionalprior network zx|c zy|zx modelcontext-dependent prior dash line figure obtain parameter isotropicgaussian zx|c bidirectional lstm bi-lstm encode wordembeddings context hidden representation multi-layerperceptron model zy|zx follow categorical distribution computingthe parameter hidden representation context another mlp.posterior networks conditional posterior network zx|x zy|zx approximate true posterior distribution latent variable question answer usetwo bi-lstm encoders output hidden representation question context give theirword embeddings hiddenrepresentations obtain parametersof gaussian distribution upper rightcorner figure reparameterization trick kingma welling train themodel backpropagation since stochasticsampling process zx|x nondifferentiable another bi-lstm encode theword embedding answer-aware context thehidden representation hiddenrepresentation compute theparameters categorical distribution lowerright corner figure categorical reparameterization trick gumbel-softmax212 maddison jang enablebackpropagation sampled discrete latentvariables.answer generation networks since considerextractive factorize y|zy intopθ ys|zy ye|zy arethe start position answer span highlight word figure respectively.to obtain estimator first encodethe context contextualized word embedding pre-trainedbert compute final hidden representationof context latent variable heuristicmatching layer bi-lstm |eci =−−−−→lstm =←−−−−lstm mi=1where linearly transform rdy×mis final hidden representation hinto separate linear layer predict ye.question generation networks design theencoder-decoder architecture networkby mainly adopt baseline zhao al.,2018 zhang bansal encode weuse pre-trained bert encode answer-specificcontext contextualized word embedding two-layer bi-lstm encode intothe hidden representation figure apply gated self-attention mechanism wang al.,2017 hidden representation good capture long-term dependency within context toobtain hidden representation rdx×m decoder two-layered lstm receive latent variable initial state ituses attention mechanism luong dynamically aggregate decode stepinto context vector j-th decoderhidden representation figure maxoutactivation goodfellow compute thefinal hidden representation follow lstm exj−1 dj−1 ĥtwadj softmax ĥajd̂j linearly transform thej-th question word embedding probability vector vocabulary compute softmax wed̂j initialize weight matrix pretrainedword embed matrix training.further copy mechanism zhao al.,2018 model directly copy tokensfrom context also greedily decode question ensure stochasticity come thesampling latent variables.3.2 consistent pair generation withmutual information maximizationone important challenge qagtask enforce consistency generated question correspond answer theyshould semantically consistent ispossible predict answer give questionand context however neural qagmodels often generate question irrelevant thecontext answer zhang bansal lack mechanism enforce thisconsistency tackle issue maximizingthe mutual information generated qapair assume answerable pair willhave high since exact computation isintractable neural approximation whilethere exist many different approximation belghazi hjelm theestimation propose chen basedon jensen-shannon divergence +12ex̃ +12ex ỹ∼n linfowhere denote expectation positiveand negative example generate negative example shuffle pair minibatch question randomly associate withan answer intuitively function like abinary classifier discriminate whether pairis joint distribution empiricallyfind following effectively achieve ourgoal consistent sigmoid xtwy =1l∑j summarize representation question answer respectively combined elbo final213objective info-hcvae follow maxθlhcvae λlinfowhere include parameter andw control effect maximization.in experiment always experiment4.1 datasetstanford question answering dataset v1.1 squad rajpurkar reading comprehension dataset consist questionsobtained crowdsourcing wikipediaarticles answer every question segment text span correspond reading passage split zhangand bansal fair comparison.natural questions kwiatkowski al.,2019 dataset contain realistic questionsfrom actual user query search engine usingwikipedia article context adapt datasetprovided mrqa share task fisch al.,2019 convert extractive format.we split original validation half asvalidation test experiments.triviaqa joshi readingcomprehension dataset contain question-answerevidence triple pair evidence context document author uploadedby trivia enthusiast choose qapairs answer span contexts.harvestingqa dataset contain top-ranking10k wikipedia article synthetic pairsgenerated answer span extractionand system propose cardie dataset semi-supervised learning.4.2 experimental setupsimplementation details experiment weuse bert-base devlin asthe model hyperparametersas describe original paper hcvaeand info-hcvae hidden dimensionalityof bi-lstm posterior prior answer generation network dimensionality encoder decoderof question generation network dimensionality define of2https //github.com/xinyadu/harvestingqa10-way categorical variable train model fine-tune modelfor epoch train model andinfo-hcvae adam optimizer kingma andba batch size initiallearning rate respectively forsemi-supervised learning first pre-train berton synthetic data epoch fine-tune iton dataset epoch prevent posterior collapse multiply divergenceterms question answer higgins detail datasets experimentalsetup please appendix c.baselines experiment variant ourmodel several baselines:1 harvest-qg attention-based neural qgmodel neural answer extraction system cardie maxout-qg neural model base onmaxout copy mechanism gated selfattetion zhao bertas word embedding suggest zhangand bansal semantic-qg neural model base onmaxout-qg semantic-enhanced reinforcement learning zhang bansal hcvae hcvae model without infomax regularizer.5 info-hcvae full model infomaxregularizer.for baseline answer spansextracted answer extraction system andcardie quantitative analysisqae r-qae crucial challenge withgenerative model lack good quantitativeevaluation metric adopt qa-based evaluation metric propose zhang bansal measure quality pair isobtained first train model synthetic data evaluate model withhuman annotate test data however onlymeasures well distribution syntheticqa pair match distribution pair consider diversity pairs.thus propose reverse qa-based evaluation r-qae accuracy modeltrained human-annotated pair evaluate generate pair synthetic214method r-qae squad em/f1 harvesting-qg questions em/f1 harvesting-qg em/f1 harvesting-qg r-qae result three datasets allresults performance test set.harvest maxout semantic hcvae info hcvae111.74 result mutual information estimation theresults base pair generate h×10 cover large distribution human annotate training data r-qae however note r-qae meaningfulwhen high enough since trivially invalidquestions also yield r-qae.results compare hcvae info-hcvaewith baseline model squad triviaqa wikipedia paragraph fromharvestingqa cardie evaluation table show hcvae infohcvae significantly outperform baseline bylarge margin three datasets whileobtaining significantly r-qae showsthat model generate high-quality diverse pair give context moreover info-hcvae largely outperform hcvae whichdemonstrates effectiveness infomax regularizer enforce qa-pair consistency.figure show accuracy function ofnumber pair info-hcvae outperform baseline large margin order ofmagnitude small number pair example info-hcvae achieve point pair outperform semantic-qg large number pair also report104 pair log-scaled qa-basedevaluation harvest-qg maxout-qgsemantic-qg info-hcvaefigure pair log-scaled squad.method r-qae baseline r-qae result ablation study onsquad dataset result performance ourtest set.the score xtwy approximate estimationof mutual information pair generate method table info-hcvaeyields large value estimation.ablation study perform ablationstudy effect model component.we start model without latent variable essentially deterministic seq2seqmodel denote baseline table weadd question latent variable +q-latent andthen answer latent variable +a-latent seethe effect probabilistic latent variable modelingand hierarchical modeling respectively resultsin table show essential improve quality diversity r-qae generated pair finally infomax regularization +infomax improvesthe performance enhance consistency ofthe generate pairs.4.4 qualitative analysishuman evaluation qualitative analysis wefirst conduct pairwise human evaluation qapairs generate info-hcvae maxoutqg randomly select paragraph specifically human judge perform blind qualityassessment pair present random order containedtwo five pair pair evalu215method diversity consistency overallbaseline table result human judgement term diversity consistency overall quality generate pairs.paragraph scotland pass byand give royal assent queen elizabeth govern function role scottishparliament delimit legislative competence forth function scottishparliament pass govern role scottish parliament pass queen elizabeth give scottish parliament theresponsibility determine legislative policy table examples one-to-many mapping infohcvae answer highlight pink denote theground-truth question denotes question generate byinfo-hcvae.ated term overall quality diversity andconsistency generated pair andthe context result table show theqa pair generate info-hcvae evaluate diverse consistent comparedto generate baseline models.one-to-many show info-hcvaecan effectively tackle one-to-many mapping problem question generation qualitatively analyze generated question give contextand answer squad validation set.specifically sample question latent variables multiple time question prior network questiongeneration network answer example table show infohcvae generate diverse semantically consistent question give answer provide morequalitative example appendix d.latent space interpolation examine infohcvae learn meaningful latent space pair qualitatively analyze pair generatedby interpolate latent code onsquad train first encode twoqa pair posterior network zx|x sample interpolated value ofzx prior network zy|zx generatecorresponding pair table show semantic pair generate smoothly transitfrom latent another high diversity andconsistency provide qualitative examplesparagraph atop main building gold dome golden statue virgin mary next mainbuilding basilica sacred heart immediatelybehind basilica grotto marian place ofprayer reflection main drive simple modern stone statue mary.ori1 grotto notre dame marian place prayer reflectiongenq grotto marian place prayer reflectionq place behind basilica prayer grottoq next main building atnotre dame basilica sacred heartq main drive stone statue maryori2q main building atnotre dame golden statue virgin marytable pair generate interpolate twolatent code encode posterior network ori1 andori2 train squad.in appendix d.4.5 semi-supervised qawe validate model semi-supervisedsetting model ground truthlabels generated label solve task whether generated pair help improvethe performance model conventionalsetting since synthetic datasets consist ofgenerated pair inevitably contain somenoise zhang bansal dong alberti refine pairsby heuristic suggest dong replace generated answer whose f1score prediction model trainedon human annotate data setthreshold select threshold theqa pair refinement model cross-validation onthe squad dataset experiments.please appendix details.squad first perform semi-supervised experiment squad synthetic pairsgenerated model context useboth paragraph original squad dataset paragraph harvestingqa dataset using info-hcvae generate different pair sample latent space denote s×10 baseline weuse semantic-qg zhang bansal withthe beam search size obtain number pair also generate pairs216data f1squad baseline +s×10 +0.95 +0.13 +h×100 +0.78 +0.56 +s×10 h×100 +1.19 +0.49 info-hcvae +s×10 +1.84 +0.88 +h×10 +1.12 +0.62 +h×20 +1.43 +0.93 +h×30 +1.51 +0.89 +h×50 +1.92 +1.15 +h×100 +2.12 +1.40 +s×10 h×100 +1.94 +1.59 table result semi-supervised experiment onsquad result performance test set.using different portion paragraph provide inharvestingqa denote h×10 bysampling latent variable context table framework improve accuracy ofthe bert-base model point significantly outperform semantic-qg.nq triviaqa model useful whenwe label data target dataset.to show well model performs insuch setting train model onlythe pair generate model train onsquad test target datasets andtriviaqa generate multiple pair fromeach context target dataset sample fromthe latent space time denote n×110 t×1-10 table fine-tune theqa model pretrained squad dataset withthe generate pair datasets table show augment train data withlarger number synthetic pair performance model significantly increase significantly outperform model trainedon squad model train qagstill largely underperform model train human label distributional discrepancybetween source target dataset.5 conclusionwe propose novel probabilistic generative framework generate diverse consistent questionanswer pair give text specifically model learn joint distribution questionand answer give context hierarchically conditional variational autoencoder enforcingconsistency generate pair maximize mutual information novel indata f1natural questionssquad +3.94 +3.79 +n×2 +4.19 +4.05 +n×3 +4.96 +4.69 +n×5 +5.42 +4.92 +n×10 +5.67 +5.40 +0.69 +1.21 +t×2 +1.05 +1.10 +t×3 +0.75 +1.51 +t×5 +1.18 +1.23 +t×10 +0.69 +1.22 trivia result semi-supervised experiment onnatural questions triviaqa dataset result theperformance test set.fomax regularizer knowledge thefirst successful probabilistic model evaluate performance model theaccuracy bert-base model train generated question multiple datasets largely outperform state-of-theart baseline +6.59-10.69 even witha small number pair validate model semi-supervised itimproved performance bert-base qamodel squad significantlyoutperforming state-of-the-art model future work plan extend model ameta-learning framework generalization overdiverse datasets.acknowledgementsthis work support engineering research center program national research foundation korea fund korean government msit nrf2018r1a5a1059921 institute information communication technology planning evaluation iitp grant fund korea government msit no.2019-0-01410 research development question generation deep learning base semantic search domain extension no.2016-0-00563 research adaptive machinelearning technology development intelligent autonomous digital companion no.2019- artificial intelligence graduate schoolprogram kaist
knowledge-driven conversation approacheshave achieve remarkable research attentionrecently however generate informative response multiple relevant knowledge without lose fluency coherence isstill main challenge addressthis issue paper propose method thatuses recurrent knowledge interaction amongresponse decode step incorporate appropriate knowledge furthermore introduce knowledge copy mechanism usinga knowledge-aware pointer network copywords external knowledge accord toknowledge attention distribution jointneural conversation model integratesrecurrent knowledge-interaction knowledge copy perform well generate informative response experimentsdemonstrate model parameter yield significant improvement overcompetitive baseline datasets wizardof-wikipedia average bleu duconv average bleu different knowledge format textual structure different language english chinese introductiondialogue system attract much researchattention recent year various end-to-end neural generative model base sequence-tosequence framework sutskever havebeen apply open-domain conversation andachieved impressive success generate fluentdialog response shang vinyals andle serban however many neural generative approach last yearsconfined within utterance response sufferingfrom generate uninformative inappropriateresponses make response meaningfuland expressive several work dialogue system exploit external knowledge knowledgedriven method focus generate informative meaningful response incorporatingstructured knowledge consist triplet al.,2017 zhou young liuet unstructured knowledge like document long parthasarathi pineau,2018 ghazvininejad dialogue generation mainly hastwo method pipeline deal knowledge selection generation successively lianet joint integrate knowledge selection generation process example several work memory network architecture sukhbaatar integrate theknowledge selection generation jointly dinanet dodge parthasarathi andpineau madotto ghazvininejad pipeline approach separate knowledge selection generation result insufficient fusion knowledgeand generator integrate various knowledge pipeline approach lack flexibility thejoint method memory module usuallyuses knowledge information statically confidence knowledge attention decrease decode step potential produceinappropriate collocation knowledge word togenerate informative dialogue response integrate various relevant knowledge without losingfluency coherence paper present effective knowledge-based neural conversation modelthat enhance incorporation knowledge selection generation produce informative meaningful response modelintegrates knowledge generator recurrent knowledge interaction dynamically update attention knowledge selectionvia decoder state update knowledge attention assist decode next state which42maintains confidence knowledge attentionduring decoding process help decoderto fetch late knowledge information thecurrent decode state generated word ameliorate knowledge selection refine nextword generation repeat interaction knowledge generator verify aneffective integrate multiple knowledge coherently generate informative meaningful response knowledge fully take account of.although recurrent knowledge interaction bettersolves problem select appropriate knowledge generate informative response thepreferable integration knowledge conversation generation still confront issue i.e. ismore likely description word external knowledge generate dialog responsehave high probability out-ofvocabulary common challenge natural language processing neural generative modelwith pointer network show theability handle problem vinyals research copyablegenerative model attention handle externalknowledge knowledge-driven conversation description word knowledge areusually important component dialog response.thus leverage knowledge-aware pointer network upon recurrent knowledge interactive decoder integrate seq2seq model pointernetworks contain pointer refer utterance attention distribution knowledge attentiondistribution show generate responsesusing knowledge copy resolve theknowledge incompleteness problems.in summary main contribution wepropose recurrent knowledge interaction whichchooses knowledge dynamically among decodingsteps integrate multiple knowledge response coherently knowledge-awarepointer network knowledge copy whichsolves problem keep knowledge integrity especially long-text knowledge integration recurrent knowledge interaction andknowledge copy result informative coherent fluent response comprehensive experiment show model generalfor different knowledge format textual structure different language english chinese result significantly outperformcompetitive baseline model parameters.2 model descriptiongiven dataset ni=1 wheren size dataset dialog responsey produce conversation history utterance also relative knowledge number oftokens conversation history responsey respectively denote size relevantknowledge candidate collection relevantknowledge candidate collection assume tobe already provide size candidate setis limit relevant knowledge element incandidate collection could passage triplet denote thenumber token knowledge element.as illustrate figure model proposedin work base architecture involvingan encoder-decoder framework sutskever al.,2014 pointer network vinyals model comprise fourmajor component lstm base utteranceencoder general knowledge encoder suitablefor structural documental knowledge recurrent knowledge interactive decoder aknowledge-aware pointer network.2.1 utterance encoderthe utterance encoder bi-directional lstm schuster paliwal encode utterance input concatenate token dialogue history obtain bi-directional hidden state utterance denote combining two-directional hidden state hidden state ash∗t −−−−→lstm ht−1 ←−−−−lstm ht+1 knowledge encoderas illustrate model description knowledgeinput collection multiple knowledge candidate relevant knowledge apassage triplet paper provide universalencoding method textual structuredknowledge relevant knowledge representedas sequence token encode atransformer encoder vaswani i.e. transformer static attention is43figure architecture calculate decode-input utterance context vector atcurrent step represent knowledge context vector result dynamic knowledge attention ugen andkgen soft switch control copy pointer utterance attention distribution knowledge attentiondistribution respectively.used encode knowledge obtain overall representation krep therelevant knowledge asaki softmax tanh wzzi krep =l∑i=1aki learnable parameter sofar knowledge representation theknowledge candidate collection crepk recurrent knowledge interactivedecoderthe decoder mainly comprise single layerlstm hochreiter schmidhuber generate dialogue response incorporate knowledge representation collection crepk shownin figure step decoder update itsstate st+1 utilize last decode state current decode-input knowledge context ctk.the current decode-input compute embeddings previous word utterancecontext vector provide procedure aseti tanh whhi +wus softmax =m∑i=1utihi learnable parameters.instead model knowledge selection independently statically incorporate representation knowledge generator thispaper propose interactive method exploitknowledge response generation recurrently theknowledge attention update decodingproceeds consistently retrieve informationof knowledge relate current decodingstep help decode next state correctly write asθti tanh wkkrepi +wks softmax =s∑idtikrepi learnable parameters.a knowledge gate employ determine howmuch knowledge decode-input thegeneration define asgt sigmoid learnable parameter thesteps proceed recurrently knowledge gate candynamically update well hence decoder update state st+1 lstm gtutd knowledge-aware pointer networkspointer network copy mechanism arewidely generative model deal oovproblem paper employ novel knowledgeaware pointer network specifically expand thescope original pointer network exploitingthe attention distribution knowledge representation besides propose knowledge-awarepointer network share extend vocabulary utterance knowledge beneficialto decode word pointer respectivelyrefer attention distribution utterance andknowledge word generation determine bythe soft switch utterance ugen soft switchof knowledge kgen define asugen wtucctu wtusst wtuutd kgen wtkcctk wtksst wtuc wtus wtkc wtks arelearnable parameter define learnable parameter therefore final probability vocabulary ispfinal λugen µkgen ugen ∑iuti kgen ∑idti softmax learnable parameter constrain note theword word appear utterance zero copy word fromknowledge instead dialogue history.3 experiments3.1 datasetswe recently release datasets wizard-ofwikipedia duconv whose knowledge formatsare sentence triplet respectively.wizard-of-wikipedia dinan anopen-domain chit-chat dataset agent wizard apprentice wizard knowledge expert access information retrievalsystem recall paragraph wikipedia relevant dialogue unobserved theagent apprentice play role curiouslearner dataset contain dialogueswith turn fortrain/valid/test test split twosubsets test seen test unseen seen overlap topic training test unseen contain topic never seenbefore train validation theground-truth knowledge information provide inthis dataset ability knowledge selection generation crucial part ourmodel.duconv proactive conversation dataset dialog utterance model mainly play role leadingplayer assign explicit goal knowledgepath comprise topic provide withknowledge relate topic knowledge dataset format triplet subject property object totally contain properties.3.2 comparison approacheswe implement model datasets wizardof-wikipedia duconv compare approach variety recently competitive baseline datasets respectively wizard-ofwikipedia compare approach follow seq2seq attention-based seq2seq without access external knowledge whichis widely open-domain dialogue vinyals memnet hard/soft knowledge groundedgeneration model knowledge candidate select semantic similarity hard knowledge candidate storedinto memory unit generation soft ghazvininejad postks concat/fusion hard knowledgegrounded model decoder whereknowledge concatenate concat softmodel hgfu incorporate knowledgeswith decoder lian joint neural conversation modelnamed knowledge-aware pointer networksand recurrent knowledge interaction hybridgenerator.while dataset duconv chinese dialoguedataset structured knowledge compareto baseline refer consists retrieval-based model well asgeneration-based models.3.3 metricwe adopt automatic evaluation severalcommon metric propose lian available automatic evaluation tool calculate experimentalresults keep standard metrics include bleu1/2/3 distinct1/2 automaticallymeasure fluency coherence relevance diversity metric evaluate performance atthe character level mainly chinesedataset duconv method incorporate generation knowledge soft fusion doesnot select knowledge explicitly therefore justmeasure result whole dialog notevaluate performance knowledge selection independently besides provide annotator toevaluate result human level annotator evaluate quality dialog response generate fluency informativeness coherence.the score range reflect fluency informativeness coherence result badto good example coherence score meansthe response good coherence without illogical expression continue dialogue historyreasonably score mean result acceptablebut slight flaw score mean statementof result illogically result improper thedialog context.3.4 implement detailwe implement model tensorflow framework abadi implementation point network inspire publiccode provide utterance sequence concats token dialog historyand separate knowledge utterance encoder single-layer bidirectional lstm structure hidden state responsedecoder single-layer unidirectional lstmstructure dimensional hidden states.and knowledge encoder transformer structure vocabulary wordswith dimensional random initialize embeddings instead pre-trained word embeddings train model adagrad duchiet optimizer mini-batch size learn rate iteration iteration wizard-of-wikipedia gpu-p100machine overall parameter million model size decrease overall best baselinepostks parameters:71 million model size results analysis3.5.1 automatic evaluationas experimental result wizard-ofwikipedia automatic evaluation summarizedin table approach outperform competitive baseline refer recently work lianet achieve significant improvement automatic metric onseen unseen test bleu-1 enhancesslightly test seen improve obviously intest unseen bleu-2 bleu-3 yield considerable increment test seen intest unseen well example bleu-3 improve absolute improvement test seen absolute improvement test unseen superior performance metric bleu mean dialog responsegenerated model closer groundtruth response preferable fluency allfigure bleu improvement wizard-of-wikipedia.bleu metric show figure findthat improvement result increase theaugment bleu gram mean dialog response produce model inline real distribution ground-truth response phrase level good improvement high gram bleu reflect model havepreferable readability fluency generally theground-truth response datasets make withthe expression knowledge conducesto informativeness response recurrent knowledge interaction module model kicprovides mechanism interact knowledge decode word dialog response stepby step moreover knowledge-aware pointer46modelstest seen test unseenbleu-1/2/3 distinct-1/2 bleu-1/2/3 distinct-1/2seq2seq hard soft concat fusion automatic evaluation wizard-of-wikipedia result baseline take lian bleu-1 bleu-2 distinct-1 distinct-2 pplnorm retrieval norm seq2seq generation automatic evaluation duconv denotes knowledge norm stand normalization onentities entity type norm generation postks table1 result baseline take wuet allow copy word theexpression knowledge decode therefore dialog response generate containsrelatively complete phrase knowledge asknowledge-informativeness ground-truth response addition improvement metricsbleu increase test seen test unseen isto advantage case unseenknowledge guide dialogue show ourmodel superior address dialogue topic never train validation besides metric distinct also achieve impressiveresults prior baseline aboutaverage competitive methodpostks metric distinct mainly reflect thediversity generated word whose improvementsindicating dialogue response produce bykic could present information addition toexperiments wizard-of-wikipedia also conduct experiment duconv verify theeffectiveness model structure knowledgeincorporated conversation dataset duconvreleased recently compare modelto baseline mention first apply duconv includingboth retrieval-based generation-based method result present table show thatour model obtain high result ofthe metric obvious improvement retrieval generation method concretely thef1 average bleu average distinct areover best result baseline norm generationabout similar towizard-of-wikipedia impressive augments ofmetrics demonstrate model capacityof produce appropriate response fluency coherence diversity.metrics wizard-of-wikipedia duconvfluency human evaluation result kic.3.5.2 human evaluationin human evaluation accord dialoguehistory related knowledge annotatorsevaluate quality dialog response term offluency coherence score range score high response morefluent informative coherent dialog context integrate knowledge manual evaluation result summarize table modelachieves high score wizard-of-wikipediaand duconv mean response generatedby also good fluency informativeness,47models bleu-1 bleu-2 distinct1 distinct2 parameterspart1 seq2seq part1 part2 copy part3 attn automatic evaluation progressive component model duconv dyn.attn.denote knowledge dynamic attention klg.copy stand knowledge-aware pointer network metrics remainconsistent table seen test unseenbleu-1/2/3 distinct-1/2 bleu-1/2/3 distinct-1/2part1 automatic evaluation progressive component model wizard-of-wikipedia part1 part2 part3 table metrics remain consistent table coherence human view close superiorperformance automatic evaluation.3.6 ablation studywe conduct ablation experiment dissectour model based seq2seq framework weaggrandize component modelkic progressively result summarizedin table table first incorporate knowledge seq2seq architecture attentionof knowledge gate control utilization knowledge generation theresults achieve considerable improvement thehelp knowledge apply knowledgeaware pointer network model illustratedin last step introduce copy mechanism whichincreases effect significantly demonstrate facilitation knowledge-aware copy mechanism toproduce dialogue response important wordsadopted utterance knowledge theend replace knowledge attention bydynamic attention update decode state recurrently whole model propose inthis paper experimental result show thatsuch amelioration also achieve impressive enhancement dynamic update knowledge attention decode effectively integrate multiple knowledge response improve theinformativeness performance modelare gradually improve addition component mean component themodel play crucial role additionally withthe considerable improvement progressivestep model size parameter increase slightly mean model agood cost performance.3.7 case studyas show figure present responsesgenerated propose model themodel postks fusion achieve overall bestperformance among competitive baseline givenutterance knowledge candidate modelis well postks fusion produce contextcoherence response incorporate appropriate multiple knowledge complete description themodel prefers integrate knowledgeinto dialogue response riching informativewithout lose fluency furthermore model hasan additional capability handle problem generate response infrequent butimportant word word thetime knowledge context like alfredhitchcock presents figure also compare result model static knowledge attention whose result mismatch betweenthe award representative work alfredhitchcock presents static knowledge attention calculate decoding informationand confidence lose decode step bystep lead mispairing expression multiple knowledge recurrent knowledgeinteraction help decoder fetch closestknowledge information current decoding48figure case study duconv mean out-of-vocabulary static denote model usingstatic knowledge attention instead recurrent knowledge interaction knowledge response boldletters inappropriate word highlight color.state superior learn coherent collocation multiple knowledge case ofwizard-of-wikipedia duconv present inthe appendix section.4 related workconversation knowledge incorporation receive considerable interest recently demonstrate effective enhance performance main method knowledgebased conversation retrieval-based approches wuet tian generation-basedapproaches generation-based method whichachieves research attention focus generate informative meaningful responsesvia incorporate generation structured knowledge young al.,2018 zhou documental knowledge ghazvininejad long work integrate knowledge generationin pipeline deal knowledgeselection generation separately pipeline approach attention knowledge selection posterior knowledge distributionto facilitate knowledge selection lian context-aware knowledgepre-selection guide select knowledge zhanget various work entirety integration knowledge generation end-toend usually manage knowledge external memory module parthasarathi pineau,2018 introduce bag-of-words memory networkand dodge perform dialogue discussion long-term memory dinan memory network retrieve knowledge andcombined transformer architecture generate response pipeline approach lack offlexibility constrict separated knowledge selection generation could exploitknowledge sufficiently end-to-end approacheswith memory module attention knowledge statically integrate multiple knowledge intoa response easy confuse whereaswe provide recurrent knowledge interactive generator sufficiently fuse knowledge intogeneration produce informative dialogueresponses.our work also inspire several work oftext generation copy mechanism vinyalset attention pointer generate word input resource index-basedcopy incorporate copy intoseq2seq learn handle unknown word seeet introduce hybrid pointer-generatorthat copy word source text whileretaining ability produce novel word intask-oriented dialogue pointer network werealso improve copy accuracy mitigate49the common out-of-vocabulary problem madottoet different fromthese work extend pointer network referringto attention distribution knowledge candidatesthat copy word knowledge resource andgenerate dialogue response guidance ofmore complete description knowledge.5 conclusionwe propose knowledge ground conversationalmodel recurrent knowledge interactivegenerator effectively exploit multiple relevant knowledge produce appropriate responses.meanwhile knowledge-aware pointer networkswe design allow copy important word usually word knowledge experimentalresults demonstrate model powerful togenerate much informative coherent response competitive baseline model infuture work plan analyze turn dialogue reinforcement learning architecture andto enhance diversity whole dialogue byavoiding knowledge reuse
propose simple data augmentation protocol provide compositional inductive bias conditional unconditionalsequence model protocol synthetic training example construct take real training example replace possibly discontinuous fragment fragment appear least similar environment protocol model-agnostic anduseful variety task applied neuralsequence-to-sequence model reduce errorrate much diagnostic tasksfrom scan dataset semantic parsing task applied n-gram languagemodels reduce perplexity roughly small corpus several languages.1 introductionthis paper propose rule-based data augmentation protocol sequence modeling approachaims supply simple model-agnostic biastoward compositional reuse previously observedsequence fragment novel environment consider language model task wishto estimate probability distribution familyof sentence follow finite sample astraining data sang.b sang.c daxed.in language processing problem often wantmodels generalize beyond dataset inferthat also probable daxed.b sang daxed.this generalization amount inference aboutsyntactic category clark andwug interchangeable arealso likely interchangeable elsewhere sangare similarly interchangeable human learnersmake judgment like novel lexical item berko fragment novel language lake expect suchjudgments unstructured generative modelstrained maximize likelihood trainingdata large body work natural language processing provide generalization data like byadding structure learn predictor chelbaand jelinek chiang dyer real-world datasets however model aretypically black-box function approximators like neural network even black-boxmodels fail place probability mass either example give small train like melis extent believe capture important inductive bias would like find softly encourage itwithout tamper structure predictorsthat work well scale paper introducea procedure generate synthetic training example recombine real assign non-negligible probability italready appear training dataset.the basic operation underlie proposal call geca good-enough compositional augmentation depict figure iftwo possibly discontinuous fragment trainingexamples appear common environment additional environment firstfragment appear also valid environment forthe second.she pick fresno.she tempe.pat pick up.pat down.⇡ latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit latexit sha1_base64= aaab7nicbvbnswmxej2tx7v+vt16crbbu9kvqy9flx4r2a9ol5jns21ongljvixlf4qxd4p49fd489+ybfegrq8ghu/nmdmvupwz6/vfxmltfwnzq7xd2dnd2z+ohh61juw1os0iudtdcbvkmaatyyynxauptijoo9hknvc7j1qbjswdnsoajngkwmwitk7q9lfswj4nqjw/7s+bvklqkbouaa6qx/2hjglchsucg9mlfgxddgvlckezsj81vgeywspac1tghjowm587q2dogajyalfcorn6eyldithtjhkdcbzjs+zl4n9el7xxdzgxovjlbvksilooret572jincwwtx3brdn3kyjjrdgxlqgkcyfyfnmvtc/qgv8p7i9rjzsijjkcwcmcqwbx0ia7aeilcezggv7hzvpei/fufsxas14xcwx/4h3+ajnij7y= /latexit figure visualization propose approach discontinuous sentence fragment underline appear similar environment highlight identify additional sentence whichthe first fragment appear synthesize newexamples substitute second fragment.7557geca crude linguistic principle isboth limit imprecise discuss sections capture narrow slice manyphenomena study heading compositionality also make number incorrectpredictions real language data nevertheless geca appear quite effective across rangeof learning problem semantic parsing givesimprovements comparable task-specific dataaugmentation approach liang onlogical expression good performance thatapproach different split data designedto test generalization rigorously correspond improvement version datasetwith different meaning representation language.outside semantic parsing solve representative problem scan dataset lakeand baroni synthetic precise inthe notion compositionality test finally help unconditional low-resourcelanguage model problem typologically diverse languages.2 backgroundrecent year tremendous success natural language transduction generation tasksusing complex function approximators especiallyrecurrent sutskever attentional vaswani neural model enoughtraining data model often accurate approach build traditional toolslike regular transducer context-free grammar knight graehl brittle anddifficult efficiently infer large datasets.however model equip explicit symbolic generative process least significant advantage aforementioned black-boxapproaches give grammar straightforwardto precisely characterize grammar willextrapolate beyond example give training out-of-distribution data indeed isoften possible researcher design formthat extrapolation take smooth n-gramlanguage model ensure memorization ispossible beyond certain length ccg-based semantic parser make immediateuse entity lexicon without ever thelexicon entry real sentence zettlemoyerand collins case black-box neural model arefundamentally incapable kind predictablegeneralization—the success model capture long-range structure text radford al.,2019 control algorithmic data graves al.,2014 indicate representation hierarchical structure learn give enough data.but precise point transition occursis well characterize evidently beyondthe scale available many real-world problems.how improve behavior highquality black-box model setting thereare many sophisticated tool available improve function approximators loss functionsthemselves—structured regularization parameter posterior regularization ganchev explicitstacks grefenstette composition operator bowman russin al.,2019 exist proposal tend taskand architecture-specific extent thatthe generalization problem address increase scale training data naturalto whether address problem increase scale artificially—in word viadata augmentation.data augmentation technique generate auxiliary training data perform structure transformation combination trainingexamples widely computer vision krizhevsky zhang summers dinneen within several dataaugmentation approach propose fortext classification ratner andzhou approach give improvementseven combine large-scale pretraining liang studydata augmentation compositionality specificsetting learn language-to-logical-form mapping begin principle data iscompositional generate explicit grammar relate string logical form viewof compositionality determine synchronicitybetween form meaning essentially montagovian well-suited problem formal semantics montague however require accessto structured meaning representation explicittypes bracketings available inmost applications.here notion compositionality thatis simpler general bias toward identify recur fragment training time andre-using environment distinct those7558in first observe view makesno assumption availability bracketsand type synchronous extentthat notion fragment permit includecontent source target side wewill find nearly effective existingapproaches specific setting theywere design also effective variety ofproblems applied.3 approachconsider example figure ourdata augmentation protocol discover substitutable sentence fragment underlined withthe fact pair fragment appear somecommon sub-sentential environment highlight take evidence fragment belong acommon category generate examplesfor model occurrence fragment isremoved sentence produce sentencetemplate populate otherfragment.why expect procedure produce well-formed training example existence syntactic category expressibilityof well-formedness rule term abstractcategories foundational principle ofgenerative approach syntax chomsky observation context provide strong signal sentence fragment category turnthe foundation distributional technique thestudy language firth combining thetwo give outline procedure.this combination productive history innatural language processing fragment aresingle word yield class-based language model brown fragment contiguous span yield unsupervised parser clark,2000 klein manning present dataaugmentation scenario distinguish mainly bythe fact unconcerned produce acomplete generative model data recover latent structure imply presenceof nest syntactic category still synthesize high-precision example well-formedsequences identify individual substitutionsthat likely correct without understandinghow grammar whole.indeed concern recover linguistically plausible analysis need notlimit word contiguous sentencefragments take pick up.b down.as evidence pick wherever wecan indeed give translationdataset sing canto.b sing marvelously maravillosamente.c marvelously maravillosamente.we apply principle synthesize idax dajo base common environment marvelously maravillosamente theperspective generalized substitution principle alignment problem machine translation thesame class induction problem languagemodeling sequence feature large number gappy fragment boundary symbol remain question make twoenvironments similar enough infer existenceof common category largeliterature question include aforementioned work language modeling unsupervisedparsing alignment current work wewill make simple criterion fragmentsare interchangeable occur least onelexical environment exactly same.given window size sequence token w1w2 define fragment aset non-overlapping span template version fragment remove anenvironment template restrict k-wordwindow around remove fragment formally denote fragment disjoint ∀wai and∃wai figure underlined pick onepossible fragment could extract thesentence corresponding template thewug fresno environment7559is show figure fragment substitute templatewith number hole denote substitution operation data augmentationoperation define geca formally state asfollows training data contains sequence t1/f1 t′1/f1 t2/f2 =env synthesize newtraining example t′1/f2.if fragment occurs multiple time within givenexample instance replace figure note despite fact aboveoperation motivate insight generativesyntax distributional semantics beemphasized statement generallinguistic principle obviously wrong counterexamples abound english stress-derived noun récord recórd take evidencethat many noun verb interchangeable inmandarin chinese kěshì dànshì mean kěshì alone particular construction mean ultimately matter relative frequencyof error contribution inaccuratemodel less inaccuracy cause theoriginal shortage training data geca stillhelps conditional problem like machinetranslation example error totally harmless synthesize pairwith outside support real training data influence model prediction onthe true support beyond provide useful generalinductive bias.implementation naïve implementation theboxed operation take t3f3 time isthe number distinct template dataset andf number distinct fragment beimproved ft2e number oftemplates environment bybuilding appropriate data structure algorithm requirement might still considerable comparable n-gram languagemodels strategy language model literature reduce memory usage heafield algorithm agnostic withrespect choice fragmentation environment function task-specific choice describedin detail experiment below.4 diagnostic experimentswe begin experiment syntheticdata design precisely test whether geca provide kind generalization design for.here scan dataset lake baroni,2018 consist simple english commandspaired sequence discrete action figure focus specifically primitive jump template around right condition whichtest whether agent expose individualcommands modifier jump jump isolation training time incorporate intomore complex command like early exampleat test time.we extract fragment maximum length token environment takento complete template generated examplesare append original dataset examalgorithm sample geca implementation.f2t dict default=set fragment templatet2f dict default=set template fragmente2t dict default=set templatefor dataset fragment fragment fragment template template fragment fragment template dict default=set fragment template fragment template2 fragment new_template template2 template1 new_template template1 template2 fragment template1 fragment template2 yield template2 fragmentwalkwalkwalk leave twicelturn walk lturn walkjumpjumpjump around leftlturn jump lturn jump lturn jump lturn jumpwalk rightrturn walkfigure example scan data example consistsof synthetic natural language command leave pairedwith discrete action sequence right scan jump nacs right scan right nacsseq2seq geca sequence match accuracy scan datasets learner must generalize compositionaluses single lexical item jump multi-word modifier around right instruction actionsequences scan vice-versa nacs bastings sequence-to-sequence model unable tomake correct generalization apply geca enable succeed time scores averagedover random seed standard deviation across seed show improvement significant pairedbinomial test effect augmentation procedure original jump split training example geca generate additional template distinct fragments.with original augmented datasets wetrain one-layer lstm encoder–decoder modelwith embed size hidden size of512 bidirectional encoder attentional decoder hochreiter schmidhuber bahdanau model train usingadam kingma step size of0.001 dropout rate show table line theoriginal experiment lake baroni baseline sequence-to-sequence model completely failsto generalize test applying geca allow learned model successfully make mosttested generalization across single multi-wordentries instruction-to-action actionto-instruction directions.analysis examples synthesized example show figure success primitive condition stem constraint thesingle example usage primitive must still avalid command action pair verb validcommands isolation three examples—run walk walk look look—provide theevidence geca synthesize usage jump remove sequenceto-sequence model training accuracy would beunchanged geca would fail synthesize anynew example involve jump test accuracywould fall zero template condition geca correctly replace occurrence lturnwith rturn produce example aroundright template example highlight usefulness geca ability discover discontinuousand non-context-free substitutions.analysis dataset statistic understand behavior geca conduct finaladd primitive jump walk thrice walk rightrturn walk walk walk walkjump opposite leave thrice turn opposite rightrturn rturn lturn lturn jump lturn lturn jumplturn lturn jumpadd template around right look right twice turn opposite right twicerturn look rturn look rturn rturn rturn rturnrun around right walk opposite right twicerturn rturn rturn rturn rturnrturn walk rturn rturn walkfigure examples synthesize scan tasks.underlined word belong filled-in fragment theremaining text template geca synthesize someexamples exactly capture desired generalization example unrelated.set analysis quantify overlap thesynthesized data held-out data firstmeasure full example overlap fraction testexamples appear augmented training design overlap exist test setand original training apply geca,5 test example primitive conditionand example template conditionare automatically synthesize next measuretoken co-occurrence overlap compute setof input output tokens occur together inany test example measure fraction ofthese pair also occur together trainingexample primitive condition gecaincreases token co-occurrence overlap template condition even prior augmentation.it important note geca seesonly training unaware subsetof data single generalization testingat evaluation time data augmentation protocol generate large number spurious trainingexamples unrelated desire generalization7561 first example figure however alsogenerates enough usage target conceptthat learner generalize successfully.5 semantic parsingnext turn problem semantic parsing also popular subject study forquestions compositionality generalization data augmentation reason discussedin section expect qualitatively different behavior approach real language datawithout controlled vocabulary scan.we study four version geoquerydataset zelle consist englishquestions united states geography pairedwith meaning representation form eitherlogical expression query standardtrain–test split dataset ensure natural language question repeat trainand test finegan-dollak note provide limited test generalization many test example feature logical form thatoverlaps training data introduce amore challenging query split ensure neitherquestions logical form repeat even afteranonymizing name entity extract fragment andat token query split theoriginal training contain example gecagenerates additional distincttemplates distinct fragment question split baseline model andliang query split samesequence-to-sequence model scanand introduce supervised copy mechanism offinegan-dollak environments areagain take identical templates.results show table splitfor liang report result geca achieve nearly improvement withweaker domain assumption remainingsplits accurate.analysis example synthesized examples forthe logical representation shownin figure despite fact sequenceto-sequence model neither gold entity or1in case average slightly thesingle-run result previously report literature notealso original publication liang reportsdenotation accuracy result take theiraccompanying code release overall trend across system arecomparable either evaluation metric.query questionlogical formsseq2seq geca geca concat queriesiyer geca meaning representation exact-match accuracy geoquery dataset logical form geca approach data augmentation approach ofjia liang standard split data question outperform split design totest compositionality query expression geca lead substantial improvement querysplit achieve state-of-the-art result scores areaveraged random seed standard deviationacross seed shown.1 †significant improvement overseq2seq baseline ‡significant improvementover liang t-test usedfor experiment paired binomial test specialize entity link machinery augmentation procedure successfully aligns natural language entity name logical representationsand generalizes across entity choice procedure also produce plausible unattested entitieslike river name florida state name westwyoming.the last example logical form sectionis particularly interesting extracted fragmentcontains population density naturallanguage side density logical formside however environment constrain substitution happen appropriate fragmentwill case environmentalready contain necessary smallest.some substitution semantically problematic example final datapoint figure asksabout population number substitution replace capital area corresponding expression would fail execute.aside type problem however exampleis syntactically well-formed provide correctevidence constituent boundary alignmentsand hierarchical structure within geography domain synthesize example like secondto-last figure correct meaning representation ungrammatical natural language inputs.7562logical formswhat point rhode island lowest place const stateid rhode island state florida state const riverid florida traverse state border state population density state next_to small state density querieswhat river west wyomingselect river0.name river river0 river0.traverse west wyoming state town major name springfieldselect city0.state_name city city0 city0.name springfield city0.pop population area large stateselect city0.pop city city0 city0.name select state0.area state state0where state0.area select state1.area state state1 figure examples synthesize semantic parsing geoquery substituted fragment underlined gecaaligns name entity logical representation abstract predicate sometimes finalexample synthesize example semantically questionable plausible hierarchical structure.analysis dataset statistic applying geca tothe geoquery data increase full example overlap describe section thequestion split language querysplit logical form query splitwith expression line observation accuracy improvement great thequery split question split augmentationincreases token co-occurrence overlap across conditions.in larger-scale manual analysis synthesized example query split evaluatingthem grammaticality accuracy whetherthe natural language capture semantics thelogical form find grammatical semantically accurate.negative result conclude corresponding experiment scholar text-tosql dataset iyer similarquery questionsql queriesseq2seq geca negative result meaning representation accuracy scholar dataset query split synthesize example overlap theheld-out data question split provide littleinformation beyond already present training dataset case model train gecaperforms indistinguishably baseline model.to geoquery size diversity complexity.in contrast geoquery however applicationof geca scholar provide improvement.on query split limited compositionalre-use sub-queries line observation finegan-dollak averagenesting depth scholar roughly half ofgeoquery correspondingly full example overlap augmentation remain tokenco-occurrence overlap increase onthe question split full example overlap large token co-occurrence overlap increase byless result suggest geca ismost successful increase similarity ofword co-occurrence statistic training andtest input dataset exhibit highdegree recursion.6 low-resource language modelingboth previous section investigate conditional model fragment extract reusedby geca essentially synchronous lexicon entry line example originally motivate geca monolingual problem whichwe simply wish improve model judgment aboutwell-formedness conclude language model experiments.we wikipedia dumps2 five language kinyarwanda pashto pisin subsetof english wikipedia well dataset ofadams language exhibit theperformance geca across range morpholog2https //dumps.wikimedia.org/7563eng train token geca perplexities low-resource language modeling english kinyarwanda pashto pisin even kneser–ney smooth model rather high-capacityneural model apply geca lead small improvement perplexity †significant improvement baseline pair binomial test complexity example kinyarwanda acomplex noun class system kimenyi andpashto rich derivational morphology tegeyand robson pisin arecomparatively simple morphologically enfield,2008 verhaar training datasets range from10k–2m token like adams find thata modified kneser–ney language model outperform several varietiesof language model base gecaexperiments n-gram model instead usethe implementation provide kenlm heafield,2011 extract fragment maximum size token environment takento window around extracted fragment usage generate fragmentsthat occur time data kinyarwanda base dataset contain sentences.geca generate additional distinct template distinct fragments.rather train directly augmenteddataset precede section find thatthe best performance come train language model original dataset theaugmented dataset interpolate finalprobabilities weight interpolation isdetermined validation dataset choose tobe show table improvements arenot universal modest precede section however geca decrease perplexity across multiple language never increasesthem result suggest substitutionprinciple underlie geca useful mechanismfor encourage compositionality even outside conditional task neural models.analysis example statistic languagemodeling geca function smoothing scheme primary effect move mass toward n-gramsthat appear productive context sense geca perform similar role kneser–neysmoothing also experiment withgeca contrast kneser–ney notion context look forward well backward capture longer-range interactions.examples synthesized sentence shownin figure sentence grammatical andmany substitution preserve relevant semantic type information substitute location location time time however illformed sentence also generated.as section manually inspect synthesized sentence sentence evaluatedfor grammaticality since explicit semantics provide instead evaluate forgeneric semantic acceptability case only51 synthesized sentence semantically acceptable grammatical.7 discussionwe introduce geca simple data augmentationscheme base identify local phrase substitution license common context anddemonstrated extra training example generate geca lead substantial improvementson diagnostic natural datasets semanticvarious copy portion code hammurabihave find baked clay tablet possibly celebrated basalt stele thenight work contains appendix german equivalent technical term glock system aclu propose direction forthe organization late triassic early nineteenth century number scots-irish trader live among choctaw andmarried high-status woman sentences synthesize english language model task example syntacticallywell-formed also semantically plausible.7564parsing language modeling.while approach surprisingly effective inits current form view result primarilyas invitation consider carefully roleplay representation sentence fragment inlarger question compositionality blackbox sequence model procedure detail inthis paper relies exact string match identifycommon context future work might take advantage learned representation span theirenvironments mikolov peters al.,2018 improvement likely obtainableby constrain extracted fragment respectconstituent boundary syntactic informationis available.the experiment present focus rewrite sentence evidence within dataset encourage generalization output alternative line work paraphrase-based data augmentation ganitkevitch iyyer external text-only resource encourage robust interpretation input correspond toknown output line work could becombined geca-identified fragmentsto indicate productive location sub-sententialparaphrasing.more generally present result underline theextent current model fail learn simple context-independent notion reuse also howeasy make progress towards address thisproblem without fundamental change modelarchitecture.reproducibilitycode experiment paper foundat github.com/jacobandreas/geca.acknowledgmentsthanks oliver adams assistance thelanguage model experiment anonymous reviewer suggestion analysis section
human conversation naturally evolve aroundrelated concept scatter multi-hop concept paper present conversationgeneration model conceptflow leverage commonsense knowledge graph explicitly model conversation flow ground conversation concept space conceptflow represent potential conversation flow traverse concept spacealong commonsense relation traverseis guide graph attention concept graph move towards meaningful direction concept space order generate semantic informative response experiments reddit conversation demonstrate conceptflow effectiveness previous knowledge-aware conversation model gpt-2 base modelswhile parameter confirm advantage explicit model conversation structure source code thiswork available http //github.com/thunlp/conceptflow.1 introductionthe rapid advancement language modelingand natural language generation techniqueshave enable fully data-driven conversation model directly generate natural language responsesfor conversation shang vinyals andle however common problem generation model degenerate dull repetitive content holtzmanet welleck conversation assistant lead off-topic uselessresponses tang zhang often develop around knowledge.a promise address degeneration prob∗indicates equal contribution.†part work conduct tsinghua university.original graphchat base futuresteampaperclasstalktexthopeplanvoice bookwritefaithdreambagcardwordideawaterdreamhopefaithfuturetalktext writecardwordchatvoicepost： chat base knowledge futureresponse：yeah dream talk robotzero-hop concept one-hop concept two-hop conceptfigure example concept shift conversation darker green indicate high relevance andwider arrow indicate strong concept shift capturedby conceptflow grind conversation external knowledge xing open-domainknowledge graph ghazvininejad commonsense knowledge base zhou orbackground document zhou recent research leverage external knowledgeby grind conversation integrate additional representation thengenerating response condition textsand ground semantics ghazvininejad al.,2018 zhou external knowledge extra semanticrepresentations additional input conversation model effectively improve quality ofgenerated response ghazvininejad logan zhou never2032theless research discourse developmentsuggests human conversation still people chat around number related concept shift focus concept others.grosz sidner model concept shiftby break discourse several segment anddemonstrating different concept objectsand property need interpret differentdiscourse segment attentional state introduce represent concept shift correspondingto discourse segment fang show people switch dialog topic entirely conversation restricting utilizationof knowledge directly appear theconversation effective reachthe full potential knowledge model humanconversations.to model concept shift human conversation work present conceptflow conversation generation concept flow leverage commonsense knowledge graphsto model conversation flow explicitconcept space example show figure concept conversation reddit evolves chat future adjacentconcept talk also distant concept dream along commonsense relations—a typical involvement natural conversation bettercapture conversation structure conceptflowexplicitly model conversation traverse incommonsense knowledge graph start thegrounded concept e.g. chat future andgenerates meaningful conversation along commonsense relation relatedconcepts e.g. talk dream traverse concept graph guide bygraph attention mechanism derive fromgraph neural network attend appropriate concept conceptflow learn modelthe conversation development along meaningful relation commonsense knowledgegraph result model able grow thegrounded concept conversation utterance along commonsense relation distant meaningful concept guide themodel generate informative on-topicresponses modeling commonsense knowledge asconcept flow good practice improvingresponse diversity scatter current conversation focus concept chen implementation solution attentionalstate mention grosz sidner experiment reddit conversationdataset commonsense knowledge graph conceptnet speer demonstrate theeffectiveness conceptflow automaticand human evaluation conceptflow significantlyoutperforms various seq2seq base generation model sutskever well previousmethods also leverage commonsense knowledge graph static memory zhou al.,2018a ghazvininejad conceptflow also outperform finetuned gpt-2 system radford whileusing parameter explicitly modelingconversation structure provide well parameterefficiency.we also provide extensive analysis casestudies investigate advantage modelingconversation flow concept space analysis show many reddit conversation naturally align path commonsenseknowledge graph incorporate distant conceptssignificantly improve quality generated response on-topic semantic informationadded analysis confirm effectiveness graph attention mechanism selectinguseful concept conceptflow ability leverage generate relevant informative less repetitive responses.2 related worksequence-to-sequence model e.g. sutskever widely natural languagegeneration build conversation system shang vinyals recently pretrained language model elmo devlinet unilm dong gpt2 radford boost nlgperformance large scale pretraining nevertheless degenerating irrelevant off-topic andnon-useful response still main challenge conversational generation rosset al.,2020 tang zhang gaoet work focus improve conversationgeneration external knowledge example incorporate additional text ghazvininejad al.,2018 vougiouklis long knowledge graph longet ghazvininejad have2033shown external knowledge effectively improvesconversation response generation.the structure knowledge graph include richsemantics represent entity relation hayashi lots previous study focus task-targeted dialog system base ondomain-specific knowledge base generate response large-scale knowledge base zhouet utilize graphattention knowledge diffusion select knowledge semantics utterance understanding andresponse generation moon focuseson task entity selection take advantageof positive entity appear golden response different previous research conceptflow model conversation flow explicitly withthe commonsense knowledge graph present anovel attention mechanism concept guidethe conversation flow latent concept space.3 methodologythis section present conversation generationmodel latent concept flow conceptflow model ground conversation concept graph traverse distant concept alongcommonsense relation generate responses.3.1 preliminarygiven user utterance mwords conversation generation model often usean encoder-decoder architecture generate response encoder represent user utterance arepresentation oftendone gated recurrent units ~hi−1 embedding word xi.the decoder generate t-th word responseaccording previous generate wordsy yt−1 user utterance =n∏t=1p yt|y minimize cross-entropy loss andoptimizes parameter end-to-end =n∑t=1crossentropy token golden response.attention attention attention control gate ∗responseconcept graph vocabcentraltwo-hop012arg softmax outer subflow decoder output 'gru gru… gru…concept embedding central concept +gnncentral graph graph /postgru gru…post embedding softmax figure architecture conceptflow.the architecture conceptflow show infigure conceptflow first construct conceptgraph central graph gcentral outer graphgouter accord distance thegrounded concept conceptflow encode central andouter concept flow central graph gcentral andouter graph gouter graph neural networksand concept embed decoder present section leveragesthe encoding concept flow utterance togenerate word concept responses.3.2 concept graph constructionconceptflow construct concept graph theknowledge conversation start thegrounded concept zero-hop concept whichappear conversation utterance annotatedby entity link systems.then conceptflow grow zero-hop conceptsv one-hop concept two-hop concept concepts well asall relation form central concept graph gcentral closely relate thecurrent conversation topic concepts connection form outer graph gouter.20343.3 encoding latent concept flowthe construct concept graph provide explicit semantics concept relate commonsenseknowledge conceptflow utilize model theconversation guide response generation itstarts user utterance traverse throughcentral graph gcentral outer graph gouter ismodeled encode central outer conceptflows accord user utterance.central flow encoding central conceptgraphgcentral encode graph neural networkthat propagate information user utteranceh central concept graph specifically itencodes concept gcentral representation ~gei ~gei gcentral concept embedding thereis restriction model wechoose graftnet whichshows strong effectiveness encode knowledgegraphs detail graftnet find inappendix a.3.outer flow encoding outer flow connect two-hop concept encode ~fep attention mechanism ~fep =∑ekθek embeddings andare concatenate attention aggregatesconcept triple ~fep softmax tanh relation embed concept neighbor concept andwt trainable parameter provide efficientattention specifically focus relation formulti-hop concepts.3.4 generating text concept flowto consider user utterance related information text user utterance thelatent concept flow incorporate decoderusing component context representation combine encoding conditioned generation word conceptsfrom context representation context representationto generate t-th time response token first calculate output context representation t-thtime decoding encoding utteranceand latent concept flow.specifically calculate update step output representation ~st−1 step context representation ~ct−1 ~st−1 ~ct−1 ~yt−1 ~yt−1 step generate token yt−1 embedding context representation~ct−1 concatenate text-based representation~c textt−1 concept-based representation ~cconceptt−1 ~ct−1 textt−1 ~ccptt−1 text-based representation textt−1 read theuser utterance encodingh standard attentionmechanism bahdanau textt−1 =m∑i=1αjt−1 attention αjt−1 utterance token αjt−1 softmax ~st−1 concept-based representation conceptt−1 acombination central flow encoding ~ccptt−1 ∑ei∈gcentralβeit−1 ~gei◦ ∑fep∈gouterγft−1 ~fep attention βeit−1 weight central conceptrepresentations βeit−1 softmax ~st−1 ~gei attention γft−1 weight flow representation γft−1 softmax ~st−1 ~fep generating tokensthe t-th time output representation include information utterance text concept different step attention upon decoder leverage togenerate t-th token form informativeresponses.it first gate control generation bychoosing word central concept concept argmaxσ∈ ffnσ generation probability word centralconcept outer concept calculated2035over word vocabulary central concept outer concept ∼softmax ~gei word embed word ~geiis central concept representation concept eiand two-hop concept embedding.the training prediction conceptflow areconducted follow standard conditional languagemodels place andtraining cross-entropy loss onlyground truth response training noadditional annotation required.4 experiment methodologythis section describe dataset evaluation metric baseline implementation detail ourexperiments.dataset experiment multi-hop extend conversation dataset base previousdataset collect single-round dialog fromreddit zhou dataset contains3,384,185 training pair test pair preprocessed conceptnet speer usedas knowledge graph contain concept relation types.evaluation metrics wide range evaluation metric evaluate quality ofgenerated response serban bleu papineni nist doddington,2002 rouge meteor lavie andagarwal relevance repetitiveness dist-1 dist-2 ent-4 diversity previous work al.,2016a zhang metric areevaluated implementation galleyet zhou concept pplmainly focus concept ground model andthis metric report appendix a.1.the precision recall score toevaluate quality learned latent concept flowin predict golden concept appear inground truth responses.baselines baseline compare comefrom three group standard seq2seq knowledgeenhanced fine-tuned gpt-2 systems.seq2seq sutskever basicencoder-decoder language generation.knowledge-enhanced baseline include memnet ghazvininejad copynet zhuet zhou memnet maintain memory store read concepts.copynet copy concepts response generation zhou leverage graphattention mechanism model central concepts.these model mainly focus grounded concept explicitly model conversation structure multi-hop concepts.gpt-2 radford pre-trainedmodel achieve state-of-the-art lots oflanguage generation task also compare ourexperiments fine-tune gpt-2 twoways concatenate conversation together andtrain like language model gpt-2 lang extendthe gpt-2 model encode-decoder architectureand supervise response data gpt-2 conv details zero-hop concept areinitialized match keywords post toconcepts conceptnet zhouet zero-hop concept extendedto neighbor form central concept graph.the outer concept contain large amount twohop concept noise reduce thecomputational cost first train conceptflow select random training data thelearned graph attention select two-hopconcepts whole dataset standardtrain test conduct prune graph.more detail filtering step find inappendix a.4.transe bordes embed andglove pennington embed areused initialize representation conceptsand word respectively adam optimizer thelearning rate train model.5 evaluationfive experiment conduct evaluate generated response conceptflow effectiveness learned graph attention.5.1 response qualitythis experiment evaluate generation quality ofconceptflow automatically manually.automatic evaluation quality generated response evaluate different metricsfrom three aspect relevance diversity novelty table table show results.in table evaluation metric calculate therelevance generate response the2036model bleu-4 nist-4 rouge-1 rouge-2 rouge-l meteor pplseq2seq lang conv relevance generated golden responses results∗ gpt-2 directly comparable different tokenization result find appendix a.1.diversity novelty w.r.t input model dist-1 dist-2 ent-4 bleu-4 nist-4 rouge-2 rouge-l meteorseq2seq lang conv diversity high better novelty well generated response diversity calculate withingenerated response novelty compare generate response input post result appendix a.1.model parameter average score best ratioapp inf.ccm gpt-2 conv conceptflow golden human table human evaluation appropriate andinformativeness average score take average human judgment best ratio indicatesthe fraction judge consider case best thenumber parameter also presented.model inf.conceptflow-ccm fleiss kappa human agreement test scenario appropriate informativeness evaluate quality generatedresponse fleiss kappa evaluate agreement fromvarious annotator focus comparison oftwo model three category loss.golden response conceptflow outperforms allbaseline model large margin responsesgenerated conceptflow on-topic andmatch well ground truth responses.in table dist-1 dist-2 ent-4 measure theword diversity generated response restof metric measure novelty compare thegenerated response user utterance conceptflow good balance generate noveland diverse response gpt-2 response morediverse perhaps sample mechanismduring decoding less novel on-topiccompared conceptflow.human evaluation human evaluation focus aspect appropriateness informativeness important conversationsystems zhou appropriateness evaluate response on-topic give utterance informativeness evaluate system ability provide information instead copyingfrom utterance zhou response sampled case select fromfour method good performance gpt-2 conv conceptflow golden response.the response score five judge high good present average score best ratio human judge first mean fivejudges latter calculate fraction judgesthat consider corresponding response bestamong four system conceptflow outperformsall model scenario parameter compare gpt-2 thisdemonstrates advantage explicitly modelingconversation flow structured semantics.the agreement human evaluation test todemonstrate authenticity evaluation results.we first sample case randomly humanevaluation response four better2037conversation system gpt-2 conv conceptflow golden responses provide witha random order group annotator toscore response range accordingto quality test scenario appropriateness informativeness annotator noclues source generate responses.the agreement human evaluation gpt-2 conv conceptflow present intable case response conceptflow compare response twobaseline model gpt-2 conv thecomparison result divide three category loss human evaluation agreement calculate fleiss kappa theκ value range indicate fairagreement confirm quality humanevaluation.both automatic human evaluation illustratethe effectiveness conceptflow next experiment study effectiveness multi-hopconcepts conceptflow.5.2 effectiveness multi-hop conceptsthis part explore role multi-hop concept inconceptflow show figure three experiment conduct evaluate performancesof concept selection quality generatedresponses different concepts.this experiment considers four variation ofouter concept selection base ignore two-hopconcepts considers central concepts.rand distract full two-hop concept inthree different rand select concept randomly distract select concept appear inthe golden response random negative distractors full conceptflow select thatselects concepts learned graph attentions.as show figure full cover moregolden concept base align ourmotivation natural conversation flow fromcentral concept multi-hop compared todistract ground truth two-hop concept conceptflow select slightlyless coverage significantly reduce numberof two-hop concepts.the second experiment study model ability generate ground truth concept compare concept generated response withthose ground truth response show figure though full filter golden twodepth amount golden coverageratio numberzero-hop one-hop two-hop three-hop statistics concept graphs differenthops include total amount connect concept ratio number covered golden concept appear ground truth response conceptflow indicate filtered two-hop graph.hop concept outperform variation bylarge margin show conceptflow graph attention mechanism effectively leverage prunedconcept graph generate high-quality conceptswhen decoding.the high-quality latent concept flow lead tobetter modeling conversation show figure full outperforms distract generated response token level perplexity even thoughdistract include ground truth two-hop concepts.this show negative select conceptflow directly appear target response also on-topic include meaningfulinformation select graph attention instead random.more study multi-hop concept selectionstrategies find appendix a.2.5.3 steps concept graphthis experiment study influence stepsin concept graph.as show table number coveredgolden concept increase compared zero-hop concept multi-hop conceptscover golden concept confirm conversation naturally shift multi-hop concept extend concept graph one-hop twohop improve recall tothree-hop improves time amount theconcepts also increase dramatically multiplehops three lead concept average entire graph inthis work choose two-hop good balanceof coverage efficiency conceptflow select filter around concept constructthe pruned graph efficiently effectively leverage distant concept graphis reserve future work.2038 golden concept coverage response concept generation response token generation.figure comparisons outer concept selection methods base consider central concept ignorestwo-hop concept rand randomly select two-hop concept distract incorporate golden concept responsewith random negative distractors full choose two-hop concept conceptflow graph attention.figure case study best view color left attention flow commonsense concept graph zerohop concept one-hop concept two-hop concept highlight right attention score concepts.darker green indicate high attention score central concept two-hop concept.figure distribution attention score distribution overall concept certain part golden concept golden response zero-hop concept appear post present attention score calculate scale mean attentionscores step decoding.5.4 case studysome case three conversation model arelisted table responses repeatthe content explicitly modelthe traverse concept space example response first third case alwaysrepeat sure hand gpt-2generates fluent response compare ccm.nevertheless case gpt-2 merely copycontents concept give post example third case gpt-2 conv mainlydiscusses concept music.in comparison generated response ourconceptflow fluent informative thanthose gpt-2 example third case conceptflow bring associatedconcepts sound check response generation grounded concept music advice introducing multi-hopconcepts effectively improve informativenessand diversity generated responses.figure present case study conceptflow.the attention score present theform color intensity championship ofzero-hop one-hop team two-hopreceive attention others to2039 post actually stay building right next lighthouse believe boiler room group already sleep lighthouse sure joking sure talk lighthouse sure blanket blanket conv pretty sure room door good luck sure good thing house post point beneficial keep playing much next season body think still season think great player conv sure fine great player player play game last year play post know change music feel natural thanks advice though sure joking sure sarcastic sure rebeing sarcastic sure sarcastic pretty sure video conv play different music always even notice natural sound check post milan inter longer club give currently financesto compete best milan club club world conv think club long time best part team want play best team country post never felt like friend whoopsccm sorry feel conv never felt like friend whoop never felt like friend whoop call friend table cases generated different models tokens zero concept one-hop concept two-hopconcepts highlighted.generate response concept flow model concept shift user post toresponse concept flow team describe concept shift response generation addition concept pretty share high attention helpto understand one-hop concept filter generate response gate σ∗according relevance conversation topic.5.5 learned attentions conceptsthis experiment study learned attention ofconceptflow different group concept weconsider average attention score centralconcepts appendix two-hop concept decode step probabilitydensity attention plot figure show attention weight central concept conceptflow effectively attendsmore golden zero-hop concept include useful information attention ontwo-hop concept show figure conceptflow attend slightly golden twohop concept rest two-hop thoughthe margin smaller—the two-hop concept arealready filter high-quality theconceptflow select step.6 conclusion future workconceptflow model conversation structure explicitly transition latent concept space inorder generate informative meaningfulresponses experiment reddit conversation illustrate advantage conceptflow overprevious conversational system study confirm conceptflow advantage come thehigh coverage latent concept flow well itsgraph attention mechanism effectively guidesthe flow highly related concept humanevaluation demonstrate conceptflow generate appropriate informative responseswhile much parameters.in future plan explore combineknowledge pre-trained language model e.g.gpt-2 effectively efficiently introduce concept generation models.acknowledgmentshouyu zhang zhenghao zhiyuan issupported national research development program china national natural science foundation ofchina nsfc wethank hongyan wang shuo wang kaitao zhang huimin chen xuancheng huang zeyunzhang zhenghao houyu zhang humanevaluations.2040
crucial step extractive document summarization learn cross-sentence relationshas explore plethora approaches.an intuitive graphbased neural network complex structure capture inter-sentence relationship paper present heterogeneous graph-based neural network extractive summarization hetersumgraph contain semantic node differentgranularity level apart sentence theseadditional node intermediary sentence enrich cross-sentencerelations besides graph structure isflexible natural extension singledocument multi-document introduce document node knowledge first introduce different typesof node graph-based neural network forextractive document summarization perform comprehensive qualitative analysis toinvestigate benefit code release github1.1 introductionextractive document summarization extractrelevant sentence original document andreorganize summary recent yearshave resounding success deepneural network task cheng lapata,2016 narayan arumae zhong lapata theseexisting model mainly follow encoder-decoderframework sentence encodedby neural component different forms.to effectively extract summary-worthy sentence document core step model∗these author contribute equally.†corresponding author.1https //github.com/brxx122/hetersumgraphthe cross-sentence relation current model capture cross-sentence relation recurrentneural network rnns cheng lapata nallapati zhou however rnns-based model usually hard capture sentence-level long-distance dependency especially case long document multidocuments intuitive modelthe relation sentence graph structure nevertheless challenge find effective graph structure summarization effortshave make various early traditionalwork make inter-sentence cosine similarity build connectivity graph like lexrank erkan radev textrank mihalceaand tarau recently work accountfor discourse inter-sentential relationship whenbuilding summarization graph approximate discourse graph sentencepersonalization feature yasunaga andrhetorical structure theory graph al.,2019 however usually rely external toolsand need take account error propagationproblem straightforward createa sentence-level fully-connected graph someextent transformer encoder vaswani al.,2017 recent work zhong lapata classify thistype learn pairwise interaction betweensentences despite success constructan effective graph structure summarization remain open question.in paper propose heterogeneous graphnetwork extractive summarization instead ofsolely build graph sentence-level node weintroduce semantic unit additional nodesin graph enrich relationship betweensentences additional node intermediary connect sentence namely eachadditional node view special rela6210tionship sentence contain duringthe massage passing heterogeneous graph additional node iteratively updatedas well sentence nodes.although advanced feature e.g. entity topic simplicity usewords semantic unit paper eachsentence connect contain word thereare direct edge sentence pair andword pair construct heterogeneous wordsentence graph following advantage different sentence interact inconsideration explicit overlapping word information word node also aggregateinformation sentence update unlike exist model usually keep wordsunchanged embed layer different granularity information fully usedthrough multiple message passing process heterogeneous graph network expandablefor type node example introduce document node multi-document summarization.we highlight contribution follow knowledge first construct heterogeneous graph network extractivedocument summarization model relation sentence contain sentencenodes also semantic unit although wejust word node paper superiorsemantic unit entity incorporate propose framework flexiblein extension easily adapt singledocument multi-document summarization task model outperform exist competitor three benchmark datasets without thepre-trained language models2 ablation study andqualitative analysis show effectiveness ourmodels.2 related workextractive document summarization withthe development neural network great progresshas make extractive document summarization focus encoderdecoder framework recurrent neural network cheng lapata nallapati al.,2017 zhou transformer encoders2since propose model orthogonal methodsthat pre-trained model believe model befurther boost take pre-trained model initializethe node representation reserve future zhong wang thesentential encoding recently pre-trained languagemodels also apply summarization contextual word representation zhong lapata zhonget intuitive structure extractive summarization graph better utilize thestatistical linguistic information sentence early work focus document graphsconstructed content similarity among sentence like lexrank erkan radev andtextrank mihalcea tarau recent work incorporate relational prioriinto encoder graph neural network gnns yasunaga methodologically work type node formulate document homogeneousgraph.heterogeneous graph graph neuralnetworks associate learning method message passing gilmer selfattention velickovic originallydesigned homogeneous graph thewhole graph share type node however graph real-world application usually come multiple type node al.,2016 namely heterogeneous graph modelthese structure recent work make preliminary exploration introduce aheterogeneous graph neural network encode document entity candidate together multihop reading comprehension linmei focus semi-supervised short text classification construct topic-entity heterogeneousneural graph.for summarization propose heterogeneous graph consisting topic word andsentence node markov chain modelfor iterative update wang modifytextrank graph keywords sentence thus forward heterorank inspiredby success heterogeneous graph-basedneural network task introduceit extractive text summarization learn betternode representation.3 methodologygiven document sentence formulate extractive summarizationas sequence label task narayan layer𝑠1𝑠2sentenceselectortf-idfedge featureword node sentence nodefigure model overview framework consist ofthree major module graph initializers heterogeneous graph layer sentence selector green circle blue represent word sentence nodesrespectively orange solid line denote edge feature tf-idf word sentence node thethicknesses indicate weight representation ofsentence node finally summary selection.liu lapata goal predict asequence label sentence represent i-th sentenceshould include summary groundtruth label call oracle extractedusing greedy approach introduce nallapatiet automatic metric rouge hovy speak heterogeneous summarization graph consist type node basicsemantic node word concept relaynodes unit discourse phrase sentence document supernodes eachsupernode connects basic node contain init take importance relation theiredge feature thus high-level discourse node canestablish relationship basicnodes.in paper word basic semantic node simplicity hetersumgraph insection special case containsone type supernodes sentence classification heterdocsumgraph section3.5 document sentence based onour framework type supernodes asparagraphs also introduce onlydifference graph structure.3.1 document heterogeneous graphgiven graph standsfor node represent edge betweennodes undirected heterogeneous graph canbe formally define denote unique word document correspond sentence thedocument real-value edge weight matrixand indicate j-th sentence contain i-th word.figure present overview model mainly consist three part graph initializers node edge heterogeneousgraph layer sentence selector initializers first create node edge encodethem document graph heterogeneous graph update node representation byiteratively pass message word sentence node graph attention network velickovic finally representation sentence node extract predictlabels summaries.3.2 graph initializerslet rm×dw rn×ds represent theinput feature matrix word sentence node respectively dimension wordembedding dimension sentence representation vector specifically firstuse convolutional neural networks lecun different kernel size capture local n-gram feature sentence ljand bidirectional long short-termmemory bilstm hochreiter schmidhuber,1997 layer sentence-level feature concatenation local feature andthe bilstm global feature sentencenode feature include information importance relationship word sentence node infuse tf-idf value edgeweights term frequency numberof time occurs inverse documentfrequency make inverse function ofthe out-degree wi.62123.3 heterogeneous graph layergiven constructed graph node featuresxw edge feature graph attention network velickovic updatethe representation semantic nodes.we refer hidden state input node graphattention layer design follow leakyrelu wqhi wkhj =exp ∑l∈ni ∑j∈niαijwvhj trainable weightsand attention weight multi-head attention denote ‖kk=1σ∑j∈niαkijwkhi besides also residual connection toavoid gradient vanish several iterations.therefore final output represent modify layer infusethe scalar edge weight mappedto multi-dimensional embedding space ∈rmn×de thus equal modify follow leakyrelu wqhi wkhj graph attention layer introduce aposition-wise feed-forward layer consistingof linear transformation transformer vaswani update pass message betweenword sentence node define informationpropagation figure specifically theinitialization update sentence node theirneighbor word ffnlayer u1s←w u1s←w +h0s u1s←w ∈rm×dh denote isused attention query thekey value.𝑤3𝑤1𝑤2𝑠1𝑠2𝑤3𝑤1𝑤2𝑠1𝑠2 update update 𝑤1figure detailed update process word sentence node heterogeneous graph layer green andblue node word sentence node involve inthis turn orange edge indicate current information flow direction first sentence word andw3 aggregate word-level information update representation ands2 sentence occur section3.3 detail notation.after obtain representation forword node updated sentence andfurther update sentence node iteratively eachiteration contain sentence-to-word wordto-sentence update process t-th iteration process represent ut+1w←s ht+1w ut+1w←s +htw ut+1s←w ht+1w ht+1w ht+1s ut+1s←w +hts figure show word node aggregate thedocument-level information sentence forexample high degree word node indicatesthe word occur many sentence likelyto keyword document regardingsentence node important wordstends select summary.3.4 sentence selectorfinally need extract sentence node includedin summary heterogeneous graph.therefore node classification sentencesand cross-entropy loss training objective whole system.trigram block following paulus lapata trigram blocking decoding simple powerful version maximal marginal relevance carbonelland goldstein specifically rank sentence score discard havetrigram overlapping predecessors.6213𝑤 figure graph structure heterdocsumgraphfor multi-document summarization correspond tothe graph layer part figure green blue andorange represent word sentence documentnodes respectively consist d2contains relay node relation ofdocument-document sentence-sentence sentencedocument build common word nodes.for example sentence share sameword connect across documents.3.5 multi-document summarizationfor multi-document summarization documentlevel relation crucial good understanding thecore topic important content cluster however exist neural model ignorethis hierarchical structure concatenate document single flat sequence fabbri others model relation attention-based full-connected graph takeadvantage similarity discourse relation liuand lapata framework establish document-levelrelationship sentence-levelby supernodes document figure mean easily adapt fromsingle-document multi-document summarization heterogeneous graph extendedto three type node vdand number ofsource document name heterdocsumgraph.as figure word node becomethe bridge sentence document sentences contain word connect witheach regardless distance across document document establish relationshipsbased similar contents.document node view special typeof sentence node document node connects withcontained word node tf-idf value usedas edge weight besides document node alsoshare update process sentence nodes.the difference initialization thedocument node take mean-pooling sentence node feature initial state thesentence selection sentence node concatenate corresponding document representation obtain final score multi-documentsummarization.4 experimentwe evaluate model single- multidocument summarization task startour experiment description datasets.4.1 datasetscnn/dailymail cnn/dailymail questionanswering dataset hermann nallapati widely usedbenchmark dataset single-document summarization standard dataset split contains287,227/13,368/11,490 example training validation test data prepossessing wefollow lapata nonanonymized version getground-truth labels.nyt50 nyt50 also single-document summarization dataset collect newyork times annotated corpus sandhaus preprocessed durrett contain article summary splitinto training test following durrett last training validation andfilter test examples multi-news dataset largescale multi-document summarization introducedby fabbri contain articlessummary pair example consist document human-written summary.following experimental setting split thedataset training validation test example truncate input articlesto tokens.4.2 settings hyper-parametersfor single-document multi-documentsummarization limit vocabulary initialize token gloveembeddings pennington filterstop word punctuation create word6214nodes truncate input document maximum length sentence thenoisy common word remove ofthe vocabulary tf-idf value thewhole dataset initialize sentence node withds edge feature gate withde layer head hiddensize inner hidden size ffnlayers train batch size andapply adam optimizer kingma witha learn rate early stop performedwhen valid loss descent three continuous epoch select number iterationst base performance validationset.3 decode select top-3 sentence forcnn/dailymail nyt50 datasets top-9 formulti-new accord average length theirhuman-written summaries.4.3 models comparisonext-bilstm extractive summarizer bilstm encoder learn cross-sentence relation byregarding document sequence sentences.for simplification directly take initialization sentence node classification whichincludes encoder word level bilstm sentence level model canalso view ablation study hetersumgraph updating sentence nodes.ext-transformer extractive summarizers withtransformer encoder learn pairwise interaction vaswani sentence purelydata-driven fully connect priori following lapata implement atransformer-based extractor baseline whichcontains encoder word follow by12 transformer encoder layer sentence exttransformer regard sentence-levelfully connect graph.hetersumgraph heterogeneous summarization graph model relation sentence base common word bedenoted sentence-word-sentence relationships.hetersumgraph directly select sentence forthe summary node classification hetersumgraph trigram block utilizesthe n-gram blocking reduce redundancy.3the detailed experimental result attach appendix section.model r-llead-3 lapata narayan zhang dong zhou durrett zhong policy policy tri-blocking performance rouge propose model recently release summarization system oncnn/dailymail.5 results analysis5.1 single-document summarizationwe evaluate single-document model oncnn/dailymail nyt50 report unigram bigram long common subsequenceoverlap reference summary andr-l. limited computational resource wedon apply pre-trained contextualized encoder i.e.bert devlin model whichwe regard future work therefore compare model without bert forthe sake fairness.results cnn/dailymail table show theresults cnn/dailymail first part thelead-3 baseline oracle upper bound whilethe second part include summarization models.we present model describe section4.3 third part compared extbilstm heterogeneous graph achieve morethan improvement andr-l indicate cross-sentence relationship learn sentence-word-sentence structure powerful sequential structure besides model also outperform exttransformer base fully connect relationships.this demonstrate graph structure effectively prune unnecessary connection sentence thus improve performance sentence node classification.compared second block figure weobserve hetersumgraph outperform allprevious non-bert-based summarization systems6215and trigram block lead great improvementon rouge metric among luoet comparable competitor hetersumgraph formulate extractivesummarization task contextual-bandit problemand solve reinforcement learning since thereinforcement learning trigram blockingplays similar role reorganize sentence intoa summary zhong additionallycompare without policy gradient hetersumgraph hetersumgraph achieve0.61 improvement without policy sentence scoring hetersumgraphwith trigram block outperforms overher reorganized summaries.model r-lfirst sentence durrett first word durrett lead-3 durrett sumo paulus ext-bilstm tri-blocking limited-length rouge recall nyt50 testset result model copy liuand lapata mean original paper report result.results nyt50 results nyt50 summarize table note limited-lengthrouge recall durrett theselected sentence truncate length ofthe human-written summary recall scoresare instead first line baseline give durrett next twolines baseline extractive summarization.the second third part report performanceof non-bert-based work modelsrespectively.again observe cross-sentence relationship modeling performs well bilstmand transformer model also strong advantage non-bert-based approacheson nyt50 meanwhile find trigram blockdoesn work well show cnn/dailymail attribute reason special formationof summary cnn/dailymail dataset cnn/dailymail order betterunderstand contribution different modulesto performance conduct ablation study propose hetersumgraph model oncnn/dailymail dataset first remove filtering mechanism tf-idf word theedge weight respectively also remove residualconnections layer compensation concatenate initial sentence featureafter update message nearby word nodesin equal u1s←w furthermore make iteration number delete word updating sentence representation classification finally remove bilstm layer initializationof sentence nodes.as table show removal tf-idfwords lead increase dropson suspect filter noisy wordsenable model good focus useful wordnodes cost lose bigram information residual connection play importantrole combination original representation updating message another typeof node replace concatenation besides introduction edge feature word update bilstm initialization sentence also show effectiveness.5.2 multi-document summarizationwe first take concatenation first-k sentence source document baselineand code model outputs5 release byfabbri models.to explore adaptability model multidocument summarization concatenate multisource document single mega-document andapply hetersumgraph baseline forcomparison extend hetersumgraph tomulti-document setting heterdocsumgraph4nallapati concatenate summary bullet write different part article havefew overlap multi-sentence summary.however human write summary whole article nyt50 multi-news phrasesrepeatedly mean roughly remove sentence n-gramoverlaps lead loss important information.5https //github.com/alex-fabbri/ multi-news6216model r-lhsg filter word edge feature residual connection sentence update word update bilstm ablation study cnn/dailymail test set.we remove various module explore influenceon model mean remove module fromthe original hetersumgraph note hetersumgraph without updating sentence nodesis actually ext-bilstm model describe section4.3.as describe section result present table observe hetersumgraph heterdocsumgraph outperform previous method heterdocsumgraph achieve good performance improvement demonstrate introduction ofdocument node better model documentdocument relationship beneficial multidocument summarization mention trigram block work multi-newsdataset since summary write wholeinstead concatenation summary bulletsfor source document.model r-lfirst-1 erkan radev mihalcea tarau carbonell goldstein lebanoff gehrmann fabbri tri-blocking tri-blocking results test multi-news wereproduce model release code anddirectly output provide fabbri evaluation.5.3 qualitative analysiswe design several experiment probe intohow hetersumgraph heterdoc0.40.60.8∆r̃ degree word nodesr̃bilstmhsgfigure relationships average degree ofword node document x-axis isthe mean line left y-axis delta hetersumgraph ext-bilstm histogram right y-axis help single- multi-documentsummarization.degree word node hetersumgraph degree word node indicate occurrenceacross sentence thus measure redundancy document extent meanwhile word high degree aggregate information multiple sentence mean thatthey benefit iteration process.therefore important explore influenceof node degree word summarizationperformance.we first calculate average degree wordnodes example base constructedgraph test cnn/dailymail divide interval base x-axis figure4 evaluate performance hetersumgraph ext-bilstm various part themean score draw line leave y-axis rouge increase theincreasing average degree word nodesin document mean article witha high redundancy easy neural model tosummarize.to make model obvious wedraw histogram right y-axis figure observe hetersumgraphperforms much good document higheraverage word node degree prove benefit bring word node aggregationof information sentence propagationof global representations.number source document also investigate number source document influence performance model end,62172 source documentsr̃first-3hsghdsgfigure relationship number source document x-axis y-axis divide test multi-news differentparts number source document discard part less example wetake first-3 baseline concatenate thetop-3 sentence source document thesummary.in figure observe lead baseline raise model performancedegrade finally converge baseline.this challenge model toextract limited-number sentence cover themain idea source document increase number document however first-3baseline force take sentence document ensure coverage besides theincrease document number enlarge performance hetersumgraph heterdocsumgraph indicate benefit ofdocument node become significant formore complex document-document relationships.6 conclusionin paper propose heterogeneous graphbased neural network extractive summarization.the introduction fine-grained semanticunits summarization graph help modelto build complex relationship sentence also convenient adapt singledocument graph multi-document documentnodes furthermore model achievedthe best result cnn/dailymail compare withnon-bert-based model take pretrained language model account betterencoding representation node future.acknowledgmentthis work support national naturalscience foundation china u1936214 and61672162 shanghai municipal science technology major project andzjlab
propose novel method hierarchicalentity classification embrace ontological structure training prediction training novel multi-levellearning-to-rank loss compare positive typesagainst negative sibling accord typetree prediction define coarseto-fine decoder restrict viable candidatesat level ontology base alreadypredicted parent type achieve stateof-the-art across multiple datasets particularlywith respect strict accuracy.11 introductionentity typing assignment semantic labelto span text span usually mention entity real world named entity recognition canonical informationextraction task commonly consider form ofentity type assigns span handful type on.fine-grained entity typing seek classifyspans type accord diverse semantically rich ontology ling weld yosef gillick corroet choi begin tobe downstream model entity link gupta raiman raiman example figure fetdataset figer ling weld mention interest hollywood hills typedwith single label traditional butmay type type /location /geography /geography/mountain fine-grained typing scheme finergrained typing scheme type usually form hierarchy coarse type code find http //github.com/ctongfei/hierarchical-typing.location geographycity county mountain islandhe inter forest lawn memorialpark hollywood hills angeles ca.personartist doctormention representationentityfigure example mention classify thefiger ontology positive type highlighted.the level—these similar traditional nertypes /person additionally finertypes subtypes top-level type /person/artist /person/doctor.most prior work concern fine-grained entitytyping approach problem multilabel classification problem give entity mention together context classifier seeksto output type type nodein hierarchy approaches include handcraft sparse feature various neural architecture shimaoka linand inter alia section historical transition flat type relatively littlework exploit ontological tree structure type label satisfy hierarchicalproperty subtype valid parent supertype also valid propose novel methodthat take explicit ontology structure account multi-level learning rank approachthat rank candidate type condition thegiven entity mention intuitively coarse typesare easier whereas finer type hard classify capture intuition allow distinct margin level ranking model.8466entityveh weaaircraft bomb bulletsbulletsmolotovcocktailammunitionairplaneperson otherentityartist athleteactorauthorevent foodaccidentelectionaida ontonotesproduct organizationentityvehiclesubstanceweaponcorporationeducationalgovernmentmuseumchemicaldrugbbnl0l1l2l3otherotherotherotherotherfigure various type ontology different level type show different shade theentity special node discuss section novel coarse-to-fine decoder thatsearches type hierarchy approach guarantee prediction violate hierarchical property achieve state-of-the-art result accord multiple measure across various commonly datasets.2 related workfet usually study allow sentencelevel context make prediction notably start ling weld gillicket create commonlyused figer ontonotes datasets fet.while researcher consider benefit ofdocument-level zhang corpuslevel yaghoobzadeh schütze context focus sentence-level variant bestcontrast prior work.progress focus primarily better mention representation starting fromsparse hand-crafted binary feature ling andweld gillick community move distribute representation yogatama pre-trainedword embeddings lstms al.,2016a shimaoka abhishek al.,2017 shimaoka cnns murtyet mention-to-context attention zhang employingpre-trained language model like elmo peterset generate ever good representation approach buildsupon development state-of-theart mention encoders.• incorporating hierarchy priorworks approach hierarchical typing problemas multi-label classification without information hierarchical structure thereare exception propose adaptive margin learning-to-rankso similar type small margin barbosa propose hierarchicalloss normalization penalize output violate hierarchical property murty propose learn subtyping relation toconstrain type embeddings type space.in contrast approach coarse-tofine decoding approach strictly guarantee thatthe output violate hierarchical property lead good performance hyena yosef apply rank siblingtypes type hierarchy number predicted positive type train separately witha meta-model hence support neuralend-to-end training.researchers propose alternative formulation whose type form type hierarchy particular ultra-fine entity typing choiet xiong onoe durrett large type derivedfrom phrase mine corpus jinet label mention type knowledge base multiple relation form typegraph augment task withentity link kbs.3 problem formulationwe denote mention tuple wherew sentential context thespan mark mention interest sentencew mention interest hierarchical entity type model outputs8467a type type ontology hierarchy take form forest whereeach tree root top-level supertype e.g./person /location dummyparent node entity supertype allentity type top-level type effectivelytransforming type forest type tree figure show type ontology associate with3 different datasets subsection thedummy entity node augmented.we introduce notation referringto aspect type tree binary relation typez subtype denote theunique parent type type tree denotedȳ undefined entity.the immediate subtypes child node aredenoted siblings sharingthe immediate parent denote aida ontology figure themaximum depth tree eachmention type typefrom level term scenario singlepath typing since path startingfrom root entity type tree isin contrast multi-path typing bbndataset mention label multiple type level tree.additionally aida thereare mention label as/per/police/ unspecified figer find instance labeled type /person butnot subtype mean whena mention label partial type path i.e. type none subtypes weconsider interpretation exclusive type anytype undefined type whether aninstance unknown.we devise different strategy deal thesetwo condition exclusive case weadd dummy node every intermediate branch node type tree mention label type none subtypesz additional label y/other label figure aida example interpret partial type path /person2 programming language literature type system support subtyping.in figer exclusive another type/person/other instance theundefined case modify label thedataset make significantdifference depend specific datasetis annotated.4 model4.1 mention representationhidden representation entity mention sentence generate leverage recent advance language model pre-training elmo peters elmo representation token denote apply probability theelmo vectors.our mention encoder largely follow first mention representation derivedusing representation word mention apply pool layer atop mention linear transformation:4m maxpool employ mention-to-context attentionfirst describe zhang later employ context vector isgenerated attend sentence queryvector derive mention vector usethe multiplicative attention luong mtqwi =n∑i=1aiwi final representation entity mentionis generate concatenation mention andcontext vector r2dw type scorerwe learn type embed typey score instance representation pass feed-forwardnetwork space typespace tanh nonlinearity final3 find elmo performs well thanbert devlin internal experimentsalso confirm finding hypothesize dueto rich character-level information contain lowerlevel elmo representation useful fet.4 propose attentive pooler alearned global query vector find simple maxpooling layer achieve similar performance.8468score inner product transformedfeature vector type embedding ffnn hierarchical learning-to-rankwe introduce novel hierarchical learning-torank loss allow natural multi-labelclassification take hierarchical ontology account.we start multi-class hinge loss rankspositive type negative type weston andwatkins jflat =∑y∈y∑y′ actually learningto-rank ranking joachims themodel learn rank positive type yhigher negative type impose margin type shouldrank high note equation since linear margin hyperparameter could type embeddingsare linearly scalable rely regularization constrain type embeddings.multi-level margins however methodconsiders candidate type flat instead ofhierarchical type give treatment without prior relative positionin type hierarchy intuitively coarse type high hierarchy easy determine /person /location befairly easy model fine-grained type /person/artist/singer harder.we encode intuition learn ranktypes level type tree different margin parameter rankingmodel respect different level ∑y∈y∑y′∈sb ξlev level type example /location andlev /person/artist/singer inequation positive type comparedagainst negative sibling margin hyperparameter ξlev i.e. margin dependent level tree intuitively since latexit sha1_base64= yckm0lgeuwguywydtskkfywzpzs= aaacw3icdvfna9taef2rtzq6sey0x15etcatjzfcidkgsqhhfookybkzwo3ixfshdkenjdavsy/jj+q/6doxxuragyxhm3k7b2bsqgphufs7fbx4ubx9aud1+83u3n6ne/d20pnschxyi429tsghfbqhjejidwervcrxkp19weavfqj1wugftchwrobgi1xwie9nup1+fjsalkbwmzmlyfgk24sg0src5ybegx5bx8xkopuryqwvfwriepwbxvfb4woscs6xbielwwl4dg5w5keghw5crzzx4afnsja31j9n4yrdvfsgnfuo1fcqokl7mlus/8qnssrpxpxqrumo+wojvjqhmxc5hjatfjnjhqfarfbeqz4fc5z8stqhm20cb4kwzd2kpujrz6ixyy/+8p+9o+mvpvdzaryl+cptuzedcb+o5pl+i9tupcbmcfiv9l9z9fxckau6+5qqzzhdkamiovht7xk/vd5zchk8iknb/p2kd95fx3ahvwcfwj/f7jsds2/sgg0zzyw7y/fsifgazaib0gnp0fpr3rfgbpufkwdgvg== /latexit latexit sha1_base64= yckm0lgeuwguywydtskkfywzpzs= aaacw3icdvfna9taef2rtzq6sey0x15etcatjzfcidkgsqhhfookybkzwo3ixfshdkenjdavsy/jj+q/6doxxuragyxhm3k7b2bsqgphufs7fbx4ubx9aud1+83u3n6ne/d20pnschxyi429tsghfbqhjejidwervcrxkp19weavfqj1wugftchwrobgi1xwie9nup1+fjsalkbwmzmlyfgk24sg0src5ybegx5bx8xkopuryqwvfwriepwbxvfb4woscs6xbielwwl4dg5w5keghw5crzzx4afnsja31j9n4yrdvfsgnfuo1fcqokl7mlus/8qnssrpxpxqrumo+wojvjqhmxc5hjatfjnjhqfarfbeqz4fc5z8stqhm20cb4kwzd2kpujrz6ixyy/+8p+9o+mvpvdzaryl+cptuzedcb+o5pl+i9tupcbmcfiv9l9z9fxckau6+5qqzzhdkamiovht7xk/vd5zchk8iknb/p2kd95fx3ahvwcfwj/f7jsds2/sgg0zzyw7y/fsifgazaib0gnp0fpr3rfgbpufkwdgvg== /latexit latexit sha1_base64= yckm0lgeuwguywydtskkfywzpzs= aaacw3icdvfna9taef2rtzq6sey0x15etcatjzfcidkgsqhhfookybkzwo3ixfshdkenjdavsy/jj+q/6doxxuragyxhm3k7b2bsqgphufs7fbx4ubx9aud1+83u3n6ne/d20pnschxyi429tsghfbqhjejidwervcrxkp19weavfqj1wugftchwrobgi1xwie9nup1+fjsalkbwmzmlyfgk24sg0src5ybegx5bx8xkopuryqwvfwriepwbxvfb4woscs6xbielwwl4dg5w5keghw5crzzx4afnsja31j9n4yrdvfsgnfuo1fcqokl7mlus/8qnssrpxpxqrumo+wojvjqhmxc5hjatfjnjhqfarfbeqz4fc5z8stqhm20cb4kwzd2kpujrz6ixyy/+8p+9o+mvpvdzaryl+cptuzedcb+o5pl+i9tupcbmcfiv9l9z9fxckau6+5qqzzhdkamiovht7xk/vd5zchk8iknb/p2kd95fx3ahvwcfwj/f7jsds2/sgg0zzyw7y/fsifgazaib0gnp0fpr3rfgbpufkwdgvg== /latexit latexit sha1_base64= yckm0lgeuwguywydtskkfywzpzs= aaacw3icdvfna9taef2rtzq6sey0x15etcatjzfcidkgsqhhfookybkzwo3ixfshdkenjdavsy/jj+q/6doxxuragyxhm3k7b2bsqgphufs7fbx4ubx9aud1+83u3n6ne/d20pnschxyi429tsghfbqhjejidwervcrxkp19weavfqj1wugftchwrobgi1xwie9nup1+fjsalkbwmzmlyfgk24sg0src5ybegx5bx8xkopuryqwvfwriepwbxvfb4woscs6xbielwwl4dg5w5keghw5crzzx4afnsja31j9n4yrdvfsgnfuo1fcqokl7mlus/8qnssrpxpxqrumo+wojvjqhmxc5hjatfjnjhqfarfbeqz4fc5z8stqhm20cb4kwzd2kpujrz6ixyy/+8p+9o+mvpvdzaryl+cptuzedcb+o5pl+i9tupcbmcfiv9l9z9fxckau6+5qqzzhdkamiovht7xk/vd5zchk8iknb/p2kd95fx3ahvwcfwj/f7jsds2/sgg0zzyw7y/fsifgazaib0gnp0fpr3rfgbpufkwdgvg== /latexit latexit sha1_base64= e6geauy33jiuodw/0nxupugwuko= aaacvxicdvhbattaef0rvatulukf+yjqaqeui4va+9zax/kyqp0ejgngq1g8ec9id9tych9g30rzxf2bjh1trkqdwdicmbnzzqaotqqujl970c6jx0+e7j7rp3/x8txrvf2di+aal3eknxb+qocawlkcksknv7vhmixgy2l2zzw//i4+kge/0algsyfrqyolgzjkctd1fpk5mhxp9gbjmflh/bckgzaqmzif7pd+5qwtjuflukmiwzrung7bk5ial/28cvidnme1zgwtgazjdu15gr8yu8av8/wsxwt2w9gccwfhcq40qnnwp7ci/5xlgqo+jvtl64bqyrtgvanjcvfqaxgpperscwygvwkvszycb0m8pv7hdpsgqanhvezswhxim1rs8tlf/go706x0nc9r8ybma7f9zgykeb3djf1htq0qnjsrfczk3zxylxtggc3f50qlvtboamlotor7pvev9xbcha/tzjh+prmchm0uuyveinfisktiozgvz+jcjiqutvwqv8rt9dncsef2rjtqbtrvrceimz9len+2 /latexit latexit sha1_base64= e6geauy33jiuodw/0nxupugwuko= aaacvxicdvhbattaef0rvatulukf+yjqaqeui4va+9zax/kyqp0ejgngq1g8ec9id9tych9g30rzxf2bjh1trkqdwdicmbnzzqaotqqujl970c6jx0+e7j7rp3/x8txrvf2di+aal3eknxb+qocawlkcksknv7vhmixgy2l2zzw//i4+kge/0algsyfrqyolgzjkctd1fpk5mhxp9gbjmflh/bckgzaqmzif7pd+5qwtjuflukmiwzrung7bk5ial/28cvidnme1zgwtgazjdu15gr8yu8av8/wsxwt2w9gccwfhcq40qnnwp7ci/5xlgqo+jvtl64bqyrtgvanjcvfqaxgpperscwygvwkvszycb0m8pv7hdpsgqanhvezswhxim1rs8tlf/go706x0nc9r8ybma7f9zgykeb3djf1htq0qnjsrfczk3zxylxtggc3f50qlvtboamlotor7pvev9xbcha/tzjh+prmchm0uuyveinfisktiozgvz+jcjiqutvwqv8rt9dncsef2rjtqbtrvrceimz9len+2 /latexit latexit sha1_base64= e6geauy33jiuodw/0nxupugwuko= aaacvxicdvhbattaef0rvatulukf+yjqaqeui4va+9zax/kyqp0ejgngq1g8ec9id9tych9g30rzxf2bjh1trkqdwdicmbnzzqaotqqujl970c6jx0+e7j7rp3/x8txrvf2di+aal3eknxb+qocawlkcksknv7vhmixgy2l2zzw//i4+kge/0algsyfrqyolgzjkctd1fpk5mhxp9gbjmflh/bckgzaqmzif7pd+5qwtjuflukmiwzrung7bk5ial/28cvidnme1zgwtgazjdu15gr8yu8av8/wsxwt2w9gccwfhcq40qnnwp7ci/5xlgqo+jvtl64bqyrtgvanjcvfqaxgpperscwygvwkvszycb0m8pv7hdpsgqanhvezswhxim1rs8tlf/go706x0nc9r8ybma7f9zgykeb3djf1htq0qnjsrfczk3zxylxtggc3f50qlvtboamlotor7pvev9xbcha/tzjh+prmchm0uuyveinfisktiozgvz+jcjiqutvwqv8rt9dncsef2rjtqbtrvrceimz9len+2 /latexit latexit sha1_base64= e6geauy33jiuodw/0nxupugwuko= aaacvxicdvhbattaef0rvatulukf+yjqaqeui4va+9zax/kyqp0ejgngq1g8ec9id9tych9g30rzxf2bjh1trkqdwdicmbnzzqaotqqujl970c6jx0+e7j7rp3/x8txrvf2di+aal3eknxb+qocawlkcksknv7vhmixgy2l2zzw//i4+kge/0algsyfrqyolgzjkctd1fpk5mhxp9gbjmflh/bckgzaqmzif7pd+5qwtjuflukmiwzrung7bk5ial/28cvidnme1zgwtgazjdu15gr8yu8av8/wsxwt2w9gccwfhcq40qnnwp7ci/5xlgqo+jvtl64bqyrtgvanjcvfqaxgpperscwygvwkvszycb0m8pv7hdpsgqanhvezswhxim1rs8tlf/go706x0nc9r8ybma7f9zgykeb3djf1htq0qnjsrfczk3zxylxtggc3f50qlvtboamlotor7pvev9xbcha/tzjh+prmchm0uuyveinfisktiozgvz+jcjiqutvwqv8rt9dncsef2rjtqbtrvrceimz9len+2 /latexit latexit sha1_base64= mg5btp8pva4px4lgss3gjw+5gb0= aaacvxicdvhbattaef0rbzo6l1z62bdrewilgkku0rce+tlhfookibkzwo3ixxsru6pgrvgz+lba7+rfdoyyyixtwmlhzjydmznfrvwgjpndi3yepx6yu/e0/+z5i5f7b4dhl8e1xujiou38dqebtbi4ikuar2upyaqnv8xs0yp/9q19um5+puwnywm3vlvkajgv5adrkerznuknb4nkmkwjfgjsdriitvxmdns/8tljxqalqsgele1qgrfgsumny37ebkxbzuagm4ywdizxu/a8ji+zkepkex6w4jw7rwjbhlawbvcaogm4n1ur/8pldvufx62yduno5v2jqtexuxi1glhuhixpbqoqxrhxwe7bgyreu/94u02qongjxnzprqrkgs12+ewv/47dava6mue1eevztdt+zwzsvi7ukv4j21yvzs0ichbybx65sjpjwjzvc6isk2g0ttqnwvi90/vxewgu3w/tzjh++ta4p9lcdk+8fm/eiujfqtgxn8wfgakpnpgufopf0vmeky7sxwnu22heiu5et38ayynftq== /latexit latexit sha1_base64= mg5btp8pva4px4lgss3gjw+5gb0= aaacvxicdvhbattaef0rbzo6l1z62bdrewilgkku0rce+tlhfookibkzwo3ixxsru6pgrvgz+lba7+rfdoyyyixtwmlhzjydmznfrvwgjpndi3yepx6yu/e0/+z5i5f7b4dhl8e1xujiou38dqebtbi4ikuar2upyaqnv8xs0yp/9q19um5+puwnywm3vlvkajgv5adrkerznuknb4nkmkwjfgjsdriitvxmdns/8tljxqalqsgele1qgrfgsumny37ebkxbzuagm4ywdizxu/a8ji+zkepkex6w4jw7rwjbhlawbvcaogm4n1ur/8pldvufx62yduno5v2jqtexuxi1glhuhixpbqoqxrhxwe7bgyreu/94u02qongjxnzprqrkgs12+ewv/47dava6mue1eevztdt+zwzsvi7ukv4j21yvzs0ichbybx65sjpjwjzvc6isk2g0ttqnwvi90/vxewgu3w/tzjh++ta4p9lcdk+8fm/eiujfqtgxn8wfgakpnpgufopf0vmeky7sxwnu22heiu5et38ayynftq== /latexit latexit sha1_base64= mg5btp8pva4px4lgss3gjw+5gb0= aaacvxicdvhbattaef0rbzo6l1z62bdrewilgkku0rce+tlhfookibkzwo3ixxsru6pgrvgz+lba7+rfdoyyyixtwmlhzjydmznfrvwgjpndi3yepx6yu/e0/+z5i5f7b4dhl8e1xujiou38dqebtbi4ikuar2upyaqnv8xs0yp/9q19um5+puwnywm3vlvkajgv5adrkerznuknb4nkmkwjfgjsdriitvxmdns/8tljxqalqsgele1qgrfgsumny37ebkxbzuagm4ywdizxu/a8ji+zkepkex6w4jw7rwjbhlawbvcaogm4n1ur/8pldvufx62yduno5v2jqtexuxi1glhuhixpbqoqxrhxwe7bgyreu/94u02qongjxnzprqrkgs12+ewv/47dava6mue1eevztdt+zwzsvi7ukv4j21yvzs0ichbybx65sjpjwjzvc6isk2g0ttqnwvi90/vxewgu3w/tzjh++ta4p9lcdk+8fm/eiujfqtgxn8wfgakpnpgufopf0vmeky7sxwnu22heiu5et38ayynftq== /latexit latexit sha1_base64= mg5btp8pva4px4lgss3gjw+5gb0= aaacvxicdvhbattaef0rbzo6l1z62bdrewilgkku0rce+tlhfookibkzwo3ixxsru6pgrvgz+lba7+rfdoyyyixtwmlhzjydmznfrvwgjpndi3yepx6yu/e0/+z5i5f7b4dhl8e1xujiou38dqebtbi4ikuar2upyaqnv8xs0yp/9q19um5+puwnywm3vlvkajgv5adrkerznuknb4nkmkwjfgjsdriitvxmdns/8tljxqalqsgele1qgrfgsumny37ebkxbzuagm4ywdizxu/a8ji+zkepkex6w4jw7rwjbhlawbvcaogm4n1ur/8pldvufx62yduno5v2jqtexuxi1glhuhixpbqoqxrhxwe7bgyreu/94u02qongjxnzprqrkgs12+ewv/47dava6mue1eevztdt+zwzsvi7ukv4j21yvzs0ichbybx65sjpjwjzvc6isk2g0ttqnwvi90/vxewgu3w/tzjh++ta4p9lcdk+8fm/eiujfqtgxn8wfgakpnpgufopf0vmeky7sxwnu22heiu5et38ayynftq== /latexit latexit sha1_base64= nqaqpunq6dceyonblki0fjnzob0= aaacw3icdvhbihnbeo2mtzvenqupvgyghsgazktqxwurffzb7c5kqqjpqdk06cvqxamjw3yjpuph+tdwskeyu1rqcdhvp+tuvv5pfshjfveiw7fv3l13cl//4ogjx4edoydnwdve4kq67fxfdgg1sjghrrovko9gco3n+fldjn/+fx1qzn6hdyuza5dwluocmtufhi7s1xnoagevspwap/pbmbkn24hvgnqhhmixp/oj3o+scli2aelqcggajhxngvckpma2n9ubk5blumqpqwsgw6zzom/jy2akuhsen6v4y+4rgjahre3olqzoea7nnus/ctoayvezrtmqjrtyqlfz65hcvfldxcipkvsaauiv2gssf+bbei+rf7zfjkjq6fg3xvqrhhlgi11++pd/xe40k13f81r8rqut235nblk8ju6s/ipbv+xolqlyfvjvhrlkompafi8zogjlqdu1tcjq+z7p9evdbgdvxmkytj+/hz6mdpc9em/eczesqxgntsqncsomqopafbc/xa/oy7smferxpvfvp3kqohg1fwaosec9 /latexit latexit sha1_base64= nqaqpunq6dceyonblki0fjnzob0= aaacw3icdvhbihnbeo2mtzvenqupvgyghsgazktqxwurffzb7c5kqqjpqdk06cvqxamjw3yjpuph+tdwskeyu1rqcdhvp+tuvv5pfshjfveiw7fv3l13cl//4ogjx4edoydnwdve4kq67fxfdgg1sjghrrovko9gco3n+fldjn/+fx1qzn6hdyuza5dwluocmtufhi7s1xnoagevspwap/pbmbkn24hvgnqhhmixp/oj3o+scli2aelqcggajhxngvckpma2n9ubk5blumqpqwsgw6zzom/jy2akuhsen6v4y+4rgjahre3olqzoea7nnus/ctoayvezrtmqjrtyqlfz65hcvfldxcipkvsaauiv2gssf+bbei+rf7zfjkjq6fg3xvqrhhlgi11++pd/xe40k13f81r8rqut235nblk8ju6s/ipbv+xolqlyfvjvhrlkompafi8zogjlqdu1tcjq+z7p9evdbgdvxmkytj+/hz6mdpc9em/eczesqxgntsqncsomqopafbc/xa/oy7smferxpvfvp3kqohg1fwaosec9 /latexit latexit sha1_base64= nqaqpunq6dceyonblki0fjnzob0= aaacw3icdvhbihnbeo2mtzvenqupvgyghsgazktqxwurffzb7c5kqqjpqdk06cvqxamjw3yjpuph+tdwskeyu1rqcdhvp+tuvv5pfshjfveiw7fv3l13cl//4ogjx4edoydnwdve4kq67fxfdgg1sjghrrovko9gco3n+fldjn/+fx1qzn6hdyuza5dwluocmtufhi7s1xnoagevspwap/pbmbkn24hvgnqhhmixp/oj3o+scli2aelqcggajhxngvckpma2n9ubk5blumqpqwsgw6zzom/jy2akuhsen6v4y+4rgjahre3olqzoea7nnus/ctoayvezrtmqjrtyqlfz65hcvfldxcipkvsaauiv2gssf+bbei+rf7zfjkjq6fg3xvqrhhlgi11++pd/xe40k13f81r8rqut235nblk8ju6s/ipbv+xolqlyfvjvhrlkompafi8zogjlqdu1tcjq+z7p9evdbgdvxmkytj+/hz6mdpc9em/eczesqxgntsqncsomqopafbc/xa/oy7smferxpvfvp3kqohg1fwaosec9 /latexit latexit sha1_base64= nqaqpunq6dceyonblki0fjnzob0= aaacw3icdvhbihnbeo2mtzvenqupvgyghsgazktqxwurffzb7c5kqqjpqdk06cvqxamjw3yjpuph+tdwskeyu1rqcdhvp+tuvv5pfshjfveiw7fv3l13cl//4ogjx4edoydnwdve4kq67fxfdgg1sjghrrovko9gco3n+fldjn/+fx1qzn6hdyuza5dwluocmtufhi7s1xnoagevspwap/pbmbkn24hvgnqhhmixp/oj3o+scli2aelqcggajhxngvckpma2n9ubk5blumqpqwsgw6zzom/jy2akuhsen6v4y+4rgjahre3olqzoea7nnus/ctoayvezrtmqjrtyqlfz65hcvfldxcipkvsaauiv2gssf+bbei+rf7zfjkjq6fg3xvqrhhlgi11++pd/xe40k13f81r8rqut235nblk8ju6s/ipbv+xolqlyfvjvhrlkompafi8zogjlqdu1tcjq+z7p9evdbgdvxmkytj+/hz6mdpc9em/eczesqxgntsqncsomqopafbc/xa/oy7smferxpvfvp3kqohg1fwaosec9 /latexit figure hierarchical learning-to-rank positive typepaths colored black negative type path colored gray blue line corresponds thresholdderived parent node positive type left rank negative type right able learn large margin easy pair show superiorthan single margin experiments.analogous reasoning equation margin relative ratiosbetween important simplicity,5 theontology level assignξl example give ontology level themargins level threshold equation rank positive type high negative type allchildren type give parent type rank basedon relevance entity mention whatshould threshold positive negative type could threshold approach multi-label classification problemas binary classification problem linand tune adaptive type-specificthreshold parent type zhang propose simpler method.we propose directly parent node asthe threshold positive type learn thefollowing rank relation mean rank high example mention gold type/person/artist/singer since the5 hyperparameter search margin hyperparameters find equation generalize well.8469parent type /person/artist consider kind prior type artist themodel learn positive type singer high confidence artist turn high type artist like author actor hence ranker learnthat positive subtype rank high thanits parent parent rank high thanits negative children. formulation decode time give parent type childsubtype score high beoutput positive label.we translate ranking relation equation ranking loss extend equation inequation expected margin betweenpositive type negative type since insert parent middle divide margin marginbetween positive type parent margin parent negativetypes visualization figure hyperparameter totune precision-recall tradeoff outputtingtypes small small expected margin positive type parent.this intuitively increase precision decreasesrecall confident type output versa increase decrease precision butincrease recall.therefore learn rank relationsfrom equation positive type bescored parent parent bescored negative sibling type positive type score abovenegative sibling type final hierarchicalranking loss formulate follows.jy αξlev +jȳ =∑y′∈sb ξlev =∑y′∈sb ξlev +jhier =∑y∈y decodingpredicting type entity mention beperformed iterative search type tree root entity node coarser type thento finer-grained type ensure outputdoes violate hierarchical property i.e. asubtype output parent must output.algorithm decoding hierarchical typing1 function hiertypedec entity queue searching3 output types4 repeat5 dequeue δlev threshold value7 decode child types8 topk klev prune branch factors9 do11 enqueue for13 queue empty14 return return decoded types15 functiongiven instance compute score type search process startswith root node entity type tree thequeue type node child nodez subtypes predict type setif correspond rankingrelation equation model learned.6here take top-k element tothe queue prevent over-generating types.this also enforce single-pathproperty dataset singlepath level type hierarchy welimit branching factor allow child beki algorithm list algorithm wherethe function topk select top-k element respect function subtyping relation constrainteach type ontology assigneda type embed notice binary subtyping relation thetypes trouillon propose relation embed method complex work wellwith anti-symmetric transitive relation suchas subtyping employ before6 ontonotes dataset introduce another setof per-level hyperparameters δlev threshold valuef modify δlev akin adaptivethreshold zhang large typedistribution mismatch training dev/test setsin ontonotes dev/test instance thesingle type /other training otherdatasets unused murty complex theloss regulate type embeddings complexoperates complex space natural isomorphism real complex spacesto type embed complex space first half embed vector real part second half imaginary part learn single relation embed /2for subtyping relation given type thesubtyping statement model thefollowing score function element-wise product complex conjugate andvice versa z.loss given instance positivetype learn following relation translating relation constraint binaryclassification problem subtype primal hinge loss jrel =∑y∈y ++∑y′∈sb different murty wherea binary cross-entropy loss randomly sample pair experiment show thatthe loss equation performs good thecross-entropy version structure thetraining pair sibling sibling parent negative sample type closer tothe positive parent type hence train withmore competitive negative samples.4.6 training validationour final loss combination hierarchicalranking loss subtyping relation constraintloss regularization jhier βjrel ‖θ‖22 adamw optimizer loshchilov hutter,2019 train model shownto superior original adam l2regularization hyperparameters ratio margin above/below threshold weight subtyping relation constraint regularizationcoefficient tuned.at validation time tune maximumbranching factor level parameter tune trade-off theprecision recall layer preventsover-generation observe case hyperparameters tune modelsachieve maximum micro score subsection experiments5.1 datasetsaida aida phase practice dataset forhierarchical entity type comprises document ldc2019e04 ldc2019e07 andthe evaluation dataset ldc2019e42 /ldc2019e77 take english part ofthe data practice dataset train/dev evaluation dataset test practicedataset comprises domain label r103 r105 r107 since evaluation dataset isout-of-domain small domain r105as remain r103 r107 astrain.the aida entity dataset ontology term type subtype subsubtype mentioncan label level hence thedataset single-path thus branching factor three layer weischedel brunstein labeleda portion million word penn treebankcorpus wall street journal text ldc95t7 two-level hierarchy result bbnpronoun coreference entity type corpus wefollow train/test split andfollow train/dev split zhang gillick sample sentence ontonotes corpus annotate entity type follow thetrain/dev/test data split shimaoka ontonotes dataset also include perlevel threshold δlev train test levels types multi-path laida single-path multi-path ontonotes multi-path figer multi-path table statistics various datasets corresponding hyperparameter settings.figer ling weld sample datasetfrom wikipdia article news report entitymentions text derived freebase bollacker al.,2008 follow data split shimaoka statistic datasets accompanying ontology list table togetherwith respective hyperparameters.85.2 setupto best compare recent prior work followlin elmo encoding ofwords update layer elmo output initial embed hasdimension type embed dimensionality initiallearning rate batch size choice tune list table employ early stopping choose model yield best micro score sets.our model implement allennlp gardner implementation forsubtyping relation constraint openke hanet baselineswe compare approach major prior work infet capable multi-path entity typing.9for aida since prior work thisdataset knowledge also implementedmulti-label classification binary classifiermodels similar baseline mention feature extractor result areshown table multi-label ontonotes dataset additional hyperparameters per-level threshold δ1,2,3 zhang include document-level information best results—for fair comparison theirresults without document context report ablation tests.5.4 metricswe follow prior work strict accuracy macro micro score given instance denote gold typeset predicted type strictaccuracy ratio instance ŷi.macro average score betweenyi instance whereas micro countstotal true positive false negative false positive globally.we also investigate per-level accuracy onaida accuracy level ratio instance whose predicted type gold type setare identical level type output atlevel append create dummytype level /person/other/other.hence accuracy last level aida level equal strict accuracy.5.5 results discussionsall result condition regard partial type path exclusive undefined.the result aida dataset show table model exclusive case outperform amulti-label classification baseline metrics.of type specify aida ontology train/dev cover type thetest cover type types.we could perform zero-shot entity typing initialize type embed type name /fac/structure/plaza together withits description open urban public space city square designate dataannotation manual leave future work.approach mifours exclusive undefined subtyping constraint margin results aida dataset.8472approach ontonotes figeracc mifling weld exclusive undefined subtyping constraint margin specific dataset strictly comparable non-standard much large training result document-level context information hence comparable.table results common datasets ontonotes figer numbers italic result obtainedwith various augmentation technique either large data large context hence directly comparable.results ontonotes figercan find table across datasets ourmethod produce state-of-the-art performanceon strict accuracy micro score stateof-the-art comparable ±0.5 performance onmacro score compare prior model especially method improve upon strict accuracy substantially across datasets show decoderare well output exact correct type sets.partial type path exclusive undefined interestingly find aida figer partial type path good consider asexclusive whereas ontonotes consider undefined lead good performance hypothesize come fromhow data annotatated—the annotation manual contain directive whether interpretpartial type path exclusive undefined thedata non-exhaustively annotate leadingto undefined partial type advocate careful investigation partial type path futureexperiments data curation.ablation studies compare best modelwith various component model remove study gain component fromthe best setting exclusive undefined report performance removingthe subtyping constraint describe subsection substitute multi-level marginsin equation flat margin i.e. marginson level result areshown table table best result show multi-level marginsand subtyping relation constraint offer orthogonalimprovements models.error analysis identify common pattern oferrors couple typical example confusing type model output /gpe/city gold type is/location/region shipmentsfrom valley either hardware softwaregoods. type semantically similar model fail discriminate betweenthese types.• incomplete type figer give instance multi-agency investigation head theu.s immigration customs enforcement shomeland security investigation unit thegold type /government agency and/organization model fail output /organization.• focusing part mention inaida give instance suggest werethe work russian special force assassins8473out blacken image kievs prowestern authority model outputs/org/government whereas gold type is/per/militarypersonnel modelfocused russian special force part ignore assassins part better mention representation require correct possibly introduce type-aware mentionrepresentation—we leave future work.6 conclusionswe propose novel multi-level learn torank loss function operate type tree accompany coarse-to-fine decoderto fully embrace ontological structure thetypes hierarchical entity typing approachachieved state-of-the-art performance across various datasets make substantial improvement upon strict accuracy.additionally advocate careful investigation partial type path interpretation relies data annotate turn influence type performance.acknowledgementswe thank colleague guanghui theanonymous reviewer insightful suggestion comment research benefit fromsupport human language technology center excellence hltcoe darpaaida u.s. government authorize reproduce distribute reprint governmentalpurposes view conclusion contain inthis publication author shouldnot interpret represent official policiesor endorsement darpa u.s. government
hierarchical text classification essentialyet challenge subtask multi-label text classification taxonomic hierarchy existingmethods difficulty model hierarchical label structure global view furthermore make full mutual interaction text feature spaceand label space paper formulate hierarchy directed graph andintroduce hierarchy-aware structure encodersfor model label dependency based onthe hierarchy encoder propose novelend-to-end hierarchy-aware global model hiagm variant multi-label attention variant hiagm-la learn hierarchyaware label embeddings hierarchyencoder conduct inductive fusion labelaware text feature text feature propagation model hiagm-tp propose deductive variant directly feed text featuresinto hierarchy encoders compared previous work hiagm-la hiagmtp achieve significant consistent improvement three benchmark datasets.1 introductiontext classification widely natural language processing application sentimental analysis pang information retrieval document categorization yang hierarchical textclassification particular multi-label textclassification problem classification result correspond node ataxonomic hierarchy taxonomic hierarchy iscommonly model tree directed acyclicgraph depict figure approach could categorize group local approach global∗this work intern alibaba group.†corresponding author.figure short sample news sport football feature book note could beeither single-path multi-path problem.approach first group tend construct multiple classification model traverse thehierarchy top-down manner previous localstudies wehrmann shimura al.,2018 banerjee propose overcomethe data imbalance child node learn fromparent however model contain largenumber parameter easily lead exposurebias lack holistic structural information.the global approach treat problem flatmlc problem single classifier forall class recent global method introduce various strategy utilize structural information oftop-down path recursive regularization gopal yang reinforcement learning meta-learning al.,2019 global method encode holistic label structure label correlation feature moreover method still exploitthe hierarchy shallow manner thus ignoringthe fine-grained label correlation information thathas prove fruitful work.in paper formulate hierarchy adirected graph utilize prior probability label dependency aggregate node information.a hierarchy-aware global model hiagm pro1107posed enhance textual information labelstructural feature comprise traditional textencoder extract textual information ahierarchy-aware structure encoder modelinghierarchical label relation hierarchy-awarestructure encoder could either treelstm hierarchy-gcn hierarchical prior knowledge integrate moreover structureencoders bidirectionally calculate allowingthem capture label correlation information inboth top-down bottom-up manner result hiagm robust previous top-downmodels able alleviate problem causedby exposure bias imbalanced data.to aggregate text feature label structuralfeatures present variant hiagm amulti-label attention model hiagm-la textfeature propagation model hiagm-tp variant extract hierarchy-aware text feature base onthe structure encoders hiagm-la extract inductive label-wise text feature hiagm-tpgenerates hybrid information deductive manner.specifically hiagm-la update label embed across holistic hierarchy employsnode output hierarchy-aware label representation finally conduct multi-label attentionfor label-aware text feature hand hiagm-tp directly utilize text feature theinput structure encoder serial dataflow.hence propagate textual information throughoutthe overall hierarchy hidden state nodein entire hierarchy represent class-specifictextual information.the major contribution paper prior hierarchy knowledge adopttypical structure encoders model labeldependencies top-down bottomup manner investigatedfor hierarchical text classification.• propose novel end-to-end hierarchyaware global model hiagm furtherpresent variant label-wise text feature hierarchy-aware multi-label attentionmodel hiagm-la hierarchy-awaretext feature propagation model hiagm-tp empirically demonstrate variantsof hiagm achieve consistent improvementson various datasets different structure encoders best model outperformsthe state-of-the-art model macrof1 micro-f1 rcv1-v2.• release code experimental splitsof web-of-science nytimes reproducibility related workexisting work could categorize intolocal global approach local approachescould subdivide local classifier node banerjee local classifier perparent node lcpn dumais chen local classifier level shimura al.,2018 wehrmann kowsari transfer parameter theparent model child model wehrmannet alleviate exposure bias problem bythe hybrid global optimization penget decompose hierarchy subgraphs conduct text-gcn n-gram tokens.the global approach improve flat model hierarchy information hofmann modify hierarchical-svmby decomposition gopal yang propose simple recursive regularization parameter among adjacent class deep learning architecture also employ global model suchas sequence-to-sequence yang metalearning reinforcement learning capsule network penget model mainly focus improve decoder base constraint hierarchical path contrast propose effective hierarchy-aware global model hiagm thatextracts label-wise text feature hierarchy encoders base prior hierarchy information.moreover attention mechanism introducedin mullenbach coding rios kavuluru train label representation basic graphcnn conductsmutli-label attention residual shortcut attentionxml convert amulti-label attention model label clusters.huang improve hmcn wehrmannet label attention level ourhiagm-la however employ multi-label attention single model simplified structureencoder reduce computational complexity.recent work semantic analysis chen al.,2017b semantic role labeling andmachine translation chen show theimprovement sentence representation syntax1https //github.com/alibaba-nlp/hiagm1108figure example taxonomic hierarchy thenumber indicate prior probability label dependency accord training corpus.encoder tree-based chen graphcnn marcheggianiand titov modify structure encoders fine-grained prior knowledgein top-down bottom-up manners.3 problem definitionhierarchical text classification subtask oftext classification organize label space apredefined taxonomic hierarchy hierarchy ispredefined base holistic corpus hierarchygroups label subset accord class relations.the taxonomic hierarchy mainly contain treelike structure direct acyclic graph structure note convert intoa tree-like structure distinguish labelnode single-path node thus taxonomichierarchy simplify tree-like structure.as illustrate figure formulate ataxonomic hierarchy directed graph refer label nodesv denote number label nodes.−→e ∈child top-down hierarchy path and←−e child bottomup hierarchy path formally define sequence text objectsx aligned sequenceof supervise label depict figure sample correspond label include multipleclasses corresponding class belong toeither sub-paths hierarchy.note sample belongs parent nodevi condition pertain child nodevj child hierarchy-aware global modelas depict figure propose hierarchyaware global model hiagm leverage thefine-grained hierarchy information aggregate label-wise text feature hiagm consistsof traditional text encoder textual information hierarchy-aware structure encoder forhierarchical label correlation features.we present variant hiagm hybridinformation aggregation multi-label attentionmodel hiagm-la text feature propagation model hiagm-tp hiagm-la update label representation structure encoder andgenerates label-aware text feature multi-labelattention mechanism hiagm-tp propagate textrepresentations throughout holistic hierarchy thus obtain label-wise text feature fusion label correlations.4.1 prior hierarchy informationthe taxonomic hierarchy describe hierarchicalrelations among label major bottleneck ofhtc make full establishedstructure previous study directly utilize hierarchy path static method base pipelineframework hierarchical model label assignmentmodel contrast base bayesian statistical inference hiagm leverage prior knowledge oflabel correlation regard predefined hierarchy corpus exploit prior probability oflabel dependency prior hierarchy knowledge.suppose hierarchy path parent node child node thisedge feature represent priorprobability ui|uj =njni ui|uj mean occurrence andp conditional probability giventhat occurs probability occur simultaneously refers thenumber training subset note thehierarchy ensure give vchild occurs.we rescale normalize prior probability ofchild node vchild total overall structure hierarchy-aware global model hiagm consist text encoder ahierarchy-aware encoder dataflows structure encoders illustrate grey dash variant hiagm-la hiagm-tp present black dashed respectively.4.2 hierarchy-aware structure encodertree-lstm graph convolutional neural network widely structure encodersfor aggregate node information al.,2015 chen rios andkavuluru depict figure hiagmmodels fine-grained hierarchy information basedon hierarchy-aware structure encoder based onthe prior hierarchy information improve typicalstructure encoders directed hierarchy graph.specifically top-down dataflow employ theprior hierarchy information =njniwhilethe bottom-up adopt tree-lstm tree-lstm could beutilized structure encoder implementation tree-lstm similar syntax encoders zhang al.,2018 predefined hierarchy identical toall sample allow mini-batch trainingmethod recursive computational module.the node transformation follow tanh +∑jfk tanh represent hidden state andmemory cell state node respectively.to induce label correlation hiagm employ abidirectional tree-lstm fusion childsum top-down module h̃↑k =∑j∈child h̃↓k hbik separately calculate inthe bottom-up top-down manner =treelstm indicate concatenation ofhidden state final hidden state node isthe hierarchical node representation hbik kipf welling propose enhance node representation basedon local graph structural information somenlp study improve text-gcns richword representation upon syntactic structure word correlation marcheggiani titov,2017 vashishth penget introduce simple hierarchy-gcnfor hierarchy structure thus gain aforementioned fine-grained hierarchy information.hierarchy-gcn aggregate dataflows within thetop-down bottom-up self-loop edge inthe hierarchy graph direct edge represent pair-wise label correlation feature thus dataflows conduct node transformation edge-wise linear transformation however edge-wise transformation shall lead overparameterized edge-wise weight matrix ourhierarchy-gcn simplifies transformation witha weight adjacent matrix weighted adjacent1110matrix represent hierarchical prior probability.formally hierarchy-gcn encode hidden stateof node base associate neighbourhoodn child parent relu ∑j∈n rdim rn×dim indicate hierarchical directionfrom node node include top-down bottomup self-loop edge note denote hierarchy probability wherethe self-loop edge employ top-downedges nknj bottom-up edgesuse holistic edge feature matrix a0,0 a0,1 ac−1 indicate theweighted adjacent matrix directed hierarchygraph finally output hidden state nodek denote label representation correspond tothe hierarchy structural information.4.3 hybrid information aggregationprevious global model classify label upon theoriginal textual information improve decoder predefined hierarchy path contrast construct novel end-to-end hierarchy-awareglobal model hiagm mutual interactionof text feature label correlation combinesa traditional text classification model hierarchy encoder thus obtain label-wise text feature hiagm extend variant parallel model inductive fusion hiagm-la anda serial model deductive fusion hiagm-tp document thesequence token embedding firstly feed intoa bidirectional layer extract text contextual feature multiple cnns forgenerating n-gram feature concatenation ofn-gram feature filter top-k max-poolinglayer extract information finally reshape obtain continuous text representation dcindicates output dimension layer.n refers multiplication top-knumber number cnns.hierarchy-aware multi-label attention thefirst variant hiagm propose base multilabel attention call hiagm-la attentionmechanism usually utilize memory unitin text classification yang al.,2019 recent study huang construct multi-label attentionbased model level avoid optimizinglabel embed among different levels.our hiagm-la similar baselinesbut simplifies multi-label attention modelsto global model based hierarchy encoders hiagm-la could overcome problemof convergence label embed across various level label representation enhancedwith bidirectional hierarchical information thislocal structural information make feasible tolearn label feature across different level single model formally suppose trainablelabel embedding node randomly initialize initial label embeddinglk directly feed structure encoders theinput vector aligned label node theoutput hidden state rc×dc represent thehierarchy-aware label feature given text representation rn×dc hiagm-la calculate thelabel-wise attention value =esj htk∑nj=1 =n∑i=1αki note indicate informative text feature vector k-th label wecan inductive label-aligned text featuresv rc×dc base multi-label attention thenit would feed classifier prediction.furthermore could directly hidden stateof hierarchy encoders pretrained label representation hiagm-la could even lighterin inference process.hierarchical text feature propagation graphneural network capable message passing gilmer duvenaud learn local node correlation overall graphstructure avoid noise heterogeneousfusion second variant obtain label-wise textfeatures base deductive method directlytakes text feature node input updatestextual information hierarchy-awarestructure encoder variant mainly conduct thepropagation text feature call hiagm-tp.formally node input reshape textfeatures single linear transformation trainable weight matrix n×dc c×dv transform text feature ∈rn×dc node input rc×dv predefined structure samplewould update textual information throughout thesame holistic taxonomic hierarchy mini-batchlearning manner initial node representation feed hierarchy encoder output hiddenstate denote deductive hierarchy-aware text feature input final classifier comparedwith hiagm-la transformation hiagmtp conduct textual information withoutthe fusion label embedding thus structureencoder would activate training andinference procedure pass textual messagesacross hierarchy could converge much easierbut slightly high computational complexitythan hiagm-la.4.4 classificationwe flatten hierarchy take node asleaf node multi-label classification matter leaf node internal node finalhierarchy-aware feature feed fully connect layer prediction hiagm complementary recursive regularization gopal andyang =∑i∈c∑j∈child ||wi parameter final fully connect layer multi-label classification hiagmuses binary cross-entropy loss function =−∑ni=1∑cj=1 yijlog y′ij y′ij ground truth sigmoidscore j-th label i-th sample thus thefinal loss function lr.5 experimentin section introduce experiment withdatasets evaluation metric implementation detail comparison ablation study analysis ofexperimental results.5.1 experiment setupwe experiment propose architecture rcv1v2 web-of-science nytimes datasets comparison ablation study.datasets rcv1-v2 lewis andnyt sandhaus news categorization corpus kowsari include abstract publish paper ofscience typical text classification datasetsdataset depth |li| train testrcv1 data statistics number classes.avg |li| average number class sample.depth indicate maximum level hierarchy.are annotate ground truth hierarchical taxonomic label benchmark splitof rcv1-v2 select small partial training subset validation dataset randomly splittedinto training validation test subset randomly select split subset originalraw data also remove sample labelor single one-level label note wosis single-path rcv1-v2include multi-path taxonomic statisticsof datasets show table metrics measure experimental result standard evaluation metric gopaland yang include micro-f1 macrof1 micro-f1 take overall precision recallof instance account macro-f1equals average f1-score label microf1 give weight frequent label whilemacro-f1 equally weight labels.implementation details one-layer bigru hidden unit parallel layer filter region size vocabulary create frequent word themaximum size word embed glove2 pennington randomly initialize out-ofvocabulary word minimum count information pertain text classificationcould extract beginning statements.thus maximum length token inputsas fixed threshold choose as0.5 dropout employ embedding layerand layer rate thebi-gru layer node transformation therate respectively additionally forhiagm-la label embedding initialize bykaiming uniform othermodel parameter initialize xavier uniform glorot bengio adam optimizer mini-batch size learn rate2https //nlp.stanford.edu/projects/glove1112model micro macrolocal modelshr-dgcnn-3 peng shimura banerjee modelssgm yang peng comparison previous model rcv1-v2.note prior probability matrix hiagm-tpis fine-tuned train hiagmla denote train without recursiveregularization indicate statistically significant difference textrcnn textrcnn+labelattention respectively.α momentum parameter penalty coefficient recursive regularization model evaluate test subset bestmodel validation subset.5.2 comparisonin table compare performance hiagm traditional model state-ofthe-art study rcv1-v2 recursive regularization last layer thoseconventional text classification model also obtaincompetitive performance propose architecture hiagm-la hiagm-tp outperform state-of-the-art result global andlocal study esspecially macro-f1 showsthe strong advancement hierarchy encoderson hiagm-la achieve performanceof macro-f1 score microf1 score hiagm-tp obtain best performance macro-f1 score micro-f1 score.to clarify improvement proposed4the result reproduce benchmark split upon thereleased project sgm.modelhiagm-la hiagm-tpmicro macro time micro macro timetreelstm comparison hiagm variant rcv1v2 fixed prior probability note time denotesthe time cost epoch inference comparedto treelstm-based hiagm-la statistically significant difference compare best one.hiagm also experiment without recursiveregularization compared state-of-theart recent work hilap ourhiagm-la hiagm-tp without recursive regularization also achieve competitive improvementby term macro-f1 itdemonstrates recursive regularization iscomplementary necessary propose architecture.according table hiagm achieve consistent improvement performance htcamong rcv1-v2 datasets indicate strong improvement label-wise textfeature task result present ourproposed global model hiagm advancedcapability enhance text feature htc.all hiagm strongly improve performance benchmark dataset rcv1-v2 andthe classical text classification datasets.especially obtain good result macro-f1score indicate hiagm strong abilityto tackle data-sparse class deep hierarchy.5.3 analysishybrid information aggregation accordingto table variant outperform baselinemodels previous study denote theenhanced text feature beneficial weclarify ablation study variant structure encoders table hiagm-la andhiagm-tp train fixed prior probability help recursive computationprocess bidirectional tree-lstm achieves well performance learn hierarchy-aware label embedding however additionally lead tolower computational efficiency compare tohierarchy-gcn regarding hiagm-tp hierarchygcn show good performance efficiencythan bidirectional tree-lstm.these variant various advantage respectively specific hiagm-tp hasbetter performance hiagm-la bi1113modelrcv1-v2 rcv1-v2-r nytmicro-f1 macro-f1 micro-f1 macro-f1 micro-f1 macro-f1 micro-f1 macro-f1global text classification baselinetextrnn experimental result propose hiagm-la hiagm-tp various datasets note rcv1v2-r refers version transpose original subset train test model train withthe constraint recursive regularization hiagm-la train fixed prior probability hiagm-tp istrained trainable one.treelstm hierarchy-gcn encoders themulti-label attention variant hiagm-la wouldsomehow induce noise randomly initialize label embedding otherwise hiagm-tp aggregate fusion local structural informationand text feature without negative impactof label embedding.as efficiency hiagm-la computationally efficient hiagm-tp especially theinference process label representation fromhierarchy encoders could utilize pretrainedlabel embed multi-label attention duringinference thus hiagm-la omit hierarchyaware structure encoder module training.we recommend hiagm-tp high performance also suggest hiagm-la empirically good performance fast inference.gcn layers impact layer alsoan important issue hiagm illustrated infigure one-layer structure encoder consistently perform best hiagm-la andhiagm-tp indicate correlation non-adjacent node essential htcbut somehow noisy hierarchical information aggregation empirical conclusion consistentwith implementation recursive regularization peng gopal yang transfer learning banerjee shimura al.,2018 adjacent label levels.prior probability according aforementioned comparison simplified structure encoders prior probability undoubtedly beneficial also investigate different choicesof prior probability hierarchy-gcn encoderon hiagm-tp variant clarify table weighted adjacent matrix initialize prior probabilities.the simple weight adjacent matrix performsbetter complex edge-wise weight matrixfor node transformation weighted adjacent matrix also achieve good result theoriginal unweighted adjacent matrix trainable randomly initialize demonstrate thatthe prior probability hierarchy capable ofrepresenting hierarchical label dependency furthermore best result obtain settingthat obey calculate direction prior probability compare result fixedadjacent matrix trainable find thatthe weighted adjacent matrix could finetunedfor high flexibility good performance.in table setting allow interacfigure ablation study depth gcn.1114top-down bottom-upfixed trainablemicro macro micro macroedge-wise matrix initialized initialized∗ ablation study fine-grained hierarchy information rcv1-v2 base gcn-based hiagmtp edge-wise matrix denote directionaledge distinct trainable weight matrix transformation others weighted adjacentmatrix =njniand allow information propagation nodeswhile others obey constraint hierarchy.tions perform others allowpropagation throughout hierarchy path asanalyzed layer interaction betweennon-adjacent node would lead negative impacton also validate conclusion basedon ablation study prior probability.performance study analyze improvement performance divide label basedon level compute level-based microf1 score baseline hiagm-la andhiagm-tp figure show model retaina good performance baseline level especially among deep levels.figure evaluation label among different levels.note observe similar result datasetsand omit clean view.6 conclusionin paper propose novel end-to-endhierarchy-aware global model extract labelstructural information aggregate label-wisetext feature present bidirectional treelstm hierarchy-gcn hierarchy-awarestructure encoder furthermore frameworkis extend parallel variant base multilabel attention serial variant text featurepropagation approach empirically achievesignificant consistent improvement threedistinct datasets especially low-frequencylabels specifically variant outperform thestate-of-the-art model rcv1-v2 benchmarkdataset best model obtain macro-f1score micro-f1 score thank anonymous reviewer theirvaluable suggestion research work support national natural science foundationof china grant no.61772337 u1736207
legal artificial intelligence legalai focuseson apply technology artificial intelligence especially natural language processing benefit task legal domain recentyears legalai draw increase attentionrapidly researcher legal professional legalai beneficial legalsystem liberate legal professional froma maze paperwork legal professional often think solve task rulebased symbol-based method nlpresearchers concentrate data-drivenand embed method paper describe history current state future direction research legalai illustrate task perspective legalprofessionals researcher showseveral representative application legalai.we conduct experiment provide indepth analysis advantage disadvantage exist work explore possible future direction find implementation work http //github.com/thunlp/claim.1 introductionlegal artificial intelligence legalai mainly focus apply artificial intelligence technologyto help legal task majority resourcesin field present text form asjudgment document contract legal opinions.therefore legalai task base naturallanguage processing technologies.legalai play significant role legal domain reduce heavy redundant workfor legal professional many task legal domain require expertise legal practitionersand thorough understanding various legal document retrieving understanding legal document take time even legal professionals.∗corresponding author.therefore qualified system legalai shouldreduce time consumption tedious jobsand benefit legal system besides legalai canalso provide reliable reference arenot familiar legal domain serve anaffordable form legal aid.in order promote development legalai many researcher devote considerable effortsover past decade early work kort ulmer nagel segal gardner,1984 always hand-crafted rule feature dueto computational limitation time recentyears rapid development deep learning researcher begin apply deep learning techniquesto legalai several legalai datasets havebeen propose kano xiao duan chalkidis whichcan serve benchmark research field.based datasets researcher begin explore nlp-based solution variety legalaitasks legal judgment prediction aletraset zhong chen court view generation yeet legal entity recognition classification cardellino angelidis al.,2018 legal question answering monroy al.,2009 taniguchi kano goebel,2017 legal summarization hachey grover,2006 bhattacharya previously mention researcher effortsover year lead tremendous advance inlegalai summarize effort concentrate symbol-based method apply interpretable hand-crafted symbol legal task ashley surden meanwhile effortswith embedding-based method designingefficient neural model achieve good performance chalkidis kampas specifically symbol-based method concentrate onutilizing interpretable legal knowledge reason5219embedding-basedmethodssymbol-basedmethodsapplicationsof legalaiconceptembeddingpretrainedlanguagemodelrelationextractioneventtimelineelementdetectionjudgmentpredictionsimilar casematchingquestionansweringtextsummarizationconceptknowledgegraphintentional harmarrestedalarmescapehomicidesomeone someone hurt hurt accident alice marry andhave david unexpectedly…… alice marry david alice david lawcommon alsoknown judicialprecedent judgemade ……knowledgeguideddatadrivenfigure overview task legalai.between symbol legal document like eventsand relationship meanwhile embedding-basedmethods learn latent feature predictionfrom large-scale data difference betweenthese method cause problem inexisting work legalai interpretable symbolicmodels effective embedding-methodswith good performance usually interpret bring ethical issue legalsystem gender bias racial discrimination shortcoming make difficult applyexisting method real-world legal systems.we summarize three primary challenge bothembedding-based symbol-based method inlegalai knowledge modelling legal textsare well formalize many domainknowledge concept legalai utilize legal knowledge great significance legal reasoning although task nlprequire reasoning legalai task somehowdifferent legal reasoning must strictly followthe rule well-defined thus combine predefined rule technology essential legalreasoning besides complex case scenario andcomplex legal provision require sophisticated reasoning analyze interpretability.decisions make legalai usually interpretable apply real legal system.otherwise fairness risk compromised.interpretability important performance inlegalai.the main contribution work conclude follow describe exist worksfrom perspective researcher andlegal professional moreover illustrate several embedding-based symbol-based methodsand explore future direction legalai describe three typical application includingjudgment prediction similar case matching andlegal question answer detail emphasizewhy kind method essential tolegalai conduct exhaustive experimentson multiple datasets explore utilize nlptechnology legal knowledge overcome thechallenges legalai find implementation github1 summarize legalaidatasets regard benchmarkfor related task detail datasets canbe find github2 several legal papersworth reading.2 embedding-based methodsfirst describe embedding-based method inlegalai also name representation learning.embedding-based method emphasize represent legal fact knowledge embeddingspace utilize deep learning methodsfor correspond tasks.2.1 character word concept embeddingscharacter word embeddings play significantrole embed discrete text into1https //github.com/thunlp/claim2https //github.com/thunlp/legalpapers5220continuous vector space many embed method prove effective mikolov al.,2013 joulin pennington peters yang bordes al.,2013 crucial theeffectiveness downstream tasks.in legalai embed method also essential bridge texts andvectors however seem impossible learn themeaning professional term directly somelegal factual description existing work chalkidisand kampas mainly revolvearound apply exist embed method likeword2vec legal domain corpus overcomethe difficulty learn professional vocabularyrepresentations capture grammatical information legal knowledge wordembedding correspond task knowledgemodelling significant legalai many result decide accord legal rule andknowledge.although knowledge graph method legal domain promise still majorchallenges practical usage firstly theconstruction knowledge graph legalaiis complicate scenario noready-made legal knowledge graph available soresearchers need build scratch addition different legal concept different representation meaning legal system indifferent country also make challenge construct general legal knowledge graph.some researcher embed legal dictionary cvrček regardedas alternative method secondly generalizedlegal knowledge graph different form withthose commonly existing knowledgegraphs concern relationship entitiesand concept legalai focus theexplanation legal concept challenge make knowledge modelling embeddingin legalai non-trivial researcher toovercome challenge future.2.2 pretrained language modelspretrained language model plms asbert devlin recentfocus many field radford yang given thesuccess legalai also avery reasonable direct choice however thereare difference text existingplms legal text also lead unsatisfactory performance directly apply plmsto legal task difference stem terminology knowledge involve legal text toaddress issue zhong propose alanguage model pretrained chinese legal document include civil criminal case documents.legal domain-specific plms provide qualified baseline system task legalai wewill show several experiment compare differentbert model legalai tasks.for future exploration plms legalai researcher integrate knowledgeinto plms integrating knowledge pretrainedmodels help reason ability legal concept lots work integrate knowledge general domain intomodels zhang peters hayashi technology also beconsidered future application legalai.3 symbol-based methodsin section describe symbol-based method also name structured prediction methods.symbol-based method involve utilizinglegal domain symbol knowledge tasksof legalai symbolic legal knowledge asevents relationship provide interpretability deep learn method employ forsymbol-based method good performance.3.1 information extractioninformation extraction widely study emphasize extract valuableinformation text many nlpworks concentrate include nameentity recognition lample kuru al.,2016 akbik relation extraction zenget miwa bansal al.,2016 christopoulou event extraction chen nguyen nguyen grishman legalai also attract interest ofmany researcher make good particularity legal text researcher ontology bruckschen cardellino al.,2017 lenci zhang orglobal consistency namedentity recognition legalai extract relationship event legal document re5221searchers attempt apply different technology include hand-crafted rule bartolini al.,2004 truyens eecke vacek andschilder joint model like vacek scale-free identifiernetwork promise results.existing work make effort improve effect need moreattention benefit extracted information extracted symbol legal basis andcan provide interpretability legal application performance method show example utilize theextracted symbol interpretability legalai relation extraction inheritance dispute.inheritance dispute type case civil lawthat focus distribution inheritance rights.therefore identify relationship theparties vital close relationship decease assets.towards goal relation extraction inheritancedispute case provide reason judgmentresults improve performance.event timeline extraction judgmentprediction criminal case criminal case multiple party often involve group crimes.to decide primarily responsible forthe crime need determine everyone hasdone throughout case order theseevents also essential example case ofcrowd fight person fight first shouldbear primary responsibility result qualified event timeline extraction model require forjudgment prediction criminal cases.in future research need concern moreabout apply extract information tasksof legalai utilization informationdepends requirement specific task andthe information provide interpretability.3.2 legal element extractionin addition common symbol general legalai also exclusive symbol name legal element extraction legal element focus extract crucial element likewhether someone kill something stolen.these element call constitutive element ofcrime directly convict offender basedon result element utilizing theseelements bring intermediate supervision information judgment prediction taskbut also make prediction result modelmore interpretable.fact description fake reason formarriage decoration borrow alice afterarrested money back alice.whether sell something ×whether make fictional fact xwhether illegally possess property ofothers xjudgment results fraud.table example element detection zhonget example theextracted element decide judgment result itshows element useful downstream tasks.towards in-depth analysis elementbased symbol propose datasetfor extract element three different kindsof case include divorce dispute labor dispute loan dispute dataset require detectwhether related element satisfy formalize task multi-label classificationproblem show performance existingmethods element extraction conductedexperiments dataset result befound table labor loanmodel maftextcnn experimental result extract elements.here denote micro-f1 macro-f1.we implement several classical encode model element extraction include textcnn dpcnn johnson zhang lstm hochreiter andschmidhuber bidaf bert devlin triedtwo different version pretrained parameter ofbert include origin parameter bert andthe parameter pretrained chinese legal document bert-ms zhong fromthe result language modelpretrained general domain performs worse5222than domain-specific prove necessity legalai following partsof paper bert pretrained legaldocuments good performance.from result element extraction canfind exist method reach promisingperformance element extraction still notsufficient correspond application element regard pre-defined legal knowledge help downstream task toimprove performance element extraction isvaluable research.4 applications legalaiin section describe several typical application legalai include legal judgmentprediction similar case matching legal question answering legal judgment prediction andsimilar case matching regard corefunction judgment civil commonlaw system legal question answering canprovide consultancy unfamiliarwith legal domain therefore explore thesethree task cover aspect legalai.4.1 legal judgment predictionlegal judgment prediction mostcritical task legalai especially civillaw system civil system judgmentresults decide accord fact thestatutory article receive legal sanctionsonly violate prohibit actsprescribed task mainly concernshow predict judgment result thefact description case content thestatutory article civil system.as result essential representative task country civil system likefrance germany japan china besides ljphas draw attention artificial intelligence researcher legal professional thefollowing part describe research progressand explore future direction ljp.related workljp long history early work revolve aroundanalyzing exist legal case specific circumstance mathematical statistical method kort ulmer nagel keown,1980 segal lauderdale clark combination mathematical method legal rule make predicted result interpretable.fact description defendant stole cash8500 yuan t-shirts jacket pant shoe identify total value yuan beijing lining store.judgment resultsrelevant articles article criminal law.applicable charges theft.term penalty months.table example legal judgment prediction fromzhong example judgment result include relevant article applicable charge andthe term penalty.to promote progress xiao propose large-scale chinese criminal judgment prediction dataset c-ljp datasetcontains million legal document publish chinese government make c-ljpa qualify benchmark c-ljp containsthree subtasks include relevant article applicable charge term penalty firsttwo formalize multi-label classificationtasks last regression task besides english datasets also exist chalkidiset size limited.with development neural network many researcher begin explore deeplearning technology wang al.,2019 al.,2019a kang work divide primary direction first isto novel model improve performance.chen gating mechanism toenhance performance predict term ofpenalty propose multi-scale attention handle case multiple defendants.besides researcher explore utilizelegal knowledge property attention mechanism factsand article help prediction applicablecharges zhong present topologicalgraph utilize relationship differenttasks besides incorporateten discriminative legal attribute help predictlow-frequency charges.experiments analysisto good understand recent advance wehave conduct series experiment cljp firstly implement several classical textclassification model include textcnn kim,2014 dpcnn johnson zhang testtask charge article term charge article termmetrics distextcnn network experimental result judgment prediction c-ljp table denote micro-f1 andmacro-f1 denote distance prediction ground truth.lstm hochreiter schmidhuber andbert devlin parameter ofbert pretrained parameter chinesecriminal case zhong secondly implement several model speciallydesigned include factlaw al.,2017 topjudge zhong gatingnetwork chen result befound table result learn modelscan reach promising performance predictinghigh-frequency charge article however themodels perform well low-frequency labelsas micro-f1 macro-f1.hu explore few-shot learningfor however model require additionalattribute information label manually istime-consuming make hard employ themodel datasets besides find thatperformance bert satisfactory doesnot make much improvement modelswith parameter main reason thelength legal text long maximum length bert handle according statistic maximum document length is56 length document over512 document understanding reason technique require ljp.although embedding-based method canachieve promise performance still needto consider combine symbol-based withembedding-based method take topjudgeas example model formalize topologicalorder task symbol-basedpart textcnn encode factdescription combine symbol-based andembedding-based method topjudge achievedpromising result comparing resultsbetween textcnn topjudge find thatjust integrate order judgment themodel lead improvement provesthe necessity combine embedding-based andsymbol-based methods.for well performance challengesrequire future effort researcher document understanding reason techniquesare require obtain global information extremely long legal text few-shot learning.even low-frequency charge ignoredas part legal integrity therefore handle in-frequent label essential interpretability want apply method reallegal system must understand makepredictions however exist embedding-basedmethods work black factor affect prediction remain unknown thismay introduce unfairness ethical issue likegender bias legal system introducing legal symbol knowledge mention willbenefit interpretability ljp.4.2 similar case matchingin country common systemlike united states canada india judicialdecisions make accord similar representative case past result toidentify similar case primary concern judgment common system.in order good predict judgment result inthe common system similar case matching become essential topic legalai.scm concentrate find pair similar case definition similarity various.scm require model relationship betweencases information different granularity like fact level event level element level in5224other word particular form semanticmatching xiao benefit thelegal information retrieval.related worktraditional method information retrieve focus term-level similarity statistical model include tf-idf salton buckley bm25 robertson walker whichare widely apply current search system inaddition term match method researcher utilize meta-information medin,2000 capturesemantic similarity many machine learn method also apply like xuet factorization rendle kabburet rapid development deeplearning technology many researchersapply neural model include multi-layer perceptron huang shen al.,2014 huang andrnn palangi ir.there several legalir datasets includingcoliee kano caselaw locke andzuccon xiao bothcoliee caselaw involve retrievingmost relevant article large corpus whiledata example give three legal documentsfor calculate similarity datasets providebenchmarks study legalir many researcher focus build easy-to-use legalsearch engine barmakian turtle also explore utilize information include citation monroy geist raghav legal concept maxwelland schafer opijnen santos goal calculate similarity semantic level deep learning method also beenapplied legalir tran propose acnn-based model document sentencelevel pooling achieve state-of-the-artresults coliee researcher explore employ well embed method legalir landthaler sugathadasa al.,2018 analysisto good view current progress legalir select xiao experiment contain triple eachtriple contain three legal document task design determine whetherb similar implement four different type baseline termmatching method tf-idf salton buckley,1988 siamese network parametershared encoders include textcnn bidaf bert devlin al.,2019 distance function semantic match model sentence level abcnn al.,2016 document level smash-rnn jianget result find table testtf-idf experimental result evaluationmetric accuracy.from result observe exist neural model capable capture semantic information outperform tf-idf performance still enough xiaoet state main reason legalprofessionals think element datasetdefine similarity legal case legal professional emphasize whether case havesimilar element consider term-level andsemantic-level similarity insufficient task.for study direction need future effort elementalbased representation researchers focusmore symbol legal document similarity legal case relate symbolslike element knowledge incorporation assemantic-level matching insufficient need consider incorporate legalknowledge model improve performanceand provide interpretability.4.3 legal question-answeringanother typical application legalai legalquestion answering answer question legal domain mostimportant part legal professional work toprovide reliable high-quality legal consultingservices non-professionals however tothe insufficient number legal professional isoften challenge ensure non-professionals5225kd-questions ca-questions allsingle single single allunskilled humans humans experimental result jec-qa evaluation metric accuracy performance unskilled andskilled human collect original paper.question crime alice commit ifthey transport million yuan counterfeitcurrency abroad china direct evidencep1 transportation counterfeit money defendant sentence three year prison.p2 smuggling counterfeit money defendant aresentenced seven year prison.extra evidencep3 motivational concurrence criminal carry onebehavior commit several crimes.p4 motivational concurrence criminal beconvicted accord serious crime.comparison seven year three yearsanswer smuggling counterfeit money.table example zhong example direct evidence extra evidence areboth require answer question hard reasoning step prove difficulty legal question answering.can enough high-quality consulting service expect address issue.in form question varies somequestions emphasize explanation ofsome legal concept others concernthe analysis specific case besides questionscan also express differently professional non-professionals especially whendescribing domain-specific term problemsbring considerable challenge conduct experiment demonstrate difficulty oflqa well following parts.related workin legalai many datasets question answering duan propose cjrc legalreading comprehension dataset format squad rajpurkar whichincludes span extraction yes/no question andunanswerable question besides coliee kanoet contain yes/no questions.moreover exam professional qualification examination lawyer examdatasets fawei zhong quite hard require professional legalknowledge skills.in addition datasets researcher havealso work method rulebased system buscaldi al.,2013 goebel prevalent earlyresearch order reach good performance researcher utilize information like explanation concept taniguchi kano fawei formalize relevant documentsas graph help reasoning monroy tran machine learn anddeep learn method like bach also apply however mostexisting method conduct experiment smalldatasets make necessarily applicable massive datasets real scenarios.experiments analysiswe select jec-qa zhong thedataset experiment largestdataset collect exam guarantee difficulty jec-qa contain multiple-answer question together relevant article help answer question jec-qa classifies questionsinto knowledge-driven question kd-questions case-analysis question ca-questions andreports performance human implement several representative question answer model include bidaf bert devlin co-matching wanget theexperimental result find table experimental result learn the5226models answer legal question well compare promising result open-domainquestion answering still huge gapbetween exist model human lqa.for qualified method several significant difficulty overcome legal multi-hop reasoning zhong state exist model perform inference notmulti-hop reasoning however legal case verycomplicated handle singlestep reasoning legal concept understand find almost model betterat case analyze knowledge understanding prove knowledge modelling still challenge exist method model legalknowledge essential legal knowledgeis foundation lqa.5 conclusionin paper describe development statusof various legalai task discuss cando future addition applicationsand task mention many othertasks legalai like legal text summarization andinformation extraction legal contract nevertheless matter kind application wecan apply embedding-based method good performance together symbol-based method formore interpretability.besides three main challenge legal tasksremain solve knowledge modelling legalreasoning interpretability foundationson legalai reliably serve legal domain exist method solvethese problem still long forresearchers go.in future exist task researcherscan focus solve three press challenge legalai combine embedding-basedand symbol-based method task notyet dataset datasets largeenough build large-scale highquality dataset few-shot zero-shot method solve problems.furthermore need take ethical issuesof legalai seriously applying technologyof legalai directly legal system bringethical issue like gender bias racial discrimination result give method cannotconvince people address issue mustnote goal legalai replace thelegal professional help work aresult regard result modelsonly reference otherwise legal systemwill longer reliable example professional spend time complex case andleave simple case model however forsafety simple case must still review ingeneral legalai play support roleto help legal system.acknowledgementsthis work support national research development program china no.2018yfc0831900 national natural science foundation china nsfc besides dataset element extraction provide gridsum
datasets annotate protected attribute gender make difficult measure classification bias standard measure fairness e.g. equal opportunity however manually annotate largedataset protected attribute slow andexpensive instead annotate example annotate subset anduse sample estimate bias whileit possible small annotated sample less certain thatthe estimate close true bias thiswork propose bernstein bound torepresent uncertainty bias estimate confidence interval provide empirical evidence confidence interval derive consistently bound thetrue bias quantify uncertainty ourmethod call bernstein-bounded unfairness help prevent classifier beingdeemed bias unbiased insufficient evidence make either claim finding suggest datasets currently usedto measure specific bias small toconclusively identify bias except mostegregious case example consider coreference resolution system accurate gender-stereotypical sentence toclaim bias confidence weneed bias-specific dataset timeslarger winobias large available.1 introductionnlp model draw criticism capturingcommon social bias respect gender andrace manzini garg ethayarajh bias quantify apply metric embed space bolukbasi unclear bias intext embeddings affect decision make downstream classifier bias propagate deterministically possible minimally bias embeddings feed classifier make maximally biased prediction andvice-versa moreover recent work find thatweat caliskan populartest embed bias easily manipulatedto claim bias present absent ethayarajhet measure embed bias measuringclassification bias difficult datasetsare annotate protected attribute preclude standard fairness measure asequal opportunity hardt however manually annotate large dataset protected attribute slow expensive responseto problem create small datasetsannotated single protect attribute typically gender estimate bias taskssuch co-reference resolution zhao kiritchenko mohammad rudinger al.,2018 create data orannotating subset exist dataset theprotected attribute intuitively less data annotate less certain sample biasis close true bias i.e. would byannotating entire population propose bernstein bound expressour uncertainty sample bias confidence interval first show standardfairness measure equal opportunity andequalized odds hardt definea cost function fairness measure isequal difference expect cost incur bythe protect unprotected group treat thecontribution annotated example biasas random variable using bernstein inequality thus estimate probability thetrue bias within constant sample bias.working backwards derive confidenceinterval true bias treating genre ofexamples mnli williams the2915protected group rate annotator disagreement cost offer empirical evidence thatour confidence interval consistently boundsthe true bias.in quantify uncertainty around bias estimate bernstein-bounded unfairness help preventclassifiers deem bias unbiasedwhen insufficient evidence make eitherclaim example even sample bias ispositive possible true bias betweengroups zero conversely sample bias zerodoes ensure absence bias population level moreover finding suggest thatmost bias-specific datasets small toconclusively identify bias except egregious case example consider co-referenceresolution system accurate ongender-stereotypical sentence claim thatthis system gender-biased confidence would need bias-specific dataset large winobias zhao thelargest dataset currently available onlydoes community need bias-specificdatasets also need datasets muchlarger currently has.2 bernstein-bounded unfairnessin section present core idea ourpaper bernstein-bounded unfairness inpractice estimate bias call thegroupwise disparity small sample annotated data given estimate deviate fromthe true bias i.e. population level bbuhelps express uncertainty bias estimate confidence interval.definition denote thecost predict true label wherec maximum cost incurred.definition −1,0 denotean annotation function example tothe protect group unprotected groupb neither groupwise disparityδ group differencein expect cost incur group definition amortized disparity example give annotation function fand cost function amortized disparity estimate ofthe groupwise disparity base solely theexpectation amortized disparity thegroupwise disparity inpractice give i.i.d example take amonte carlo estimate partition xinto protected unprotected group fand calculate difference mean cost.an equivalent frame nrandom variable weare take mean estimate becauseexamples i.i.d. random variables.this mean bernstein inequalityto calculate probability sample meanδ̄ deviate true groupwise disparity bysome constant bound eachrandom variable ∑var denote variance bernstein inequality −nt22σ2 since interval define frequency protected unprotected example want strictly bind randomvariable thepopulation size assume leastone protected example however theinterval could criticize loosea bound effectively useless therefore assume proportion population isprotected unprotected bound thelower bound proportion known.definition denote boundsof proportion population protectedand unprotected respectively protect group necessarily small group thissetup less toreflect unprotected group small thanthe protect group boundedin −c/γb c/γb random variable using interval rewrite −nt22σ2 proposition give confidence true groupwise disparity fall inter2916val derive follow =b+√b2−8nσ2 =−2c3γlog derive rearrange side equal apply thequadratic formula find solution note thatthe width confidence interval grow desire confidence increase samplesize decrease decrease knowledge bernstein bound tight canbe apply consider variance ofthe random variable also validate empirically good candidate hoeffdingbounds another common choice.standard fairness measures usebernstein-bounded unfairness derive confidenceintervals bias metric demographic parity equal opportunity equalize odds demographic parity require successrates equal across group case cost would since therate predict positive outcome must constraint onthe annotation function equal opportunity require true positive rate equal across group hardt al.,2016 cost would still theannotation function would terminology hardt include mean annotate qualified example i.e. unqualified i.e. equalized odds require true andfalse positive rate equal across group hardt annotation functionwould equal opportunity butthe cost would account differencesin false positive rate well could bedone zero-one loss.it thus possible define cost annotation function groupwise disparity isequivalent bias define common fairnessmeasure framing problem treat cost something minimized.for example equal opportunity groupwisedisparity define difference false negative rate however could forequal opportunity well groupwisedisparity difference true positive rates.both perspective equivalent bemore intuitive depend case.3 proof-of-concept experimentswe begin provide empirical evidence confidence interval consistently boundsthe true bias i.e. population-level groupwise disparity conduct experiment mnlidev williams testingnatural language inference treat genresof examples mnli protect group genre annotation give calculate true bias difference annotator disagreement rate in-genre versus out-genre example effectively treat human annotatorsas classifier whose bias want measure.we check whether true biasfalls within confidence interval weestimate bias subset data.the experiment mnli measure animportant social bias rather mean proof-of-concept treat mnli genre protect group protected attribute– genre clearly annotate mnliover small datasets annotate attribute suchas gender setup cost isthe rate annotator disagreement require model training make result easyto replicate moreover case illustrate thatour conception bias need restrict social bias difference cost incurredby arbitrarily define groups.lastly examine large bias-specificdataset need order conclude agiven classifier bias specifically considera co-reference resolution system accurate sentence contain stereotypical genderroles fixing confidence level weshow magnitude sample bias δ̄decreases need large bias-specific dataset i.e. large order make bias claim with95 confidence.3.1 setupannotator disagreement mnli sethas genre example e.g. fiction with2917figure true bias government genre mnli bias estimate confidenceintervals blue base small sample data bias define difference annotator disagreementrates across genre confidence interval consistently bound true bias bound grow tight thesample size increase leave frequency protected group increase right left protectedgroup frequency right sample size in-genre cost out-genre cost ∆facetoface −0.012fiction −0.006government −0.024nineeleven −0.014oup −0.002travel −0.018verbatim mean in-genre out-genre cost eachgenre mnli cost example rateof annotator disagreement gold label.roughly genre since genre annotationis know treat protected attribute wedefine cost give example proportion human annotator whose annotation differsfrom gold label true bias genre i.e. groupwise disparity across data thedifference mean cost incur in-genreand out-genre example statistic intable annotation function genrejust sample in-genre out-genre example protected unprotected groupsrespectively setup ratio in-genre toout-genre example control wethen sample calculate confidenceinterval table fall within confidence interval correctly bound true bias genre.gender bias second experiment consider hypothetical co-reference resolution systemm accurate input sentenceis gender-stereotypical example mightassume doctor always replace amale pronoun nurse female pronoun.the existence system motivate creation bias-specific datasets winobiasand winogender co-reference resolution zhaoet rudinger define thecost give example zero-one loss i.e.,1 true bias corresponds difference accuracy gender-stereotypicaland non-gender-stereotypical sentence former protected group percentage point accurate genderstereotypical sentence large must forus claim confidence genderbiased i.e. bounding population-level biason mnli data even example sample estimate bias confidence interval bound truebias time outcome averageacross mnli genre average resultsacross figure bbubounds also grow tighter annotated samplesize increase frequency protectedgroup increase based thederivation interval width thesetrends expected.3.3 making claims biasin gender bias experiment want knowhow large need give bias estimate co-reference resolution system calculate sample annotateddata much data need claim isgender-biased confidence small thebias estimate data require winobias thelargest dataset available whenδ̄ confidence coreference resolution system gender-biased.in word want find small suchthat since sett← work backwards hypothetical scenario maximum costc bias estimate assume since bias-specific datasetsoften equally many protect unprotectedexamples also assume variance ismaximal i.e. input word would need bias-specific dataset least11903 example claim confidence thatthe system bias time largerthan size winobias zhao thelargest dataset currently available figure2 plot amount data need themagnitude sample bias note winobias example could onlymake bias claim confidence biasestimate high i.e. systemm percentage point accurate onthe gender-stereotypical example winobias implicationsit possible claim existence bias aparticular direction without know truebias example consider errorbars figure right confidence interval bias face government genre inmnli fall range mean thatwe confident government examplesin mnli face annotator disagreement thanother genre even know precisely howmuch however show section3.3 datasets currently estimate classification bias winobias zhao al.,2018b winogender rudinger –are small conclusively identify bias exceptin egregious cases.there possible remedy forone even though apply think wasthe tight applicable bound possibleto derive tight confidence interval could small datasets make bias claimswith high degree confidence however evenin optimistic scenario current datasets wouldprobably remain insufficient detect smallmagnitudes bias straightforward remedy would create large bias-specific datasets.even mnli example order magnitudelarger winobias suggest create largebias-specific datasets well within realm ofpossibility.4 conclusionwe first show many standard measure offairness e.g. equal opportunity expressedas difference expect cost incur protected unprotected group given mostbias estimate make small sample weproposed bernstein-bounded unfairness forquantifying uncertainty bias estimateusing confidence interval using mnli provide empirical evidence confidenceintervals consistently bound true populationlevel bias quantify uncertainty bbuhelps prevent classifier deem biasedor unbiased insufficient evidence tomake either claim although datasets currentlyused estimate classification bias e.g. winobias undoubtedly step right direction ourfindings suggest need much largerin order useful diagnostic.acknowledgmentsmany thanks aidan perreault dallas card andtengyu provide detailed feedback wethank nelson helpful discussion.2919
traditional question generation aimsto generate question give input passageand answer sequence ofanswers perform sequential questiongeneration produce series interconnected question since frequently occur information omission coreferencebetween question rather challenging.prior work regard dialog generation task recurrently produce question however suffer problemscaused error cascade could capture limited context dependency generate question semi-autoregressiveway model divide question different group generates group themin parallel process build twographs focus information passage answer respectively perform dual-graphinteraction information generation.besides design answer-aware attentionmechanism coarse-to-fine generationscenario experiments dataset contain question show modelsubstantially outperform prior works.1 introductionquestion generation teach machinesto human-like question range inputssuch natural language text image mostafazadeh knowledgebases serban recent year hasreceived increase attention wide application asking question dialog system enhance interactiveness persistence humanmachine interaction wang benefit question answering model dataaugmentation duan joint learning also play importantrole education heilman smith andclinical weizenbaum systems.traditional question generation define reverse task i.e. passage andan answer often certain span passage provide input output question ground input passage target thegiven answer sequence answer perform sequential question generation produce series interconnected question table show example compare twotasks intuitively question much moreconcise regard give answersas qa-style conversation since naturalfor human test knowledge seek information coherent question reddy al.,2019 wide application e.g. enablingvirtual assistant question base previousdiscussions good user experiences.sqg challenge task aspect first information omission question lead tocomplex context dependency second arefrequently occur coreference questions.prior work regard dialog generationtask namely conversational questionsare generate autoregressively recurrently i.e. question produce base previous output although many powerful dialog generationmodels adopt address challengesmentioned major obstacles.first model suffer problem cause byerror cascade empirical result experimentsreveal later generated question tend become shorter quality especially become irrelevant give answer e.g. else second model recurrently generate question struggle capture complexcontext dependency e.g. long-distance coreference essentially rather different fromdialog generation since answer give inadvance strict semantic constraintsduring text generation.226 small name john park swing swing friend name play slide john want play slide could play slide cried.turn answer1 park park john2 john park swinging3 john swing wings4 john park friend5 name john friend named tim6 play side7 john john could play slide8 john notable comparison traditional question generation sequential question generation thegiven passage contain five sentence mark give answer passage blue.to deal problem perform semi-autoregressive specifically wedivide target question different group question group closely-related andgenerate group parallel especially scenario become non-autoregressive grouponly contain single question since eliminatethe recurrent dependency question indifferent group generation process muchfaster model better deal problem cause error cascade information generation process perform dualgraph interaction passage-info graph andan answer-info graph construct iteratively update passage-infograph good capture context dependency answer-info graph makegenerated question relevant give answerswith help answer-aware attention mechanism besides coarse-to-fine text generationscenario adopt coreference resolutionbetween questions.prior work perform coqa reddyet high-quality dataset conversational illustrate numberof data coqa suitable someresearchers directly discardedthese data remain question become incoherent e.g. antecedent word formany pronoun unclear builda dataset coqa contain relabeled question main contributionsof work build dataset contain passage question coqa isthe first dataset specially build faras know.• perform semi-autoregressive underdual-graph interaction first timethat regard dialog generation task also propose answer-awareattention mechanism coarse-to-fine generation scenario good performance.• extensive experiment show ourmodel outperforms previous work substantial margin analysis illustratedthe impact different components.dataset paper available http //github.com/chaiz-pku/sequential-qg.2 related work2.1 traditional question generationtqg traditionally tackle rule-based method lindberg mazidi nielsen,2014 hussein labutov e.g. fill handcraft template certaintransformation rule rise data-drivenlearning approach neural network havegradually take mainstream pioneer nn-based adopt seq2seqarchitecture sutskever many ideaswere propose since make powerful include answer position feature zhou al.,2017 specialized pointer mechanism zhao al.,2018 self-attention scialom answerseparation addition enhance seq2seq model complicatedstructures variational inference adversarialtraining reinforcement learning al.,2018 kumar also gain muchattention also work performingtqg certain constraint e.g. control the227topic difficulty al.,2018 question besides combine withqa wang tang al.,2019 also focus many researchers.2.2 sequential question generationas human tend coherent questionsfor knowledge test information seek sqgplays important role many application priorworks regard dialog generation task namely conversational pretrained model perform dialog generation andthen fine-tuned parameter reinforcementlearning make generated question relevant togiven answer iteratively generated question previous output leveragedoff-the-shelf coreference resolution model introduce coreference loss besides additional humanannotations perform sentence input passage conversation flow modeling.since essentially different dialoggeneration discard dialog view proposethe first semi-autoregressive model compared additional human annotationin dual-graph interactiondeals context dependency automatically besides answer-aware attention mechanism ismuch simpler fine-tuning process panet make output answer-relevant.3 datasetas reverse task often performedon exist datasets e.g. squad rajpurkaret newsqa trischler etc.however question independent qadatasets make choice recentyears appearance large-scale conversationalqa datasets like coqa reddy andquac choi make possible traindata-driven model coqa datasetwas widely adopt prior work since testset coqa release public training passage question wassplit training validation itsvalidation passage question test set.different traditional datasets wherethe answer certain span give passage answer coqa free-form text1 cor1only answer overlap passage afterignoring punctuation case mismatches.responding evidence highlight passage.this bring trouble example consider yes/no question count among question given answer acorresponding evidence group first meet onjuly campus ohio state university many potential output e.g. group first meet july groupfirst ohio state consider thecontext form previous question potentialoutputs become even original question incoqa found year whenthere many potential output significantly different semantic meaning train converge model become extremely difficult forthis reason directly discardedquestions answer span frompassages however remain question become incoherent e.g. antecedent word manypronouns become unclear.to build dataset coqaby preserve passage rewrite allquestions answer specifically firstdiscarded question unsuitable todo three annotator hire vote thepreservation/deletion question questionis preserve answer certain span input passage2 result deleted question yes/no questionsand unanswerable question besides kappascore result give different annotatorswas indicate strong interagreement annotator remainingqa-pairs preserve original order andreplaced answer span input passages.after rewrite question make themcoherent avoid over-editing annotator wereasked modify little possible turn outthat case need deal withcoreference since prototype pronoun wereno longer exist guarantee annotation quality hire another project manager whodaily examine annotation eachannotator provide feedback annotationwas consider valid accuracy ofexamined result surpass annotationprocess take month finally datasetcontaining passage qa-pairs.2using certain span input passage instead freeformed text answer conversion thenumber potential output question greatly reduced.228figure architecture model example correspond table modelin section formalize task introduce model detail show figure model first build passage-info graph ananswer-info graph passage-info encoder andanswer-info encoder respectively perform dual-graph interaction representationsfor decoder finally different group question generate parallel coarse-to-finescenario encoders decoder take formof transformer architecture vaswani problem formalizationin input passage compose sentence ni=1 sequence answer li=1 certain span target output series question li=1 whereqi answer accord inputpassage previous qa-pairs.as mention perform ansemi-autoregressive i.e. target question aredivided different group ideally question group expect closelyrelated question different group shouldbe independent possible model take asimple effective unsupervised question cluster method intuition answer comefrom sentence correspondingquestions likely closely-related morespecifically k-th sentence contain panswers li=1 cluster ananswer-group gansk wherej1 continuous index replace answer ganskwith correspond question questiongroup gquesk define corresponding target-output special token table four targetoutputs since third sentence table contain answer correspondingwith second sentence hesay correspond last sentence supposing answer- question-groups model generate target-outputsin parallel i.e. question generate asemi-autoregressive way.4.2 passage-info encoderas show figure passage-info encodermaps input sentence ni=1 sentencerepresentations ni=1 every r2ds regard sentence sequence wordsand replace word pre-trained word embeddings mikolov densevector sequence word embeddings send transformer-encoder outputsa correspond sequence vector average vector local representationslocali si.after local representation sentence ni=1 passage another transformerencoder adopt sequence slocali ni=1into sglobali ni=1 sglobali call the229figure illustration answer embeddings ananswer-attention head forth sentence table representation word thepassage-info encoder take hiarachical structure.we expect local global representation capture intra- inter- sentence context dependenciesrespectively final representation issi slocali sglobali r2ds answer-info encoderas describe section input answer aresplit answer-groups gansk correspond k-th sentence input passage wedefine gansk rationale furtherobtain representation r2dr answerinfo encoder base transformerencoder regard sentence input.to consider information gansk twomore component answer-infoencoder show figure first adopt theanswer-tag feature word sentencesk embed layer computes rdras final embedding pre-trainedword embedding contains answer-tag feature specifically give label outside beginning inside answer gansk vector correspond label second design answer-aware attention mechanism.in multi-head attention layer notonly vanilla self-attention head also answer-aware head answer gansk answer-aware head correspond answer word belong mask outduring attention mechanism output ofthe transformer-encoder sequence vectorshenck henck henck correspond withthe input word sequence sk.after henck send sequence vector bi-directional network chung take last hiddenstate final rationale embed r2dr graph constructionin task input passage contain sentence represent ni=1 ∈r2ds leverage passage-info encoder amongall input sentence contain certainanswers define rationale base sentence gansf mj=1 j-th rationale correspond sentence inputpassage example intable respectively using answer-info encoder representation mj=1 ∈r2ds rationales.we build passage-info graph ananswer-info graph base representation rationale correspond k-thsentence input passage node vkin graph respectively example table compused iscompused show figure initial representation compute relu r2dr rationale representation embedding index de+2dr trainable parameters.and initial representation relu r2ds sentence representation andwv de+2ds parameters.after point arem node uand respectively correspondingwith i-th j-th input sentence respectively weadd edge ahyper-parameter similarly edge vand graph isomorphic.4.5 dual-graph interactionin answer-info graph node representationscontain information focus input answer inthe passage-info graph node representation capture inter- intra-sentence context dependencies.as mention good question be230answer-relevant well capture complex context dependency combine information dual-graph interaction process iteratively updatenode representation time stept representation update intou respectively three steps.first introduce information transfer step.taking example receive ifrom neighbor node neighbor thereis edge =∑uj∈n wheren compose neighbor nodeui rdg×dg parameterscontrolling information transfer andui′ whose samew word first create asequence matrix rdg×dg andvectors j|as index retrieve corresponding graph similarly computeã =∑vj∈n w̃ij b̃ij second step compute multiple gates.for compute update gate reset gate r2dg×dg paramenters similarly compute finally perform information interaction graph update node representationsunder control gate compute othergraph specifically node representation areupdated tanh tanh idea gate compute othergraph update node representation graphenables information input passage answer interact frequently actas strong constraint output questions.by iteratively perform three step ttimes final representation andv decoderfor k-th input sentence containing certainanswers decoder generate correspondingtarget-output mention generation process target-outputs independent.the decoder base transformer-decodercontaining mask multi-head self-attentionlayer multi-head encoder-attention layer feedforward projection layer softmax layer.to compute value multi-headencoder-attention layer leverage outputsfrom answer-info encoder i.e. henck describe section generate correspondingwith k-th sentence.to generate coherent question need capture context dependency input answer passage andv come dual-graph interactionprocess additional input generate first concatenate outputof head mask multi-head selfattention layer multi-head encoder-attentionlayer send next layer second theyare concatenate input feed-forwardprojection layer representation alsoexpected make generated question relevant give inputs.4.7 coarse-to-fine generationsince semi-autoregressive generation scenariomakes challenging deal coreference question especially question indifferent group perform question generationin coarse-to-fine manner decoder needsto generate coarse question pronounsare replace placeholder finalresults additional pre-trained coreference resolution model fill pronouns differentplaceholders make fair comparison usethe coreference resolution model clark manning adopt prior work corenqg duand cardie corefnet bleu1 bleu2 bleu3 rouge meteor lengthseq2seq cardie serban xing experimental result column bold underline best performance baseline method respectively evaluation bleu rouge-l meteor model differs others except themeteor score corefnet significantly base one-side paired t-test experimentsin section first introduce three kind ofbaselines compare analyse theresults different model automaticand human evaluation metrics.5.1 baselineswe compare model seven baseline thatcan divide three group first usedthree model seq2seq model pioneer nn-based copynet model introduce pointermechanism corenqg cardie hybrid feature word answer andcoreference embeddings encoder adoptedcopy mechanism decoder second since priorworks regard conversation generationtask directly powerful multi-turn dialog system latent variable hierarchical recurrent encoder-decoder architecture vhred serbanet hierarchical recurrent attention architecture hran xing third prior work mention panet adopt redr model whichhad best performance corefnet model although cfnet inthis paper good result require additionalhuman annotation denote relationship input sentence target question isunfair compare cfnet methods.it worth mention generatingquestions second third group baseline previously generate output wereused dialog history i.e. gold standard question remain unknown prior work directly dialog history wethink inappropriate practice coqa ourspassage average number word passage questionand answer different datasets.5.2 automatic evaluation metricsfollowing convention bleu papineni rouge-l andmeteor lavie agarwal automaticevaluation metric also compute averageword-number generated question shownin table semi-autoregressive model outperform method substantially.when focus second third group ofbaselines regard multi-turn dialog generation task find model thirdgroup powerful since make good useof information input passage besides model second group tend generate shortestquestions finally similar problem dialog system often generate dull response model also suffer produce generalbut meaningless question like else compare first third group ofbaselines model surprising model show advantagesthan model take relationshipsbetween question consideration besides corefnet good performance among baseline especially redr indicate compare implicitly perform reinforcementlearning model explicitly target answer input effective.232corenqg corefnet oursfluency human evaluation result scores metric range large score better.note directly compare performancebetween task task samemodel e.g. seq2seq model evaluation scoresfor task much high surprising since hard deal withdependencies question another fact liesin computation automatic evaluation metrics.as show table question datasetsare much short since automaticevaluation metric base n-gram overlapsbetween generate gold standard question thescores significantly growth reason bleu4 score listedin table also illustrate importance ofperforming human evaluation.5.3 human evaluationit generally acknowledge automatic evaluation metric enough weperform human evaluation five aspect fluencymeasures question grammatically correct andis fluent read coherence measure question coherent previous coreferencemeasures question correct pronoun answerability measure question target onthe give answer relevance measure question ground give passage since perform human evaluation rather expensive andtime-consuming pick best model corenqg model corefnet comparewith model randomly select passagesfrom test give answer asked10 native speaker evaluate output eachmodel independently aspect reviewersare choose score where3 indicate best quality.the average score evaluation metricare show table find modelgets best competitive performance eachmetric come fluency model gethigh performance corefnet outputsbleu3 rouge meteorno interact co2fine results ablation tests.shortest question best score coherence corenqg poor result since generatesquestions independently come coreference model slightly corefnet direct supervision attention weightsby coreference resolution model finally ourmodel best performance answerabity relevance however worth noticingthat model rather poor performance underthese aspect indicate make concisequestion meaningful i.e. target give answer information input passage i.e. perform proper information elimination major challenge besides pointedout table question dataset aresignificantly short compare dataset make subtle error much easy noticed.6 analysis6.1 ablation testin section perform ablation test verifythe influence different component model.first modify equation intou tanh tanh interact model i.e. graph independently update without interaction second build uni-graph model removingthe passage-info encoder remain rationalegraph update similarly discard attention-aware head therationale encoder uni-heads model build co2fine model without coarseto-fine generation scenario finally build anon-auto model perform nonautoregressive i.e. question generatedin parallel.233peter puppy inside store long time fact three month peter many puppy find person begin wonder could one.he thought maybe pretty enough maybe bark loud enough triedto please every person come store pick small puppy however thischanged sammie come store look golden puppy want puppy could snugglewith happen peter tire sammie come hold peter want show bark tire fell right sleep sammie love love hold herarms sammie take peter home make memories.turn gold standard corefnet ours1 long peter store long long peter someone think come store come store come store sammie look look peter want show peter want show show want else take sammie take sammie take table example output different model mark give answer passage blue.as show table component ourmodel play important part results theno interact model indicate compare independently update passage-info graph andanswer-info graph make information moreinteracted dual-graph interaction scenariois powerful surprisingly uni-graphmodel remove passage encoder i.e. less focus context dependency sentencesfrom input passage uni-heads model discard answer-aware attention mechanism i.e. less focus give answer significantworse performance compare full model.besides coarse-to-fine scenario help better deal dependency questionssince widespread coreference finally although architecture non-auto model aspecial case model group onlycontains single question performance dropssignificantly indicate importance usingsemi-autoregressive generation however dualgraph interaction still make performance betterthan seq2seq copynet table running examplesin table present generate examplescomparing model strong baselinecorefnet hand model performsbetter corefnet especially outputquestions target give answer turn2 also correctly deal coreference e.g. distinguish peter sammie onthe hand generated question poorquality gold standard question involve morereasoning turn besides gold standardquestions concise well turn conclusionin paper focus animportant challenge task different fromprior work regard dialog generation task propose first semi-autoregressivesqg model divide question different group generates group ofclosely-related question parallel thisprocess first build passage-info graph ananswer-info graph perform dual-graphinteraction representation capture context dependency passage questions.these representation ourcoarse-to-fine generation process perform experiment analyze limitation existingdatasets create first dataset specially usedfor contain question experimentalresults show model outperforms previousworks substantial margin.for future work major challenge generate meaningful informative concisequestions besides powerful question clustering coarse-to-fine generation scenario alsoworth exploration finally perform onother type input e.g. image knowledgegraphs interest topic.acknowledgmentsthis work support national natural science foundation china keylaboratory science technology standard inpress industry laboratory intelligent pressmedia technology thank anonymous reviewer helpful comment xiaojun wanis corresponding author.234
despite recent progress conversational question answering prior work focus follow-up question practical conversational question answer system often receive follow-up question ongoing conversation crucial system tobe able determine whether question follow-up question current conversation effective answer find subsequently paper introduce newfollow-up question identification task propose three-way attentive pooling networkthat determine suitability follow-upquestion capture pair-wise interactionsbetween associated passage conversation history candidate follow-up question enable model capture topiccontinuity topic shift score particular candidate follow-up question experiments show propose three-way attentive pooling network outperform baselinesystems significant margins.1 introductionconversational question answer mimicsthe process natural human-to-human conversation recently conversational gain muchattention system need answer seriesof interrelated question associated textpassage structured knowledge graph choiet reddy saha conversational task explicitly focus require model identify thefollow-up question practical conversational qasystem must possess ability understand theconversation history well identify whetherthe current question follow-up particular conversation consider user tryingto conversation machine e.g. siri google home alexa cortana first userasks question machine answer whenpassage script verhoeven first american film flesh blood star rutger hauerand jennifer jason leigh verhoeven move hollywood wider range opportunity filmmaking working u.s. make serious change instyle direct big-budget violent special-effectsheavy smash robocop total recall robocop verhoeven follow success theequally intense provocative basic instinct receive academy awards nomination forfilm editing original music history first film verhoeven flesh bloodq genre film make big-budget violent special-effects-heavysmashescandidate follow-up question example year first film debut validdid make film final year invalidwhat debut film invalidfigure examples illustrate follow-up questionidentification task.the user second question important machine understand whether itis follow-up first question answer.further need determine every question pose user ongoing conversation.by identify whether question follow-upquestion machine determine whether conversation history relevant question based onthis decision expect suitable answerfinding strategy answer question additionally system first retrieve relevantdocuments information retrieval engine answer question follow-up questionidentifier predict question invalid followup question give retrieved document cancommunicate engine retrieve additionalsupporting documents.a example instance give figure toillustrate follow-up question identification taskin conversational reading comprehension setting.960we present dataset learn identifyfollow-up question namely given text passage knowledge series question-answerpairs conversation history require modelto identify whether candidate follow-up questionis valid invalid propose dataset requiresa model understand topic continuity andtopic shift correctly identify follow-up question instance first example give infigure model need capture topic continuity first question-answer pair i.e. firstfilm flesh blood topic shift thesecond question-answer pair i.e. genre film conversation history candidate followup question second example invalid sincethe associate passage provide information final year last follow-upquestion example invalid since verhoeven she.there research past whichfocuses identify part conversation history important process follow-upquestions bertomeu kirschner andbernardi however recently proposedneural network-based model conversationalqa explicitly focus follow-up question paper propose three-way attentive pooling network follow-up question identification conversational reading comprehensionsetting evaluate candidate follow-up question base perspective topic shift andtopic continuity propose model make useof attention matrix conditionedover associated passage capture topic shiftin follow-up question also rely anotherattention matrix capture topic continuity directlyfrom previous question-answer pair conversation history comparison develop several strong baseline system follow-upquestion identification.the contribution paper follows:1 propose task follow-up questionidentification conversational reading comprehension support automaticevaluation.2 present dataset namely whichis derive recently release conversational dataset quac choi propose three-way attentive pooling network capture topic shift andtopic continuity follow-up question identification propose model significantlyoutperforms baseline systems.2 task overviewgiven passage sequence question-answerpairs conversation history candidatefollow-up question task identify whetheror candidate follow-up question validfollow-up question denote passage consist token sequenceof previous question corresponding answer denote number ofprevious question-answer pair conversationhistory candidate follow-up question denote formulate task binaryclassification task classify valid orinvalid remainder paper denotethe length candidate follow-up question model concatenate previous questionsand answer special separator token asfollows combined length previous questionanswer pair conversation history denotedas datasetin section describe prepare thelif dataset follow analysis dataset.3.1 data preparationwe rely quac dataset choi prepare dataset question thequac dataset assign three category could followup question construct valid instance ofthe dataset follow-up questioninstances since test quac hide wesplit quac development halvesto generate development test oflif split passage level ensurethat overlap passage thedevelopment test set.to create instance quac wetake associated passage previous questionanswer pair till follow-upquestion next question gold validcandidate follow-up question instance sample invalid follow-up question twosources:9611 questions conversation quacwhich serve potential distractors and2 non-follow-up question conversation quac occur goldvalid follow-up question.the sample first source involve atwo-step filtering process first compare cosine similarity associated passage andall question conversation embeddings generate infersent conneauet take question basedon high similarity score second step concatenate gold valid candidate follow-upquestion question-answer pair conversation history form augmented follow-upquestion calculate token overlapcount ranked question obtain inthe first step augmented follow-up question.we normalize token overlap count dividingit length ranked question remove stop word valid instance athreshold take least question high normalized token overlapcount invalid candidate follow-up questions.we also introduce potential distractors thesame conversation quac check theremaining question-answer pair occur afterthe valid follow-up question questionas invalid candidate question appear justbefore label follow-upquestion throughout invalid question sample process exclude generic follow-up question contain keywords else anyother interesting aspect avoid select follow-up question potentiallyvalid e.g. insteresting aspect aboutthis article training development wecombine candidate follow-up question fromboth conversation conversation.we keep three test candidate different source conversation andthe conversation test-i conversation test-ii conversation test-iii overall dataset statisticsare give table randomly sample follow-up question test-i andmanually check verify ofthem truly invalid.lif train/dev/test-i/test-ii/test-iii instances prev passage question answer fuq† dataset statistic †follow-up question3.2 challenges datasetto identify whether question valid follow-upquestion model need ability capture itsrelevance associate passage conversation history model require identifywhether subject question asin associated passage conversation history often distract introduction ofpronouns e.g. possessive pronoun e.g. resolution pronounsis critical aspect determine validityof follow-up question also need examinewhether action characteristic thesubject describe candidate follow-up question logically infer associatedpassage conversation history moreover capture topic continuity topic shift necessaryto determine validity follow-up question.the subject action characteristic inthe invalid follow-up question often mentionedin passage associate different topics.3.3 data analysiswe randomly sample invalid instance fromthe test-i manually analyze basedon different property give table wefound invalid question identical topic associated passage thequestions require pronoun resolution thequestions subject entity goldfollow-up question question havethe subject entity last question theconversation history pronouns invalidquestions match pronoun correspondingvalid follow-up question match last question conversation history another ofthe case case question typesare valid question ofthe case last question inthe conversation history also observe invalid question mention actionsas corresponding valid arethe last question conversation962properties exampleidenticaltopic band release secondalbum rush blood head album name pronounresolution wealthy person college entitymatch gold song causeda feud song onthis album entitymatch last writing award award pronounmatch gold many goal make marry many year pronounmatch last bear live exile q-typematch gold year happen year enact thereproductive health q-typematch last happen fight happen episode actionmatch gold tour brando newyork actionmatch last release next albumreleased table analysis dataset percentage since many example consist multiple property invalid follow-up question associated passage gold valid follow-upquestion last question conversation history history case distribution ofthese property show challenge tacklingthis task.4 three-way attentive pooling networkin section describe propose three-wayattentive pooling network1 first apply embed layer associate passage conversation history candidate follow-up question.further encode derive sequence-levelencoding vector propose three-wayattentive pooling network apply score eachcandidate follow-up question.4.1 embedding encodingwe character word embeddings2 similar obtain character-level1the source code data release http //github.com/nusnlp/lif2we also experiment elmo bert notobserve consistent improvement.embedding convolutional neural network first character embed vectorsusing character-based lookup table arefed whose size input channelsize output maxpooled entire width obtain fixed-sizevector token pre-trained vectorsfrom glove pennington obtain afixed-length word embed vector token.finally word character embeddings areconcatenated obtain final embeddings.for encode conversation history thecandidate follow-up question bidirectionallstms hochreiter schmidhuber werepresent sequence-level encoding conversation history candidate follow-up question ru×h rv×h respectively number hidden unit similarly compute sequence-level passage encoding result rt×h similarity matrixa rt×u derive joint encodingwe jointly encode passage conversation history apply row-wise softmaxfunction obtain rt×u passage word aggregated representation conversation history give rt×h aggregated vector correspond passage word concatenate passage vector followedby another bilstm obtain joint representationv rt×h multi-factor attentionin addition multi-factor self-attentive encoding kundu apply joint representation represent number factor multi-factor attention rt×m×t formulate rh×m×h tensor.a max-pooling operation perform number factor result selfattention matrix rt×t normalize byapplying row-wise softmax function resultingin rt×t self-attentive encodingcan give rt×h selfattentive encoding vector concatenatedwith joint encoding vector feed-forward963neural network-based gating apply controlthe overall impact result rt×2h thefinal passage encode rt×h obtain byapplying another bilstm layer y.4.2 three-way attentive poolingnow propose three-way attentive pooling network score every candidate follow-upquestion architecture network depict figure pooling first propose dossantos successfully theanswer sentence selection task essentiallyan attention mechanism enable joint learningof representation pair input well astheir similarity measurement primary idea isto project paired input common representation space compare plausiblyeven input semantically comparable question-answer pair paper extend idea attentive pooling networkto propose three-way attentive pooling network follow-up question identification task model need capture suitabilityof candidate follow-up question comparingwith conversation history associatedpassage particular propose model aimsto capture topic shift topic continuation thefollow-up question santos asingle attention matrix compare pair inputs.in contrast propose model relies threeattention matrix additional attention matrix make associate passage.moreover propose model develop dealwith complex follow-up question identification task contrast propose model indos santos score candidatefollow-up question base relevance theconversation history different perspective consider associate passage i.e. knowledge without consider passage.attention matrix computationin step compute three different attentionmatrices capture similarity theconversation history candidate follow-upquestion matrix associate passage take consideration another onewhen passage consider attentionmatrix rt×u capture tokenwise contextual similarity conversationhistory passage give fattn fattn function write asfattn intuitively capture contextual similarity score thei-th token passage i.e. i-th andthe j-th token conversation history i.e. j-th similarly attention matrixac rt×v capture contextual similarity candidate follow-up question theassociated passage give fattn note jointly tocapture similarity give p.the attention matrix ru×v capture similarity candidate follow-upquestion conversation history without consider associated passage give fattn attention poolingafter obtain attention matrix applycolumn-wise row-wise max-pooling theassociated passage consider capture similarity conversation history thecandidate follow-up question perform columnwise max-pooling follow bynormalization softmax result ruand respectively instance isgiven softmax max1 intuitively i-th element represent therelative importance score contextual encoding i-th token conversation historywith respect passage encode vector every element interpret samefashion associate passage encodingis consider perform row-wise andcolumn-wise max-pooling generaterqc respectively.candidate scoringin step score candidate follow-upquestion candidate score base twoperspectives without consideration of964figure architecture three-way attentive pool network.the associate passage encode score fsim fsim encoding similarity function fsim similarity function fsim wherem binary cross entropy loss trainingthe model prediction find threshold tomaximize score development forthe test instance threshold predictwhether follow-up question valid invalid.5 baseline modelswe develop several rule-based statistical machinelearning neural baseline model themodels threshold determine base bestperformance development set.5.1 rule-based modelswe develop model base word overlapcounts candidate follow-up question passage candidatefollow-up question conversation history.we normalize count value base lengthof candidate follow-up question.next develop model base contextual similarity score infersent sentenceembeddings conneau model compare candidate follow-up question withthe associate passage conversation history respectively similarity score computedbased vector cosine similarity.we also develop another rule-based model usingtf-idf weighted token overlap score prependthe last question conversation history tothe candidate follow-up question tfidf overlap word concatenatedcontext passage.5.2 statistical machine learning modelswe handcraft feature statistical machine learning model featuresconsists tf-idf weight glove vector sincewe adopt dimensional glove vector ourexperiments feature dimension test-i test-ii test-iiimodels v-p/-r/-f1/macro v-p/-r/-f1/macro v-p/-r/-f1/macro v-p/-r/-f1/macro f1norm overlap overlap hist hist overlap regressiontf-idf glove count comparison result follow-up question identification task compare performance threeway attentive pooling network several rule-based statistical machine learning neural model valid precision recall passage hist conversation history feature consists word overlapcounts compute pairwise word overlapcounts among candidate follow-up question associated passage conversation history overlap count-based feature dimension experiment logistic regressionusing derive features.5.3 neural modelswe also develop several neural baseline models.we first concatenate associated passage theconversation history candidate follow-upquestion follow embed describe earlier apply sequence-levelencoding either bilstm equal number unigram bigram trigram filter output concatenate toobtain final encoding next apply eitherglobal max-pooling attentive pooling obtainan aggregated vector representation follow afeed-forward layer score candidate follow-upquestion sequence encoding concatenate text rl×h rowof aggregated vector attentivepooling obtain learnable vector also develop baseline model bert devlin al.,2019 first concatenate input thenapply bert derive contextual vector next aggregate single vector attention feed-forward layer scoreeach candidate follow-up question.6 experimentsin section present experimental setting result performance analysis.6.1 experimental settingswe update glove vector training.we character-level embeddingvectors number hidden unit thelstms dropout srivastava probability followingkundu number factorsas multi-factor attentive encoding theadam optimizer kingma learn rate clipnorm following choi consider previous questionanswer pair conversation history beinga binary classification task precision recall macro evaluation metric scoresreported paper resultstable show propose model outperformsthe compete baseline model significant margin across test perform statisticalsignificance test paired t-test bootstrapresampling performance propose model issignificantly well best baseline system provide high macro-f1score test-i lstm-based neural baselinesperform well rule-based statisticalmachine learning model case testiii statistical model tend predict valid andthe number valid instance much high thanthe invalid instance resulting966model v-f1 macro history knowledge multi-factor attn joint encode char embed ablation study development set.in high valid score baseline system perform well valid question performpoorly evaluate macro measure performance valid invalid followup question macro overall evaluationmetric compare system overall identify follow-up question conversation test-iii hard compare conversation test-ii perform ablation study show table4 propose model performs worst wedo consider conversation history isbecause question-answer pair conversation history help determine topic continuitywhile identify valid follow-up question theperformance also drop considerthe associated passage i.e. knowledge ithelps capture topic shift performance alsodegrades remove perform betterthan model consider conversation history conversation historyis take consideration passage encoding.the performance also degrade removeother component multi-factor attentive encoding joint encoding character embedding.6.3 qualitative analysisthe propose model capture topic continuity topic shift three-way attentivepooling network attention pool andac capture topic shift follow-upquestion give conversation history consider first example table donot consider passage could identify thefollow-up question correctly proposedmodel correctly identify topic shift duration riot validate passagewords four restore order takeback prison september secondexample model could correctly identifytopic continuity schuur model withouthistory fail identify follow-up question.we perform error analysis propose model fail identify follow-up question randomly sample instance valid invalid development set.we find require pronoun resolution subject follow-up questions.38 instance require validation actions/characteristics subject e.g. theyhave child give birth daughter error occur require matchingobjects predicate occur different form e.g. hatred hate television remain case could correctlycapture topic shift.7 related workmany data-driven machine learn method havebeen show effective task relevant fordialog dialog policy learning young al.,2013 dialog state tracking henderson williams naturallanguage generation sordoni al.,2016 bordes recentdialog system either goal orient e.g. simple chit-chat domain-specific theyare goal orient e.g. help desk last fewyears surge interest conversational question answering saha release complex sequential question answering csqa dataset learn conversation througha series interrelated pair inferencing overa knowledge graph choi releaseda large-scale conversational dataset namelyquestion answer context quac mimic student-teacher interactive scenario reddyet release coqa dataset manysystems evaluate propose sdnet fuse context traditional reading comprehension model huang propose flow mechanism incorporateintermediate representation generate theprocess answer previous question throughan alternate parallel processing structure aconversation setting give previous pairsas conversation history model focuson answer next question work focus identify follow-up question recently saeidi propose dataset regulatory text require model follow-upclarification question however answer arelimited make task rather967passage september prisoner state penitentiary attica take control cell block andseized thirty-nine correctional officer hostage four negotiation department correctional servicescommissioner russell oswald agree inmate demand various reform refuse grant completeamnesty rioter passage country removal prison superintendent negotiationsstalled hostage appear imminent danger rockefeller order york state police national guardtroops restore order take back prison september attica prison september prisoner state penitentiary attica take riot inmate demand various reform refuse grant complete amnesty rioter passage country removal prison superintendent.candidate follow-up question long riot last validpassage schuur audition drummer/bandleader shaughnessy escorted twin brother backstage sing blue hire vocalist orchestra energy force jazztrumpeter dizzy schuur discover schuur audition drummer/bandleader shaughnessy follow-up question hire shaughnessy validtable examples take development model correctly identify valid follow-upquestion.restrictive moreover saeidi focus generate clarification question inresponse question conversation focuson identify whether question follow-upquestion conversation.8 conclusionin paper present follow-up question identification task conversational setting.we develop dataset namely derive previously release quac dataset.notably propose dataset support automaticevaluation propose novel three-way attentive pooling network identify whether afollow-up question valid invalid consider associate knowledge passage theconversation history additionally developedseveral strong baseline system show thatour propose three-way attentive pooling networkoutperforms baseline system incorporating three-way attentive pooling network intoopen domain conversational system beinteresting future work.acknowledgmentsthis research support national researchfoundation singapore singapore programme award number aisg-rp-2018-007
graphs capture relation textualunits great benefit detect salientinformation multiple document generate overall coherent summary thispaper develop neural abstractive multidocument summarization model whichcan leverage well-known graph representation document similarity graphand discourse graph effectively process multiple input document produceabstractive summary model utilizesgraphs encode document order capture cross-document relation crucial summarize long document ourmodel also take advantage graph toguide summary generation process whichis beneficial generate coherent concise summary furthermore pre-trained language model easily combine withour model improve summarization performance significantly empirical result wikisum multinewsdataset show propose architecturebrings substantial improvement severalstrong baselines.1 introductionmulti-document summarization bringsgreat challenge widely sequence-tosequence seq2seq neural architecture require effective representation multiple inputdocuments content organization long summary different document containthe content include additional information present complementary contradictory information radev different single document summarization cross-document linksare important extract salient information detect redundancy generate overallcoherent summary graphs capture∗corresponding author.relations textual unit great benefitsto help generate informative concise coherent summary multipledocuments moreover graph easily construct represent text span sentence paragraph graph node semanticlinks edge graph representation document similarity graph basedon lexical similarity erkan radev discourse graph base discourse relation christensen widely usedin traditional graph-based extractive model however well study mostabstractive approach especially end-to-endneural approach work study effectiveness explicit graph representation neuralabstractive mds.in paper develop neural abstractivemds model leverage explicit graph representation document effectively process multiple input document distill abstractive summary model augment end-toend neural architecture ability incorporate well-established graph document representation summary generation process specifically graph-informed attentionmechanism develop incorporate graph intothe document encode process enable ourmodel capture rich cross-document relations.furthermore graph utilize guide summary generation process hierarchical graphattention mechanism take advantage ofthe explicit graph structure help organize thesummary content benefiting graph modeling model extract salient informationfrom long document generate coherent summary effectively experiment threetypes graph representation include similaritygraph topic graph discourse graph allsignificantly improve performance.6233additionally model complementary tomost pre-trained language model likebert devlin roberta al.,2019 xlnet yang beeasily combine model process muchlonger input combined model adopt advantage graph model pre-trainedlms experimental result show graphmodel significantly improve performance ofpre-trained mds.the contribution paper follow work demonstrate effectiveness ofgraph modeling neural abstractive mds.we show explicit graph representationsare beneficial document representation summary generation.• propose effective method incorporateexplicit graph representation neuralarchitecture effective method combine pre-trained graph model toprocess long inputs effectively.• model bring substantial improvementsover several strong baseline wikisum multinews dataset also reportextensive analysis result demonstrate thatgraph model enables model processlonger input good performance andgraphs rich relation beneficial mds.12 related work2.1 graph-based mdsmost previous approach extractive extract salient textual unit documentsbased graph-based representation sentences.various rank method develop torank textual unit base graph select mostsalient inclusion final summary.erkan radev propose lexrank tocompute sentence importance base lexicalsimilarity graph sentence mihalcea tarau propose graph-based ranking modelto extract salient sentence document propose incorporate documentlevel information sentence-to-document relation graph-based ranking process series variant pagerank algorithm been1codes result http //github.com/paddlepaddle/research/tree/master/nlp/acl2020-graphsumfurther developed compute salience textual unit recursively base various graph representation document xiao recently yasunaga propose neural graph-based model extractive approximate discourse graph isconstructed base discourse marker entitylinks salience sentence estimate usingfeatures graph convolutional network kipfand welling also propose agraph-based neural sentence order model whichutilizes entity link graph capture globaldependencies sentences.2.2 abstractive mdsabstractive approach meet limited success traditional approach mainly include sentence fusion-based banerjee filippova strube barzilay mckeown barzilay information extractionbased pighin wang andcardie genest lapalme andzhuge paraphrasing-based bing al.,2015 berg-kirkpatrick cohn lapata recently research parsethe source text representation thengenerate summary base liao neural abstractive model haveachieved promising result al.,2017 paulus gehrmann celikyilmaz narayanet yang sharma perez-beltrachini straightforward extend lack ofsufficient training data early approach tosimply transfer model task lebanoffet zhang baumel utilize unsupervised model rely reconstruction objective liu,2019 later propose construct large scale dataset namely wikisum base wikipedia develop seq2seq modelby consider multiple input document aconcatenated flat sequence propose construct local knowledge graphfrom document linearize graph asequence good sale seq2seq model multidocument input fabbri also introduce middle-scale news dataset namely multinews propose end-to-endmodel incorporate traditional mmr-based6234hierarchical graph attentionadd normalizefeed forwardadd normalizefeed forwardpositionalencodingmasked self-attentionadd normalizegraph decoding layertoken1 endgraph encoding layerfirst paragraph last paragraphgraph-informed self-attentionadd normalizefeed forwardadd normalizefeed forwardparagraphposition encodingtokenpositionencodingtransformer transformerfigure illustration model follow encoder-deocder architecture encoder stack oftransformer layer graph encode layer decoder stack graph decode layer incorporateexplicit graph representation graph encode layer graph decode layers.extractive model standard seq2seq model.the seq2seq model study importance cross-document relation graphrepresentations mds.most recently lapata proposea hierarchical transformer model utilize hierarchical structure document proposeto learn cross-document relation base selfattention mechanism also propose incorporate explicit graph representation modelby simply replace attention weight agraph matrix however achieve obviousimprovement accord experiment ourwork partly inspire work approach quite different contrast totheir approach incorporate explicit graph representation encode process graphinformed attention mechanism guidanceof explicit relation graph model learnbetter rich cross-document relation thusachieves significantly well performance.we alsoleverage graph structure guide summarydecoding process beneficial long summary generation additionally combine theadvantages pretrained model.2.3 summarization pretrained lmspretrained peters radford devlin dong al.,2019 recently emerge technologyfor achieve impressive improvement widevariety natural language task include bothlanguage understanding language generation edunov rothe andlapata attempt incorporate pre-trainedbert encoder model achieve significant improvement dong furtherpropose unified language understanding language generation task achievesstate-of-the-art result several generation tasksincluding work propose effective method combine pretrained ourgraph model make able processmuch long input effectively.3 model descriptionin order process long source document effectively follow lapata split source document multiple paragraphs byline-breaks graph representation document construct paragraph example similarity graph build base cosinesimilarities tf-idf representation paragraph denote graph representation matrix input document indicatesthe relation weight paragraph task generate summary ofthe document collection give input paragraphsp1 graph representation g.our model illustrate figure follow encoder-decoder architecture bahdanauet encoder compose several token-level transformer encode layer andparagraph-level graph encode layer canbe stack freely transformer encoding layerfollows transformer architecture introducedin vaswani encode contextual information token within paragraph the6235graph encode layer extend transformer architecture graph attention mechanism toincorporate explicit graph representation theencoding process similarly decoder compose stack graph decode layer theyextend transformer hierarchical graphattention mechanism utilize explicit graph structure guide summary decoding process inthe follow focus graph encodinglayer graph decode layer model.3.1 graph encoding layeras show figure base output thetoken-level transformer encode layer graphencoding layer encode documentsglobally exist neural work utilizesattention mechanism learn latent graph representation document graph edge areattention weight lapata niculae fernandes however much work traditional show explicit graph representation beneficial tomds different type graph capture differentkinds semantic relation lexical relationsor discourse relation help modelfocus different facet summarization task.in work propose incorporate explicitgraph representation neural encoding process graph-informed attention mechanism ittakes advantage explicit relation graphsto learn good inter-paragraph relation paragraph collect information relatedparagraphs capture global information thewhole input.graph-informed self-attention graphinformed self-attention extend self-attentionmechanism consider pairwise relation inexplicit graph representation xl−1i denotesthe output graph encode layerfor paragraph inputparagraph vector paragraph thecontext representation compute aweighted linearly transform paragraphvectors =softmax xl−1i xl−1j t√dheadui =l∑j=1αij xl−1j rd∗d parameterweights denote latent relation weight paragraph main difference ofour graph-informed self-attention additionalpairwise relation bias compute agaussian bias weight graph representation matrix denote standard deviation represent influence intensity graph structure.we empirically tune developmentdataset gaussian bias −inf measure tightness paragraph andpj exponential operation softmaxfunction gaussian bias approximates multiply latent attention distribution weight∈ graph-attention mechanism term eijin equation keep ability model latent dependency paragraph theterm incorporate explicit graph representation prior constraint encode process.this model learn good richerinter-paragraph relation obtain informative paragraph representations.then two-layer feed-forward network withrelu activation function high-way layernormalization apply obtain vector ofeach paragraph =wo2relu xl−1i =layernorm xl−1i rdff∗d rd∗dff learnable parameter hidden size feedforward layer.3.2 graph decoding layergraphs also contribute summary generation process relation textual unitscan help generate coherent concise summary example christensen propose leverage approximate discourse graph tohelp generate coherent extractive summary thediscourse relation sentence tohelp order summary sentence work wepropose incorporate explicit graph structure intothe end-to-end summary decoding process graphedges guide summary generationprocess hierarchical graph attention which6236is compose global graph attention local normalized attention component inthe graph decode layer similar transformer architecture focus extension ofhierarchical graph attention.global graph attention global graph attention develop capture paragraph-levelcontext information encoder part differentfrom context attention transformer utilize explicit graph structure regularize theattention distribution graph representationsof document guide summarygeneration process.let yl−1t denote output decode layer t-th token summary assume token align withseveral relate paragraph thecentral position since prediction centralposition depend corresponding query token apply feed-forward network transformyl−1t positional hidden state thenmapped scalar linear projection sigmoid tanh wpyl−1t rd∗d denote weightmatrix indicate central position paragraph t-th summary token.with central position paragraph determine graph structure attentiondistribution paragraph regularization graph structure obtain =softmax denote attention weight betweentoken vector yl−1t paragraph vector whichis compute similarly equation globalcontext vector obtain weighted sumof paragraph vector =∑lj=1 βtjxjin decoder graph also model agaussian bias different encoder central mapping position firstly decide thengraph relation correspond position areused regularize attention distribution relation graph helpalign information source input andsummary output globally thus guide summary decoding process.local normalized attention local normalized attention develop capture tokenlevel context information within paragraph.the local attention apply paragraphindependently normalize global graphattention model process longerinputs effectively.let denote local attention distributionsof t-th summary token i-th token inthe j-th input paragraph normalized attentionis compute jiβtj local context vector compute aweighted token vector paragraph =∑lj=1∑nk=1 jixjifinally output hierarchical graph attention component compute concatenate andlinearly transform global local contextvector r2d∗d weight matrix throughcombining local global context decodercan utilize source information effectively.3.3 combined pre-trained lmsour model easily combine pre-trainedlms pre-trained mostly base sequential architecture effective shorttext example bert devlin roberta pre-trained withmaximum token lapata propose utilize bert single document summarization task truncate input documentsto token task however thanks tothe graph modeling model process muchlonger input natural idea combine ourgraph model pretrained combinethe advantage specifically tokenlevel transformer encode layer model canbe replace pre-trained like bert.in order take full advantage graphmodel pre-trained input documentsare format following first paragraph second paragraph last paragraph encode pre-trained andthe output vector token asthe vector corresponding paragraph finally paragraph vector feed graph encoderto learn global representation graph decoderis generate summaries.62374 experiments4.1 experimental setupgraph representations experiment withthree well-established graph representation similarity graph topic graph discourse graph thesimilarity graph build base tf-idf cosine similarity paragraph capture lexical relation topic graph build base topicmodel blei capture topic relationsbetween paragraph edge weight cosinesimilarities topic distribution theparagraphs discourse graph build capture discourse relation base discourse marker however moreover co-reference entitylinks christensen type ofgraphs also model experiment explicitly state similaritygraph default widely usedin previous work.wikisum dataset follow lapata treat generation lead wikipedia section task thesource document reference webpage thewikipedia article search result returnedby google summary wikipediaarticle first section source document arevery long messy split multipleparagraphs line-breaks paragraphsare rank title rank paragraphsare select input system directly utilize ranking result lapata top-40 paragraph assource input average length paragraphand target summary token respectively seq2seq baseline paragraph concatenate sequence theranking order lead token input.the dataset split instance fortraining validation test similar lapata buildsimilarity graph representation paragraph onthis dataset.multinews dataset proposed fabbri multinews dataset consist news articlesand human-written summary dataset comesfrom diverse news source site wikisum dataset multinews ismore similar traditional dataset asduc much large scale fabbri dataset split instance fortraining validation testing.the average length source document outputsummaries token token respectively seq2seq baseline truncaten input document token take firstl/n tokens source document weconcatenate truncated source document asequence original order similarly ourgraph model input document truncated paragraph take first paragraphsfrom source document build threetypes graph representation dataset toexplore influence graph type mds.training configuration train modelswith maximum likelihood estimation labelsmoothing szegedy smoothingfactor optimizer adam kingma ba,2015 learn rate β1=0.9 β2=0.998.we also apply learn rate warmup first8,000 step decay vaswani clip maximum gradient norm2.0 also utilized training model aretrained gpus tesla v100 stepswith gradient accumulation every four step weapply dropout probability linear layer model number hiddenunits model feed-forwardhidden size number head number transformer encode layer graphencoding layer graph decoding layer setas respectively parameter setas tune validation dataset duringdecoding beam search beam size andlength penalty factor trigram block isused reduce repetitions.for model pretrained apply different optimizers pretrained part andother part lapata twoadam optimizers β1=0.9 β2=0.999 areused pretrained part part respectively learning rate warmup stepsfor pretrained part part modelconfigurations line correspondingpretrained choose base version ofbert roberta xlnet experiments.4.2 evaluation resultswe evaluate model wikisum andmultinews datasets validate efficiency ofthem different type corpus summa6238model r-llead evaluation result wikisum test setusing rouge abbreviationsfor rouge-1 rouge-2 rouge-l respectively.rization quality evaluate rouge linand report unigram bigramoverlap rouge-1 rouge-2 system summary gold reference mean ofassessing informativeness long commonsubsequence rouge-l2 mean accessingfluency.results wikisum table summarize theevaluation result wikisum dataset severalstrong extractive baseline abstractive baseline also evaluate compare ourmodels first block table show theresults extractive method lead lexrank erkan radev second blockshows result abstractive method flat transformer transformer-based encoderdecoder model flat token sequence tdmca best perform model hierarchical transformer modelwith hierarchical transformer encoder flattransformer decoder propose lapata report result follow andlapata last block show resultsof model paragraph token input result showthat abstractive model outperform extractive compared t-dmca model graphsum achieve significant improvement three metric demonstrate theeffectiveness model.furthermore develop several strong base2for fair comparison previous work lapata,2019a report summary-level rougel result datasets sentence-level rougel result report appendix.model r-llead similarity +roberta topic +roberta discourse +roberta evaluation result multinews test set.we report summary-level rouge-l value theresults different graph type also compared.lines combine flat transformer withpre-trained replace encoder ftby base version pre-trained include bert+ft xlnet+ft roberta+ft forthem source input truncate token result show pre-trained significantly improve summarization performance asroberta boost summarization performancemost significantly also combine ourgraphsum model namely graphsum+roberta result show graphsum+roberta improve summarization performance onall metric demonstrate graph model canbe effectively combine pre-trained thesignificant improvement roberta+ft alsodemonstrate effectiveness graph model even pre-trained lms.results multinews table summarize theevaluation result multinews dataset similarly first block show popular extractive baseline second block show severalstrong abstractive baseline report resultsof lead lexrank pg-brnn himap follow fabbri last block showsthe result model result show thatour model graphsum consistently outperform allbaselines demonstrate effectiveness model different type corpus wealso compare performance roberta+ftand graphsum+roberta show ourmodel significantly improve metrics.3longer input achieve obvious improvements.4as xlnet bert achieve result thanroberta report result graphsum+roberta6239len model r-l500ht +0.47 +0.41 +0.34800ht +0.29 +0.41 +0.311600ht +0.95 +1.00 +0.902400ht +0.95 +1.17 +1.243000ht +0.65 +0.89 +0.84table comparison different input length thewikisum test rouge indicate theimprovements graphsum ht.the evaluation result wikisumand multinews dataset validate effectiveness model propose method model graph end-to-end neural model greatly improve performance mds.4.3 model analysiswe analyze effect graph type andinput length model validate effectiveness different component model byablation studies.effects graph types study effect ofgraph type result graphsum+robertawith similarity graph topic graph discoursegraph compare multinews test thelast block table summarize comparison result show topic graph achieve good performance similarity graph rouge-1and rouge-2 discourse graph achievesthe best performance rouge-2 rougel result demonstrate graph richerrelations helpful mds.effects input length different length input affect summarization performance seriously seq2seq model restrictthe length input model withhundreds lead token state andlapata model achieve bestperformance input length rouge-1 rouge-2 rouge-lgraphsum graph graph ablation study wikisum test set.tokens long input hurt performance toexplore effectiveness graphsum modelon different length input compare withht token ofinput respectively table summarize comparison result show model outperform length input importantly advantage model three metricstend become large input become longer.the result demonstrate model graph theend-to-end model enable model process muchlonger input good performance.ablation study table summarize resultsof ablation study validate effectiveness individual component experimentsconfirmed incorporate well-known graphsinto encode process graph encoder seew/o graph utilize graph guide thesummary decode process graph decoder graph beneficial mds.4.4 human evaluationin addition automatic evaluation alsoaccess system performance human evaluation.we randomly select test instance wikisum test multinews test invite annotator access output different model independently annotators accessthe overall quality summary rank themtaking account following criterion informativeness summary convey importantfacts input fluency summaryfluent grammatical succinctness thesummary avoid repeat information annotatorsare rank system best ranking could differentsystems similar quality example ranking five system could system score rank respectively rating ofeach system compute average scoreson test instances.table summarize comparison result offive system percentage rank results6240model ratingft ranking result system summary human evaluation best thelarger rating denote well summary quality andg.s abbreviation roberta graphsum respectively indicate overall rating corresponding model significantly welch t-testwith outperform model graphsumand graphsum+roberta.and overall rating report result demonstrate graphsum graphsum+robertaare able generate high quality summary thanother model specifically summary generate graphsum graphsum+robertausually contain salient information aremore fluent concise model thehuman evaluation result validate effectiveness propose models.5 conclusionin paper explore importance graphrepresentations propose leveragegraphs improve performance neural abstractive propose model able incorporate explicit graph representation thedocument encode process capture rich relation within long input utilize explicit graphstructure guide summary decoding processto generate informative fluent concisesummaries also propose effective methodto combine model pre-trained whichfurther improve performance significantly experimental result show modeloutperforms several strong baseline wide margin future would like explore othermore informative graph representation asknowledge graph apply improve summary quality.acknowledgmentsthis work support national research development project china no.2018aaa0101900
many natural language question require qualitative quantitative logical comparison entity event paper address problem improve accuracyand consistency response comparisonquestions integrate logic rule neural model method leverage logical andlinguistic knowledge augment labeled training data consistency-basedregularizer train model improvingthe global consistency prediction approach achieve large improvement previous method variety question answering task include multiple-choice qualitative reasoning cause-effect reasoning andextractive machine read comprehension inparticular method significantly improvesthe performance roberta-based modelsby across datasets advance state ofthe around wiqa quareland reduce consistency violation onhotpotqa demonstrate ourapproach learn effectively limiteddata.11 introductioncomparison-type question tandon tafjord yang relationship property entity eventssuch cause-effect qualitative quantitative reasoning create comparison question require inferential knowledge reason ability annotator need understand context presentedin multiple paragraph carefully grind question give situation make challenge annotate large number comparisonquestions current datasets comparisonquestions much small standard machinereading comprehension datasets rajpurkar1our code data available http //github.com/akariasai/logic_guided_qa.q ceramic vase less flexiblethan plastic ball wasa breakableq ceramic vase flexiblethan plastic ball wasa less breakableq silent outer collectless sound wave positive causal relationship outer collect less soundwaves less sound detect positive causal relationship silent less sound beingdetected positive causal relationship robertamorebreakablemorebreakablerobertamoremorelessconflictconflictfigure inconsistent prediction roberta toprow show example symmetric inconsistency andthe second show example transitive inconsistency example partially modified.et joshi pose newchallenges standard model knownto exploit statistical pattern annotation artifactsin datasets sugawara al.,2019a importantly state-of-the-art model showinconsistent comparison prediction show infigure improving consistency predictionshas previously study natural language inference task minervini riedel address qa.in paper address task produce globally consistent accurate prediction forcomparison question leverage logical symbolic knowledge data augmentation training regularization data augmentation aset logical linguistic knowledge developadditional consistent label training data subsequently method symbolic logic incorporate consistency regularization additionalsupervision signal beyond inductive bias givenby data augmentation method generalizesprevious consistency-promoting method nlitasks minervini riedel adapt substantially different question formats.our experiment show significant improvementover state variety task classification-based causal reason multiple choice qualitative reasoning anextractive task comparison entity notably data augmentation consistency constrain train regularization improvesperformance roberta-based model al.,2019 wiqa quareland hotpotqa approach advance stateof-the-art result wiqa quarel absolute accuracy improvement respectively reduce inconsistent prediction demonstrate approach learn effectively limit label data give original label data method achievesperformance competitive baselinelearned full label data.2 related workdata augmentation explore variety oftasks domain krizhevsky cubuket park backtranslation dictionary basedword replacement zhang beenstudied relevant work kang study nli-specific logic knowledgebased data augmentation concurrent work gokhale study visual model ability answer logically composed question andshow effectiveness logic-guided data augmentation data augmentation rely ontask-specific assumption adapt todifferent format task leverageconsistency-promoting regularization givesimprovements accuracy consistency.improving prediction consistency trainingregularization study task minervini riedel present model-dependentfirst-order logic guide adversarial example generation regularization introduceconsistency-based regularization incorporate thefirst-order logic rule previous approach modeldependent relies nli-specific rule ourmethod model-agnostic generally applicable combine data augmentation.regularizing loss penalize violation structural constraint model output alsostudied previous work constraint satisfactionin structure learning ganchevet work regularize model produce globally consistent prediction among augment data follow logical constraint whilethose study incorporate structured predictionmodels follow linguistics rules.3 methodwe present component method first-order logic guide data augmentation section section consistency-basedregularization section consistent question answeringfor globally consistent prediction require response follow important generallogical rule symmetric consistency transitiveconsistency illustrate figure andare formally describe below.let question paragraph ananswer predict model answercandidates element span class category arbitrary answer choice.x represent logic atom.symmetric consistency comparison question small surface variation replacingwords antonym reverse answer keep overall semantics question define symmetry questionsin context follow qsym a∗sym qsym antonyms ofeach a∗sym opposite groundtruth answer example question first figure symmetric pairs.we define symmetric consistency predictionsin follow logic rule qsym asym indicate system predict asym give qsym predict consistency transitive inference three predicate represent gazes al.,2012 context transitive example mainly causal reason question thatinquire effect give cause thesecond figure show example wheretransitive consistency violate question effect equal cause define thetransitive consistency prediction follow qtrans atrans quarel hotpotqa tandon tafjord yang reason causal reasoning qualitative reasoning qualitative comparison entitiesformat classification multiple choice span extractionpthe rain seeps wood surface.when rain evaporate leave wood.it take finish wood it.the wood begin lose luster.supposed stand planet earth andmercury look upin golf magazine monthly golfmagazine time inc. elnuevo cojo ilustrado american spanish language magazine.qq1 tsunami happen wood bemore moist wood moremoist weather occur planet would sunappear large nuevo cojo golf magazine time less effect mercury earth golf magazine nuevo cojo mercury golf magazineqaugif tsunami happens weather occur planet would sunappear small timeinc golf magazine nuevo cojo a∗aug earth nuevo cojotable augmented transitive example wiqa symmetric example quarel hotpotqa wepartially modify paragraph question bold character denote shared event connect questions.the part write blue denote antonym highlight text negation data augmentation.3.2 logic-guided data augmentationgiven training example formof automatically generate additionalexamples xaug qaug a∗aug symmetry transitivity logical rule goal toaugment training data symmetric andtransitive example observe training.we provide augmented example table symmetric example create asymmetric question convert question intoan opposite following operation replace word antonym remove word select frequentadjectives verb polarity e.g. small increase train corpus expert annotator write antonym frequent word denote small dictionary detail appendix negation word remove negation word e.g. question training data question include word operation match template e.g. operation apply operation generate qsym.2 weobtain a∗sym re-labeling answer opposite answer choice appendix transitive example first find apair cause-effect question whose consist of2we observe less effective wiqaor quarel especially contribute performanceimprovements hotpotqa much hold whena∗1 positive causal relationship create newexample xtrans augment data adding consistent example change data distributionfrom original lead deterioration performance onecan select data base model predictioninconsistencies minervini riedel orrandomly sample epoch kang work randomly sample augment dataat beginning training example epoch training despite itssimplicity yield competitive even betterperformance sample strategies.33.3 logic-guided consistency regularizationwe regularize learning objective task loss ltask regularization term promote consistency prediction consistency loss lcons ltask lcons xaug first term ltask penalize make incorrect prediction second term lcons4 penalize makingpredictions violate symmetric transitivelogical rule follow lcons λsymlsym λtransltrans λsym λtrans weight scalar tobalance consistency-promoting objectives.3we xaug pair already exist.4we mask lcons example without symmetricor transitive consistent examples.5645dataset wiqa quarel hotpotqadev test test data sota –roberta wiqa quarel hotpotqa result report test development accuracy wiqa andquarel development hotpotqa denote data augmentation consistency regularization. sota tandon wiqa mitra quarel present violation consistency.previous study focus consistency calculate prediction inconsistency pair example swap premise hypothesis directly apply task instead ourmethod leverage consistency data augmentation create paired example base generallogic rule enable application consistency regularization variety tasks.inconsistency loss loss compute dissimilarity predicted probability theoriginal label answer augment data define follow lsym |log −log aaug|qaug likewise transitive loss absoluteloss product t-norm project logical conjunction operation product probability operation a1|q1 a2|q2 follow calculate transitive consistency loss ltrans |log a1|q1 a2|q2 −log atrans|qtrans |.annealing model prediction beaccurate enough beginning train forconsistency regularization effective perform anneal kirkpatrick al.,2019 first trans train model epoch andthen train full objective.4 experimentsdatasets experimental setting experiment three datasets wiqa tandon al.,2019 quarel tafjord hotpotqa oracle comparison questions5 yang train model bridge comparison question evaluate extractive comparison question only.wiqa quarelmetric logic logic standard –baseline ablation study data augmentation onwiqa quarel development dataset.as show table three datasets substantially different term require reasoning ability task format wiqa symmetric example transitive example symmetric pair and1,609 transitive triple miss original training data hotpotqa quarel nothave training pair require consistency ourmethod randomly sample augment data wiqa quarel hotpotqa result newly create training example datasets respectively.we standard score performance evaluation hotpotqa accuracyfor wiqa quarel report violation ofconsistency follow minervini riedel evaluate effectiveness approach forimproving prediction consistency computethe violation consistency metric percentage example agree symmetricand transitive logical rule model experimental detail appendix.main results table demonstrate ourmethods constantly give point improvement state-of-theart roberta performance three ofthe datasets advance state-of-the-art scoreson wiqa quarel respectively three datasets method signifi5646wiqa input roberta da+regp sound enter person sound drum inside ears.q person protected less sound detect qsym person less protect less sound detect asym∗ less less squirrels much possible squirrel gain weight.q1 weather snow squirrels much possible less squirrel much possible squirrel gain weight qtrans weather snow squirrel gain weight a∗trans less hotpotqa comparison input roberta reeves eason film director actor screenwriter albert rogell film director.q scope profession reeves eason albert rogell reeves eason reeves eason reeves easonqsym less scope profession reeves albert rogell a∗sym albert rogell reeves eason albert rogelltable qualitative comparison roberta example partially modified.cantly reduce inconsistency prediction demonstrate effect data augmentation regularization component notably onwiqa roberta show violation consistencyin symmetric example ofthe transitive example approach reduce theviolations symmetric transitive consistenciesto respectively.results limited training data table alsoshows approach especially effective scarce training data setting label data available together give absolute accuracy improvement roberta baselineson wiqa quarel respectively.ablation study analyze effectiveness ofeach component table improve baseline combination performsthe best wiqa quarel standard follow previous standard data augmentation technique paraphrase word verb adjective linguistic knowledge namely wordnet miller incorporate logical rule importantly standard givenotable improvement baseline model bothin accuracy consistency suggest thatlogic-guided augmentation give additional inductive bias consistent beyond amplify thenumber train data wiqa consist sometransitive symmetric example also reportthe performance wiqa performance improvement small demonstratingthe importance combine da.qualitative analysis table show qualitativeexamples compare method robertabaseline qualitative analysis show thatda+reg reduce confusion oppositechoices assigns large probability theground-truth label question dashows relatively small probability differences.on hotpotqa baseline model show largeconsistency violation show table hotpotqa example table show thatroberta select answer andqsym answer correctly question demonstrate robustness surface variation hypothesize baseline model exploit statistical pattern dataset bias present inquestions method reduce model stendency exploit spurious statistical pattern elkahky whichleads large improvement consistency.5 conclusionwe introduce logic guide data augmentationand consistency-based regularization frameworkfor accurate globally consistent especiallyunder limit training data setting approachsignificantly improve state-of-the-art modelsacross three substantially different datasets.notably approach advance state-of-the-arton quarel wiqa standard benchmarksrequiring rich logical language understanding.we show approach effectivelylearn extremely limited train data.acknowledgmentsthis research support n0001418-1-2826 darpa n66001-19-2-403 iis1616112 iis1252835 allen distinguished investigator award sloan fellowship nakajima foundation fellowship thank antoinebosselut dettmers koncel-kedziorski sewon keisuke sakaguchi david wadden yizhong wang member groupand anonymous reviewer theirinsightful feedback.5647
present problem grounding natural language instruction mobile user interface action create three datasets forit full task evaluation create pixelhelp corpus pair english instruction action perform people amobile emulator scale training decouple language action data annotate action phrase span howto instruction synthesizing ground description action mobile user interface weuse transformer extract action phrase tuples long-range natural language instruction ground transformer contextually represent object theircontent screen position connects themto object description given start screenand instruction model achieve accuracy predict complete ground-truthaction sequence pixelhelp.1 introductionlanguage help work together thing done.people instruct another coordinate joint effort accomplish task involve complex sequence action take advantage ability different member speech community child parent cannotreach visually impaired individual forassistance friend building computationalagents able help interaction important goal require true language grounding inenvironments action matters.an important area language ground involve task like completion multi-step action graphical user interface condition languageinstructions branavan al.,2018 domain matter foraccessibility language interface could helpvisually impaired individual perform task withopen drawer navigate tosettings network internet wifi click network andthen enter starbucks ssid.action phraseextraction modelscreen operation object argumentscreen_1 click obj_2screen_2 click obj_6screen_3 click obj_5 screen_6 input obj_9 starbucks instructionsoperation_desc object_desc argument_desc open drawer navigate setting navigate network internet navigate wifi click network enter ssid starbucks executable action base thescreen stepaction phrase tuplesgrounding modeltransition tonext screen…mobile userinterface ateach stepfigure model extract phrase tuple describe action include operation object andadditional argument ground tuples executable action sequence ui.interfaces predicate sight alsomatters situational impairment sarsenbayeva,2018 access device easily whileencumbered factor cooking.we focus domain task automation inwhich natural language instruction must interpret sequence action mobile touchscreen existing search quite capable ofretrieving multi-step natural language instructionsfor user query turn flightmode android. crucially miss piecefor fulfil task automatically thereturned instruction sequence action thatcan automatically execute device with8199little user intervention goal paper.this task automation scenario require auser maneuver detail useful average user especially valuable forvisually situationally impaired user ability execute instruction also useful forother scenario automatically examiningthe quality instruction.our approach figure decompose problem action phrase-extraction step agrounding step former extract operation object argument description multi-step instruction transformers vaswaniet test three span representations.the latter match extract operation objectdescriptions object screen transformer contextually representsui object ground object description them.we construct three datasets assess fulltask performance naturally occur instruction create dataset multi-step englishinstructions operating pixel phones producetheir correspond action-screen sequence usingannotators action phrase extraction trainingand evaluation obtain english how-to instruction annotate action descriptionspans transformer span represent bysum pooling obtain accuracy predict span sequence completelymatch ground truth train groundingmodel synthetically generate single-stepcommands action cover differentui object across mobile screens.our phrase extractor ground model together obtain partial complete accuracy match ground-truth actionsequences challenging task also evaluate alternative method representation object span present qualitative analysis toprovide insight problem models.2 problem formulationgiven instruction multi-step task token instruction want generate sequence automatically executable action sequenceof user interface screen initial screen s11our data pipeline available http //github.com google-research google-research /tree/master/seq2act.and screen transition function sj=τ aj−1 sj−1 m|s1 =m∏j=1p action consist operation text object ojthat perform e.g. button icon additional argument need e.g.the message enter chat text ornull operation starting froms1 execute sequence action arrive atscreen represent screen step aj−1 m|s1 =m∏j=1p screen cj,1 contain setof object structural relationships.cj,1 isthe number object choose define structural relationship betweenthe object often tree structure theview hierarchy android interface2 similarto tree page instruction describe possibly multiple action denote phrase describesaction represent tuple ofdescriptions correspond span—asubsequence tokens—in accordingly mrepresents description tuple sequence werefer brevity also define possible description tuple sequence thus ā.p =∑āp ā|sj independent rest instruction give current screen descriptionāj related instruction wecan simplify =∑āp |āj ā|t1 developer android /reference/android/view/view.html8200we define likely description ofactions n.â maxāp ā|t1 maxā1 mm∏j=1p define action phrase-extraction model ground model |âj m|t1 ≈m∏j=1p |âj identify description tuples foreach action |âj ground descriptionto executable action give screen.3 datathe ideal dataset would natural instructionsthat execute people ui.such data collect annotatorsperform task accord instruction mobileplatform difficult scale requiressignificant investment instrument different version apps different presentation behavior apps must instal configuredfor task create small datasetof form pixelhelp full task evaluation.for model training scale create otherdatasets androidhowto action phrase extraction ricosca ground datasetsare target english hope start witha high-resource language pave create similar capability languages.3.1 pixelhelp datasetpixel phone help pages3 provide instruction forperforming common task google pixel phonessuch switch wi-fi setting checkemails help page contain multiple task witheach task consisting sequence step wepulled instruction help page keptones automatically execute instructions require additional user input astap want uninstall discarded.3https //support.google.com/pixelphonefigure pixelhelp example open device ssettings network internet click wi-fi.turn wi-fi instruction pair action show specific screen.also instruction involve action physicalbutton press power button fewseconds exclude event cannotbe execute mobile platform emulators.we instrument logging mechanism apixel phone emulator human annotatorsperform task emulator follow thefull instruction logger record every user action include type touch event trigger object manipulate screeninformation view hierarchy itemthus include instruction input screenfor step task target actionperformed screen m.in total pixelhelp include multi-step instruction task category general task configure account gmail task task photos related task thenumber step range eight witha median four natural instruction ground action reserve pixelhelp evaluate full task performance.3.2 androidhowto datasetno datasets exist support learn actionphrase extraction model mobile address extract englishinstructions operating android device process page identify candidate instruction how-to question changethe input method android crawl service scrape instruction-like content variouswebsites filter content bothheuristics manual screening annotators.annotators identify phrase instructionthat describe executable action givena tutorial task instruct skipinstructions difficult understand label.for component action description they8201select span word describe component annotation interface detail areprovided appendix interface recordsthe start position marked span.each instruction label three annotator three annotator agree full instructionsand least agree consistencyat tuple level agreement across annotator operation phrase forobject phrase input phrase thediscrepancies usually small e.g. descriptionmarked gmail address gmail address.the final dataset include data pointsfrom unique how-to instruction splitinto training validation test test example perfect agreement acrossall three annotator entire sequence intotal operation span objectspans input span label lengths ofthe instruction range token withmedian describe sequence actionsfrom step median ricosca datasettraining grounding model |âj involve pair action tuples along screen sjwith action description difficult collect data scale past bottleneck exploit property task generatea synthetic command-action dataset ricosca.first precise structure visual knowledge layout spatially relate uielements overall screen second grammar ground cover manyof command kind reference need forthe problem capture manner ofinteracting conversationally proveseffective train grounding model.rico public corpus android uiscreens mine android apps deka al.,2017 screen rico come screenshot image view hierarchy collection ofui object individual object setof property include name often englishphrase send type e.g. button imageor checkbox bound position thescreen manually remove screen whose viewhierarchies match screenshots annotator visually verify whether bound view hierarchy leave match uiobject corresponding screenshot image thisfiltering result unique screens.for screen randomly select elementsas target object synthesize command operate generate multiple command tocapture different expression describe operation target object example thetap operation refer click orpress template refer target objecthas slot name type location areinstantiated following strategy name-type target name and/or type theok button absolute-location target screen location menu right corner relative-location target relative location object icon right ofsend command synthesize spanthat describe part action respectto know meanwhile actualaction associated screen present constituent action synthesized.in total ricosca contain single-stepsynthetic command operate different target object across android screens.4 model architecturesequation part findsthe best phrase tuple describe action atthe step give instruction token sequence.p |âj compute probability executable action give best description theaction screen step.4.1 phrase tuple extraction modela common choice model conditionalprobability equation areencoder-decoders lstms hochreiter andschmidhuber transformers vaswaniet output model correspondsto position input sequence architecture closely relate pointer networks vinyalset depict model encoder compute latent representation n∈rn×|h| thetokens embeddings decoder generate hidden stateqj=f compute aquery vector locate phrase tuple step āj= they8202open apptransformer encodereos…drawer navigate settings …span encodinginstructionencoderstarttransformer decoder……span querydecoder hiddenspan inputdecoder⌀⌀⌀⌀⌀⌀…rjqojqujqb dhitjqfigure phrase tuple extraction model encode instruction token sequence output tuplesequence query possible span encoded sequence tuple contain span position ofthree phrase instruction describe action operation object optional argument respectively ateach step indicate phrase miss instruction represent special span encoding.are assume conditionally independent give previously extract phrase tuples instruction =∏ȳ∈ denote specificspan action tuple step j.we therefore rewrite explicitly indicatethat correspond span startingat position position inthe instruction parameterizethe conditional probability softmax show figure indicate task-specificquery vector computedas qyj=φ multi-layer perceptron follow linear transformation aretrainable parameter separate parametersfor r|φy |×|h| |φy|is output dimension multi-layer perceptron alignment function score aquery vector match span whose vector representation compute encoding d.span representation quadraticnumber possible span give token sequence important designa fixed-length representation variablelength token span quickly computed.beginning-inside-outside ramshaw andmarcus –commonly indicate spansin task name entity recognition–markswhether token begin inside outsidea span however ideal task subsequence describe different actionscan overlap e.g. click click participatesin action click click experiment consider several recent flexiblespan representation liet show impact section fixed-length span representation canuse common alignment technique neural network bahdanau luong product query vectorand span representation =qyj step decode previouslydecoded phrase tuples decoder wecan concatenation vector representation three element phrase tuple orthe vector representation inputfor decode step entire phrase tupleextraction model train minimize softmax cross entropy loss predict andground-truth span sequence phrase tuples.4.2 grounding modelhaving compute sequence tuples bestdescribe action connect executable action base screen stepwith ground model step-bystep instruction part action oftenclearly state thus assume probabilitiesof operation object argument are8203openuiobjectsapp drawertransformer encoder……object obj4 operation click obj1 obj2obj3 obj4obj5 obj45⌀argument none navigate settingstransformer encoder…object obj3 operation click ⌀argument none objectembeddingscreenencoderobjectencodinggroundedactions…eosobject none operation stop ⌀argument none ⌀initial screentransformer encoder…userinterfacescreen…obj1 obj2obj3 obj4obj5 obj9screen obj2obj3 obj4obj5 obj20final screenextractedphrasetuplesfigure grounding model ground phrase tuple extract phrase extraction model operationtype screen-specific object argument present base contextual representation object forthe give screen grounded action tuple automatically executed.independent give description screen.p |âj |r̂j |ôj |ûj |r̂j |ôj simplify assumption operation often fully describe instruction without rely screen information mobile interaction task argument presentfor text operation uj=ûj parameterize |r̂j feedforward neural network |r̂j softmax r̂′j multi-layer perceptron trainable parameter r∈r|φr|×|r| also trainable where|φr| output dimension and|r| vocabulary size operation take embed vector eachtoken operation description input r̂′j=∑dk=b start andend position instruction.determining select object avariable-number object screen sjwhere base give object description parameterize conditional probability deep neural network softmax outputlayer take logits alignment function |ôj k|ôj cj,1 softmax ô′j alignment function score object description vector ô′j match latent representation object assimple product vector latent representation ô′j acquire multi-layerperceptron follow linear projection ô′j d∑k=be start index objectdescription trainable parameter wo∈r|φo|×|o| |φo| outputdimension dimension ofthe latent representation object description.contextual representation objects tocompute latent representation candidateobject object propertiesand context i.e. structural relationship withother object screen differentways encode variable-sized collection ofitems structurally relate include graph convolutional networks niepert transformers vaswaniet gcns adjacency matrix predetermine structure regulate thelatent representation object affectedby neighbor transformers allow objectto carry positional encoding relationship object learn instead.the input transformer encoder combination content embedding positionalencoding object content propertiesof object include name type compute content embedding concatenate thename embedding average embeddingof token object name the8204type embedding positional property anobject include spatial position structural position spatial position include thetop leave right bottom screen coordinate ofthe object treat coordinate adiscrete value represent embedding.such feature representation coordinate wasused imagetransformer represent pixel position image parmar spatialembedding object fourcoordinate embeddings encode structural information index position object inthe preorder postorder traversal viewhierarchy tree represent index positionsas embeddings similar represent coordinate content embedding summedwith positional encoding form embed ofeach object object embeddingsinto transformer encoder model compute thelatent representation object k.the ground model train minimizingthe cross entropy loss predict andground-truth object loss predict ground-truth operation.5 experimentsour goal develop model datasets tomap multi-step instruction automatically executable action give screen information assuch pixelhelp pair natural instruction action-screen sequence solely testing.in addition investigate model quality onphrase tuple extraction task crucialbuilding block overall grounding quality4.5.1 datasets metricswe metric measure predictedtuple sequence match ground-truth sequence.• complete match score sequence length theidentical tuple step otherwise partial match number step predicted sequence match ground-truthsequence divide length groundtruth sequence range train validate androidhowtoand ricosca evaluate pixelhelp training single-step synthetic command-action4our model code release http github google-research google-research /tree/master/seq2act.span rep. partial completesumpooling∑dk=b androidhowto phrase tuple extractiontest result different span representation d=∑dk=b learnedweight function token embedding al.,2017 pseudocode fast computation ofthese appendix.examples dynamically stitch form sequence example certain length distribution.to evaluate full task complete andpartial match grounded action sequence mwhere token vocabulary size compile instruction corpus uiname corpus type include object type type catch allless common output vocabulary operation include click text swipe eos.5.2 model configurations resultstuple extraction action-tuple extractiontask transformer theencoder decoder evaluate three different span representation area attention al.,2019 provide parameter-free representation ofeach possible span one-dimensional area encoding token subsequence =∑dk=b representation ofeach span compute constant time invariant length span areatable previous work concatenate encode ofthe start token span representation generalized version evaluatedthese three option implement representation areatable similar approach area attention forfast computation hyperparameter tune andtraining detail refer appendix.table give result androidhowto testset span representation perform well encodings token transformer alreadycapture sufficient information entire sequence even start encoding yield strong result nonetheless areaattention provide small boost others dataset also considerable headroomremaining particularly complete match.8205screen encoder partial completeheuristic pixelhelp ground accuracy difference statistically significant base t-test grounding task compare transformer-based screen encoder generate object representation baselinemethods base graph convolutional networks.the heuristic baseline match extract phrasesagainst object name directly bleu scores.filter-1 performs graph convolution withoutusing adjacent node object representation object compute base itsown property distance distancebetween object view hierarchy i.e. number edge traverse object anotherfollowing tree structure contrast thetraditional definition base adjacency butis need object often leave thetree adjacent otherstructurally instead connect nonterminal container node filter-1 anddistance number parameter appendix detail train grounding model first train thetuple extraction sub-model androidhowtoand ricosca latter language relatedfeatures command tuple position command stage screen actionfeatures involve freeze tuple extraction sub-model train groundingsub-model ricosca commandand screen-action related feature screen token embeddings ground sub-model shareweights tuple extraction sub-model.table give full task performance pixelhelp transformer screen encoder achievesthe best result accuracy completematch partial match setsa strong baseline result dataset whileleaving considerable headroom gcn-basedmethods perform poorly show importance contextual encoding informationfrom object screen distance gcndoes attempt capture context object thatare structurally close however suspect thedistance information derive viewhierarchy tree noisy developer canconstruct structure differently ui.5as result strong bias introduce structure distance always help nevertheless model still outperform heuristic baseline achieve partial match and42.25 complete match.5.3 analysisto explore model ground instructionon screen analyze relationship betweenwords instruction language refer specific location screen actual positionson screen first extract embeddingweights trained phrase extraction modelfor word bottom leave right thesewords occur object description checkbox screen also extract embed weight object screen position whichare create object positional encoding wethen calculate correlation word embedding screen position embed cosinesimilarity figure visualize correlation aheatmap brighter color indicate high correlation word strongly correlate withthe screen trend locationwords less clear leave strongly correlatedwith left side screen region thescreen also show high correlation likelybecause left right refer absolute location screen also forrelative spatial relationship icon theleft button bottom strong correlation occur bottom screenbecause many object dataset fallin region region often reserve system action on-screen keyboard arenot cover dataset.the phrase extraction model pass phrase tuplesto grounding model phrase extractionis incorrect difficult groundingmodel predict correct action mitigate cascading error hidden stateof phrase decode model step intuitively compute access theencoding token instruction thetransformer encoder-decoder attention can5while possible directly screen visual data forgrounding detect object pixel nontrivial.it would ideal structural visual data.8206figure correlation location-related wordsin instruction object screen position embedding.potentially robust span representation.however early exploration find thatgrounding performs stunningly well forgrounding ricosca validation example performs poorly pixelhelp learned hiddenstate likely capture characteristic syntheticinstructions action sequence manifest pixelhelp hidden stateto ground remain challenge learning fromunpaired instruction-action data.the phrase model fail extract correct stepsfor task pixelhelp particular result extra step task extract incorrect step task skip step forany task error could cause different language style manifest three datasets.synthesized command ricosca tend bebrief instructions androidhowto seem togive contextual description involve diverse language style pixelhelp often hasa consistent language style give concisedescription step.6 related workprevious work branavan liuet investigate approach ground natural language desktop interface manuvinakurike contribute dataset natural languageinstructions actionable image edit commandsin adobe photoshop work focus newdomain ground natural language instructionsinto executable action mobile user interfaces.this require address model challenge dueto lack paired natural language actiondata supply harvest rich instruction data synthesize command base large scale android corpus.our work relate semantic parsing particularly effort generate executable output suchas query suhr also broadlyrelated language grounding human-robotinteraction literature human dialog result inrobot action khayrallah task setting closely related work onlanguage-conditioned navigation agentexecutes instruction sequence movement chen mooney misraet anderson chen al.,2019 operating user interface similar navigate physical world many mobile platform consist million apps eachis implement different developer independently though platform android strive toachieve interoperability e.g. intent aidlmechanisms apps often build byconvention expose programmatic waysfor communication opaque tothe outside world manipulate itis guis hurdle workingwith vast array exist apps like physical obstacle ignore must benegotiated contextually give environment.7 conclusionour work provide important first step thechallenging problem ground natural languageinstructions mobile action decomposition problem mean progress eithercan improve full task performance example action span extraction relate semanticrole labeling extraction ofmultiple fact text jiang andcould benefit innovation span identification multitask learning reinforcement learning apply previous groundingwork help improve out-of-sample predictionfor grounding improve direct grounding hidden state representation lastly ourwork provide technical foundation investigate user experience language-based humancomputer interaction.acknowledgementswe would like thank anonymous reviewersfor insightful comment improve thepaper many thanks google data computeteam especially ashwin kakarla muqthar mohammad help annotation andsong wang justin christina theirhelp early data preprocessing.8207
paper propose multi-granularityinteraction network extractive abstractive multi-document summarization whichjointly learn semantic representation forwords sentence document wordrepresentations generate abstractive summary sentence representation produce extractive summary employ attention mechanism tointeract different granularity semantic representation help capture multi-granularity information improve performance abstractive andextractive summarization experiment resultsshow propose model substantiallyoutperforms strong baseline method andachieves best result multi-newsdataset.1 introductiondocument summarization produce fluent condense summary give document single document summarization show promisingresults sequence-to-sequence model encode source document decode intoa summary paulus gehrmann çelikyilmaz summarization require producinga summary cluster thematically relateddocuments give document complement overlap multi-document summarization involve identify important information filter redundant information frommultiple input sources.there primary methodology multidocument summarization extractive abstractive extractive method directly select importantsentences original relativelysimple rank sentence arecursive neural network yasunaga employ graph convolutional network toincorporate sentence relation graph improvethe performance extractive summarization.abstractive method generate word andnew sentence technically difficultthan extractive method work multidocument summarization simply concatenate multiple source document long flat sequence andmodel multi-document summarization longsequence-to-sequence task fabbri however approach ttake hierarchical structure document cluster account too-long input oftenleads degradation document summarization cohan lapata recently hierarchical framework show theireffectiveness multi-document summarization zhang lapata theseapproaches usually multiple encoders modelhierarchical relationship discourse structure method incorporate structural semantic knowledge explore thecombination extractive abstractive beenexplored single document summarization chenand bansal extracted sentence asthe input abstractive summarization subramanian concatenate extractedsummary original document input ofthe abstractive summarization.in work treat document sentence word different granularity semantic unit connect semantic unit within athree-granularity hierarchical relation graph withthe multi-granularity hierarchical structure canunify extractive abstractive summarization intoone architecture simultaneously extractive summarization operate sentence-granularity anddirectly supervise sentence representationswhile abstractive summarization operate wordgranularity directly supervise word repre6245sentations propose novel multi-granularity interaction network enable supervision promote learning granularity representations.we employ attention mechanism encode therelationships semantic granularity hierarchical relationship thedifferent semantic granularity respectively andwe fusion gate integrate various relationship update semantic representations.the decode part consist sentence extractorand summary generator sentence extractorutilizes sentence representation select sentence summary generator utilizes theword representation generate summary thetwo task train unified architecture topromote recognition important informationsimultaneously.we evaluate model recently releasedmulti-news dataset propose architecture bring substantial improvement severalstrong baseline explore influence semantic unit different granularity ablation study show joint learning extractiveand abstractive summarization unified architecture improve performance.in summary make following contribution paper establish multi-granularity semantic representation document sentence andwords propose novel multi-granularityinteraction network encode multiple inputdocuments.• approach unify extractive abstractive summarization architecturewith interactive semantic unit promotethe recognition important information indifferent granularities.• experimental result multi-newsdataset show approach substantiallyoutperforms several strong baseline andachieves state-of-the-art performance ourcode publicly available http //github.com/zhongxia96/mgsum.2 related workthe method multi-document summarizationcan generally categorize extractive abstractive extractive method produce summary extract merge sentence fromthe input document abstractive method generate summary arbitrary wordsand expression base understanding thedocuments lack available trainingdata previous multi-document summarizationmethods extractive erkan radev christensen yasunaga neural abstractive model haveachieved promising result single-documentsummarization paulus gehrmann çelikyilmaz work train abstractive summarizationmodels single document dataset adjustedthe model adapt multi-document summarization task zhang document setencoder single document summarizationframework tune pre-trained model themulti-document summarization dataset lebanoffet combine extractive summarization algorithm sentence extraction toreweight original sentence importance distribution learn single document abstractivesummarization model recently large scalemulti-document summarization datasets beenproposed long input generate wikipedia anotherdedicated generate comprehensive summarization multiple real-time news fabbri al.,2019 concatenate multiplesource document long flat text introduce decoder-only architecture scalably attend long sequence much longerthan typical encoder-decoder architecture andlapata introduce intermediate documentrepresentations simply document representation word representation modelingthe cross-document relationship compared withour propose multi-granularity method andlapata incline traditional bottomup hierarchical method effectively utilize hierarchical representation ignoringthe hierarchical relationship sentence fabbriet incorporate hierarchicalpointer-generator network address information redundancy multi-document summarization.3 approachour model consist multi-granularity encoder sentence extractor summary generator firstly multi-granularity encoder readsmultiple input document learn multi6246granularity representation word sentence document self-attention mechanism employ capture semantic relationship therepresentations granularity crossattention mechanism employ information interaction representation withdifferent granularity fusion gate integrate information different attentionmechanisms sentence extractor scoressentences accord learned sentence representation meanwhile summary generatorproduces abstractive summary attend tothe word representation following section describe multi-granularity encoder thesentence extractor summary generator respectively.3.1 multi-granularity encodergiven cluster document establish explicit representation document sentence word connect within hierarchical semantic relation graph multi-granularityencoder stack identical layer eachlayer sub-layers first multigranularity attention layer second multiple fully connect feed-forward network themulti-granularity attention sub-layer transfer semantic information different granularity granularity feed-forwardnetwork aggregate multi-granularityinformation employ multi-head attention toencode multi-granularity information afusion gate propagate semantic information toeach figure show overview themulti-granularity encoder layer figure illustrate semantic representation update take sentence representation example.let k-th word sentence document bottom encoderstack input word convert thevector representation learned embeddings.we assign positional encoding indicate position word three position needto consider namely rank document position sentence within thedocument position word within thesentence concatenate three position embed final position embed input word representation obtain simply wordduplicatewordsentencedocumentself-attentioncross-attentionfigure overview multi-granularity encoder layer.embedding position embed h0wi definition positional encode peis consistent transfomer vaswani al.,2017 convenience denote output ofl-th multi-granularity encoder layer theinput first layer symbols subscript denote word sentence document granularity respectively.both sentence representation h0si documentrepresentations h0di initialize zeros.in multi-granularity attention sub-layers word representation update information word granularity sentence granularity perform multi-head self-attention acrossthe word representation sentencehl−1wi hl−1wi k|wi contextrepresentation h̃lwi order propagate semantic information sentence granularity theword granularity duplicate sentence-aware representation←−h correspond sentencesi employ fusion gate integrate h̃lwi kand←−h updated word representationf fusion h̃lwi h̃lwi mhatt hl−1wi hl−1wi hl−1si mhatt denote multi-head attention propose vaswani fusion denotesthe fusion gate hl−1wi query hl−1wi are6247multi-headself-attentionmulti-headcross-attentionfeed-forwardfusion gatefusion gatesi,1 si,2 difigure multi-granularity encoder layer update sentence representation sentence representation update fusion gate integratethe information different granularities.the value attention fusion gateworks fusion sigmoid function parameter ∈r2∗dmodel×1 r.the sentence representation update fromthree source take sentence representation hl−1si query word representationshl−1wi hl−1wi k|wi value perform multi-head cross-attention theintermediate word-aware representation−→h l−1si multi-head self-attention across sentence representation hl−1si hl−1si perform getthe context representation h̃lsi order propagate document granularity semantic informationto sentence duplicate document-awarerepresentation←−h correspond documentdi mhatt hl−1si hl−1wi h̃lsi mhatt hl−1si hl−1si hl−1di semantic representation three sourcesare fuse fusion gate updatedsentence representation fusion fusion h̃lsi update document representation multihead self-attention across document representation hl−1d∗ hl−1di perform context representation h̃ldi meanwhile take thedocument representation hl−1di query sentence representation hl−1si keysand value perform multi-head cross-attention toget intermediate sentence-aware representation−→h fusion gate aggregate aboveoutputs h̃ldi and−→h fusion h̃ldi h̃ldi mhatt hl−1di hl−1d∗ mhatt hl−1di hl−1si feed-forward network transform multiple-granularity semantic information construct deep network residualconnection layer normalization connect adjacent layers.h̃=layernorm hl−1+ hl=layernorm consist lineartransformations relu activation between.note different layernormfor different granularity final representation hl1s feed sentence extractor hl1wis feed summary generator convenience denote hl1s hl1w ow.3.2 sentence extractorwe build classifier select sentence base onthe sentence representation multigranularity encoder classifier lineartransformation layer sigmoid activationfunction prediction score sentenceỹs oswo sigmoid function parameter ∈rdmodel×1 r.these score sort sentence ofmultiple document produce extracted summary.3.3 summary generatorthe summary generator model also stackof identical layer layer consist threeparts masked multi-head self-attention mechanism multi-head cross-attention mechanism and6248a fully connect feed-forward network theinput output multi-document summarization generally long multi-head attentiondegenerates length increase lapata following zhao idea adopt sparse attention mechanism eachquery attend top-k value accordingto weight calculate rather thanall value original attention vaswani al.,2017 hyper-parameter ensuresthat generator focus critical informationin input ignores much irrelevant information denote multi-head sparse attention asmsattn.similar multi-granularity encoder addthe positional encoding word summaryto input embed bottom decoder stack denote output l-th layeras input first layer theself-attention sub-layer mask mechanismis encode decoded information themasking mechanism ensure prediction ofthe position depend know output ofthe position t.g̃ layernorm gl−1+msattn gl−1 gl−1 cross-attention sub-layer take selfattention output query multigranularity encoder output valuesto performs multi-head sparse attention feedforward network transform theoutputs.c layernorm +msatt layernorm generation distribution targetvocabulary calculate feed output gl2tto softmax layer.pgt softmax gl2t rdmodel×dvocab rdvocab anddvocab size target vocabulary.the copy mechanism employ tackle problem out-of-vocabulary word compute copy attention εtwith decoder output input representation obtain copy distribution softmax gl2t kεtz one-hot indicator vector forwi kand rdvocab gate decoder output gl2to control generate word vocabulary orcopying word directly source text thefinal distribution mixture twodistributions gl2t sigmoid function ∈rdmodel×1 r.3.4 objective functionwe train sentence extractor summarygenerator unified architecture end-toend manner cross entropy theextractor loss generator loss.lext −1nn∑n=1 labs −1nn∑n=1log ground-truth extracted label isthe ground-truth summary number ofsamples corpus.the final loss belowlmix labs λlext hyper-parameter.4 experiment4.1 datasetwe experiment late release multi-newsdataset fabbri first largescale multi-document news summarization dataset.it contain pair training development test eachsummary average word paired witha document cluster average word discuss topic number source documentsper summary present show table whilethe dataset contains abstractive gold summary itis readily suit train extractive models.so follow work zhou extractive summary labeling construct gold-labelsequences greedily optimize rouge-2 onthe gold-standard summary.6249 source frequency source frequency2 distribution number source articlesper instance multi-news dataset.4.2 implementation detailswe model parameter base preliminaryexperiments development prune thevocabulary word sourcedocuments maximum weight copy attention replace unknown word generatedsummary dimension word embeddings hidden unit dmodel feed-forwardunits head multi-head selfattention mask multi-head sparse self-attention multi-head sparse cross-attention thenumber multi-granularity encoder layer summary decoder layer setdropout srivastava rate anduse adam optimizer initial learning rateα momentum weight decay valid losson development increase consecutive epoch learning rate halve amini-batch size hyper-parameterk given salience score predict sentence extractor apply simplegreedy procedure select sentence selectone sentence base descend order thesalience score append extracted summary summary reach word wedisallow repeat trigram paulus al.,2018 edunov beam searchwith beam size summary generator.4.3 metrics baselineswe rouge evaluate produced summary experiment followingprevious work report rouge multinews dataset compare model severaltypical baseline several baseline propose inthe late years.lead-3 extractive baseline concatenate first-3 sentence source documentas summary lexrank erkan radev rouge evaluation option r-su4lead-3 erkan radev mihalcea tarau carbonell goldstein zhang gehrmann fabbri lapata rouge evaluation result multinews test set.is unsupervised graph base method compute relative importance extractive summarization.textrank mihalcea tarau also unsupervised algorithm sentence importancescores compute base eigenvector centrality within weighted-graphs extractive sentencesummarization carbonell goldstein,1998 extract sentence ranked list thecandidate sentence base relevance andredundancy hibert zhang firstencodes sentence sentence transformer encoder encode whole document document transformer encoder.it single document summarization model andcannot handle hierarchical relationship document migrate multi-document summarization concatenate multiple source document long sequence extractive method give output token base model anattention mechanism allow system copywords source text point abstractive summarization copytransformer gehrmannet augments transformer theattention head choose randomly copy distribution hi-map fabbri expandsthe pointer-generator network model hierarchical network integrate module tocalculate sentence-level score train onthe multi-news corpus baseline hasbeen compare report fabbri release multi-news dataset andwe directly cite result methodsfrom paper lapata atransformer base model attention mechanism share information cross-document abstractive multi-document summarization used6250initially generate wikipedia reproducetheir method multi-document news summarization.4.4 automatic evaluationfollowing previous work report rouge-1 unigram rouge-2 bigram rouge-su4 skip bigram maximum distance word score metric automatic evaluation linand hovy table report result multi-news test proposedmulti-granularity model denote mgsum outperform various previous model abstractivemethod achieves score three rouge metric extractivemethod achieves score three rouge metric also seethat abstractive method perform good thanthe extractive method attribute resultto observation gold summary thisdataset tends expression summarizethe original input documents.owing characteristic news lead3 superior unsupervised extractive method extractive method achieves improvement rouge-2 comparedwith hibert attribute improvement totwo aspect firstly abstractive objective canpromote recognition important sentencesfor extractive model multi-granularityinteraction network besides extractive goldlabel sequence obtain greedily optimize rouge-2 gold-standard summary gold label accurate joint learningof objective correct bias theextractive model inaccurate label wecalculate oracle result base gold-labelextractive sequence achieve score of29.78 rouge-2 point higherthan score extractive method whilethere model oracle effort make improve extractiveperformance.among abstractive baseline copytransformer perform much good andachieves point improvement rouge2 demonstrate superiority thetransformer architecture abstractive modelgains improvement point comparedwith copytransformer point compare withhi-map point compare on2.812.892.732.983.052.952.822.97 informativeness non-redundancypgn copytransformer hi-map mgsumfigure human evaluation compare systemsummaries likert scale best verify effectiveness ofthe propose multi-granularity interaction networkfor summary generation.4.5 human evaluationto evaluate linguistic quality generated summary carry human evaluation focuson three aspect fluency informativeness andnon-redundancy fluency indicator focuseson whether summary well-formed grammatical informativeness indicator reflectwhether summary cover salient point fromthe input document measure whether thesummary contains repeat information sample instance multi-news test andemploy graduate student rate summary.each human judgment evaluate output different system sample human judgment obtain every sample finalscores average across different judges.results present figure seethat model performs much good baseline fluency indicator model achievesa high score high ofcopytransformer indicate thatour model reduce grammatical error andimprove readability summary informativeness indicator model betterthan rouge-2 indicate ourmodel effectively capture salient information non-redundancy indicator mgsumoutperforms baseline large margin indicate multi-granularity semantic informationand joint learn extractive summarizationdoes help avoid repeat information thegenerated summary.6251model r-su4mgsum-ext sentence extractor representation summary generator representation send representation results ablation study multi-newsdevelopment set.4.6 ablation studywe perform ablation study developmentset investigate influence different module propose mgsum model modules aretested four remove sentenceextractor train generator verify theeffectiveness joint learn abstractivesummarization remove summary generator part train sentence extractorto verify effectiveness joint learn theextractive summarization remove document representation sentence andword representation verify effectiveness ofthe document granularity semantic information remove document sentence representation word representation verifythe importance sentence representation since interaction thesentences different document without documentrepresentations establish connection betweenall sentence document representation isremoved furthermore also establish connection word sentence representation remove model degeneratesinto transformer time.table present result find therouge-2 score extractive summarizationdrops summary generator remove indicate joint learningmethod help extractive summarization benefit abstractive summarization rouge2 score abstractive summarization dropsby sentence extractor removed.this indicate extractive summarization doeshelp abstractive summarization identify importantsentences interactive encode phrase.rouge-2 score extractive summarizationdrops rouge-2 score abstractive summarization drop document representation remove indicate eshuman race governor mansion state today could night helm two-thirds ofthe state currently control country stateoffices expect keep three republican forgrabs utah north dakota indiana wrest north carolina fromthe dems bring toll potential take three report race montana hampshire washington stilltoo close call three democrat incumbent seekingreelection result could impact health care since asupreme court ruling grant state ability obamacare smedicaid expansion romney victory would dramatically empowerrepublican governor analyst click state-by-statebreakdown could happen delaware hampshire missouri expect notchsafe state report state state statehas state office expect twothirds nation state report washington post montana montana rhode island indiana missouri state ishome list state office hampshire montana incumbent john kasich first woman hold state seat thestate note huffington post north carolina state wingop-held seat vermont delaware jersey statein history year population around thepopulation report montana hampshire missouri statedepartment emergency declare state emergency clickfor full list check list state vote tonight gain voter state pick governor enlarge image toggle caption cole/ap cole/ap voter stateswill pick governor tonight republican appear track increase number least potential extend theirhold two-thirds nation state office health care political scientist thad kousser co-author thepower american governor republicans currently hold governorship democrat rhode island lincoln chafee anindependent eight gubernatorial seat grab heldby democrat three republican hand poll race analyst suggest three tonight contest consider competitive allin state incumbent democratic governor montana hampshire washington voter state pick governor tonight republican appear track increase number least potential extend hold two-thirds thenation state office republican currently hold governorship democrat rhode island lincoln chafee anindependent seat expect former charlotte mayorwalter dalton last election vote reportsthe washington post democrat expect hold seatsin west virginia missouri democrat likely hold seatsin vermont delaware report poll race analyst thatonly three tonight contest consider competitive allin state incumbent democratic governor matter presidency national politics bestalemated affordable care political scientist sample summary document cluster fromthe multi-news test underline show overlapparts abstractive summary human summary.the extractive abstractive summary generate mgsum high overlap different overlap mark indifferent color document representation simulatethe relationship document necessaryto improve performance extractive andabstractive summarization rouge-2 scoredrops compare mgsum summary generator afterremoving document representation thesentence representation extractivesummarization co-promote recognition ofimportant information abstractive summarization sentence representation removed.it indicate semantic information sentencegranularity great importance encode multi6252ple documents.4.7 case studyin table present example summary generate strong baseline extractive andabstractive method output model hasthe high overlap ground truth moreover extractive abstractive summary showconsistent behavior high overlap whichfurther indicate method jointlypromote recognition important information.compared extracted summary generated summary concise coherent.5 conclusion future workin work propose novel multi-granularityinteraction network encode semantic representation document sentence word canunify extractive abstractive summarizationby utilize word representation generatethe abstractive summary sentence representation extract sentence experiment resultsshow propose method significantly outperform strong baseline method achievesthe best result multi-news dataset.in future introduce task likedocument rank supervise learning themulti-granularity representation improvement.acknowledgmentsthis work support national natural science foundation china tencentai rhino-bird focused research program no.jr201953 laboratory science technology standard press industry keylaboratory intelligent press media technology thank anonymous reviewer helpful comment xiaojun correspondingauthor
non-task orient dialogue system haveachieved great success recent year tolargely accessible conversation data development deep learning technique givena context current system able yield arelevant fluent response sometimesmake logical mistake weak reason capability facilitate conversation reason research introduce mutual novel dataset multi-turn dialoguereasoning consist manually annotate dialogue base chinese student english listen comprehension exam compared previous benchmark non-task oriented dialogue system mutual much morechallenging since require model canhandle various reason problem empirical result show state-of-the-art methodsonly reach behind human performance indicate thereis ample room improve reason ability mutual available http //github.com/nealcly/mutual.1 introductionbuilding intelligent conversational agent oneof long running goal existing conversational agent categorize taskoriented dialogue system kannan non-task-oriented chatbot system shum al.,2018 owing rise deeplearning technique large amount conversation data training lowe al.,2017 zhang witnessingpromising result chatbots academia andindustry dialogue system train largedialogue corpus predict response givena context line method retrievebased method generation base method rely∗contribution internship msra.m ma'am forget phone.f thanks could live without little thing.m know mean great significance enjoy dinner everything perfect hard take whole family butyour restaurant perfect johnny place play time talkwith sister husbands.✓ thanks compliment restaurant.✘ sorry good time.✘ goodbye brother love you.✘ hurry honey late dinner.figure incorrect reason toapologize exclude relationship speaker waiter customerbased context.on matching score perplexity score respectively development text matchingand pre-training model devlin liuet machine able achieve highlycompetitive result datasets even closeto human performance instance esim chenet achieve dialogue welleck bert achieve term andr10 ubuntu corpus whang still huge highperformance leader-board poor practical user experience chatbot engine often generate response logically incorrect violatecommonsense knowledge shum alikely reason current dialogue system donot strong reasoning skill theca previous benchmark tackle linguistic information matching previous work hasdemonstrated neural encoders capture richhierarchy syntactic semantic information jawahar clark however reason capability commonsense knowledgeare capture sufficiently young important research question canevaluate reason ability chatbots canpotentially allow bridge highperformance leader-board unsatisfactorypractical performance develop1407dataset task reasoning domain manuallyubuntu lowe next utterances prediction technique persona-chat zhang next utterances prediction persona dialogue welleck next utterances prediction persona coqa reddy conversational diverse douban next utterances prediction open dream reading comprehension open levesque coreference resolution open swag zellers plausible inference movie commonsenseqa talmor reading comprehension open race reading comprehension open clark reading comprehension science drop reading comprehension open cosmos huang reading comprehension narrative mutual next utterances prediction open table comparison dataset datasets manually indicate human writing thequestion answer involve data annotation process rather mere manual selection data.an open domain multi-turn dialogue reasoningdataset mutual facilitate conversation modelreasoning capability particular give context prepare four response candidate ofwhich relevant context ofthem logically correct show figure response follow topic thefirst appropriate require reason ability social etiquette relationship make thecorrect choice consider existingdialogue benchmarks.we build dataset base chinese highschool english listening comprehension test data student except select best answer three candidate option give multiturn dialogue question original data isformatted 〈dialogue question answer〉 whichis directly suitable goal since chatbotsonly concern respond context insteadof answer additional question therefore weask human annotator rewrite question andanswer candidate response candidate thenour dataset follow traditional response selection setting lowe modelshould recognize correct response others fora multi-turn dialogue.the result dataset mutual consist question term almost question involve reasoning design bylinguist expert high-quality annotator weevaluate state-of-the-art retrieval-based model andpre-training model mutual best methodgives significantly underperform human performance best ofour knowledge mutual first human-labeledreasoning-based dataset multi-turn dialogue.we provide detail analysis provide insightsinto develop potentially reasoning-based chitchat dialogue systems.2 related worktable compare dataset prior dialogueand reason related benchmarks.dialogue ubuntu dialogue corpus alarge retrieval-based dataset lowe extract ubuntu chat persona-chat zhang considers consistent personality dialogue crowd worker require toact part give provided persona chatnaturally dialogue welleck anatural language inference dataset modify frompersona-chat demonstrate canbe improve consistency dialoguemodels coqa reddy collect bypairing annotator chat passage inthe form question answer questionis dependent conversation history arealso several large-scale datasets chinese assina weibo shang douban conversation corpus e-commercedialogue corpus zhang show table exist conversation benchmark focus test reason ability exception coqa whichconsiders pragmatic reasoning difference isthat coqa machine comprehension dataset inwhich conversation base give passage.another related reading comprehension dataset isdream designedspecifically challenge dialogue-based reading1408ma'am forget phone.oh thanks could live without little thing.i know mean great significance didyou enjoy dinner everything perfect hard take thewhole family restaurant perfect.johnny place play time talk withmy sister husbands.i glad hear area always popular.well sure back.mmmfffquestion answerwhat probable relationship speaker waiter customer.✘ brother sister.✘ husband wife.ma'am forget phone.oh thanks could live without little thing.i know mean great significance didyou enjoy dinner everything perfect hard take thewhole family restaurant perfect.johnny place play time talk withmy sister husbands.✓ thanks compliment restaurant.mmmffmutuallistening comprehension✘ sorry good time.✘ goodbye brother love you.✘ hurry honey late dinner.dialogue audio context text responsefigure process modify listening comprehension test data.comprehension rely external questionto test model understanding capability incontrast dataset dataset nextutterance prediction task fundamentalproblem retrieval-based chatbots addition dataset require various specific reason ability algebraic reasoning intention prediction main characteristic ofour dataset.reasoning recently effort make todevelop benchmark task address reason language understanding winograd schemachallenge levesque reasoningbased coreference resolution task pair sentence differs phrase swag zellerset derive pair consecutivevideo caption include short context eachwith four candidate ending commonsenseqa talmor question answer datasetextracted conceptnet speer conceptnet construct datasetensures question directly target commonsensereasoning race machine read comprehension dataset collect english exam forchinese student reasoning challenge clarket contain genuine grade-schoollevel science question corpus science reference sentence drop cosmos huang focus factualunderstanding commonsense comprehension respectively.despite success datasets hardlyhelp chatbots directly following traditional dialogue response selection setting deeply modifyenglish listen comprehension conversation toform utterance prediction task.3 dataset3.1 collectionthe original listen comprehension material andquestion-answer pair design linguist expert students require choose bestanswer three option question base piece audio ensure student fully understand audio question need beanswered reason capability.we crawl listen exam public websites1 since audio either conversation people simple passage onlycrawled data conversation format rawdata format triple 〈conversation audio question choices text answer image thefollowing data pre-processing method appliedto convert data data figure pre-processing question candidate choice problem weconsider duplicate delete them.if three candidate option inone problem randomly drop incorrect optionsuntil three candidate left.the answer store image apply acommercial system convert image text.it easy recognize printed alphabet answerfor system manually correct ocr1all problem dataset freely accessible onlinewithout copyright consult legal adviser.1409outputs ensure quality original listeningcomprehension test conversation store asaudio adopt commercial system convert speech text recruit experiencedannotators correct transcription error tofurther ensure quality transcript theyare double-checked annotator next step.step candidate response creation figure illustrate process modify listen comprehension problem first annotatoris require segment original conversation clue answer question appeared.then construct positive response responsea figure negative response responsec response consult correct choice choice incorrect choice choice andchoice respectively make mutual morechallenging annotator construct negative response response base correct choice step mutual keep reason test designedby expert also introduce anothertype reason instance show infigure response exclude basedon relationship speaker isincorrect attitude reasoning.it worth note negative response arelogically correct context consider butthey appropriate response context istaken account therefore dataset focuseson multi-turn conversation reason rather thanthe logic sentence frame negativeresponse encourage annotator copy somephrases context discourage model thatcan solve problem text matching calculate lexical overlap responseand context word inthe positive negative response occur thecorresponding context suggest mutual ishard solve plain text matching.annotators step english-major graduate student chinese familiar withenglish language exam china fluent english pass tem-82 annotators requiredto draft annotate instance repeatedly untiltheir labeling sufficiently accurate provide useful annotation conversation areadapted construct reasoning-based responseproblem annotator right skip con2the high level test english major foreignlanguage china.mutual context-response pairs turns dialogue words utterance size context size response size original dialogues original questions response candidates data statistic mutual.versation employ five annotator constructthe response quality inspector checkit discard instance inspector doubtthe uniqueness correctness answer.3.2 analysisthe detailed statistic mutual summarizedin table mutual average turns.the vocabulary size smallerthan dialogue datasets lowe wuet mutual modify fromlistening test english foreign language thecomplexity morphology grammar muchsimpler datasets.for human-annotated datasets alwaysa trade-off number instance beingannotated quality annotation kryciskiet dataset small previous crawling-based dialogue dataset lowe al.,2015 collection method.but comparable high-quality reasoningbased dataset clark khashabi al.,2018 talmor human-designeddialogue dataset zhang moreover around sufficient train discriminativemodel nivre fine-tune pretraining model wang distribution different reasoningtypes annotate specific type reasoningthat involve instance sample thetest categorize group thedefinition ratio group show asfollows.attitude reasoning type instance testsif model know speaker attitude towards anobject.algebraic reasoning type instancestests whether model equip algebraicabilities choose response.intention prediction type test whether amodel predict speaker donext.1410algebraicreasoning please give refund.✓ ticket right total more.m della long stay london concert weekend.m look forward concert much tell sing inpublic first time high school concert legs shook uncontrollably almost fell.attitudereasoning haha imagine nervous then.✘ nervous time firstsinging high school concert.✘ yeah would happy too.✘ feel disappoint hear problem meet school ableto study next term.m difficulty receive scholarship thing arefinally look up.intentionprediction drop school mean scholarship glad hear continue studies.✘ receive scholarship excuse smoking area.m sorry move smoking area.f afraid table smoking area available now.situationreasoning sorry smoke hospital again.✓ smoke could please give menu could please tell customer tosmoke stand smell.✘ sorry smoke bus.m painting valuable museum collection.f amazing glad spend ticket exhibit today.m museum purchase worth hear museum purchase hear museum purchase sculpture worth now.✓ painting worth now.others restaurant popular.✘ restaurant crowd all.✘ table restaurant.✘ show table.f like ticket concert.m good evening ma'am reservation awfully sorry empty table leave now.context candidates responses reasoning typefigure examples mutual dataset choice relevant context logiccorrect negative choice might reasonable extreme case positive appropriate.clue word purple underline.situational reasoning situation information e.g. location relationship speaker consider type instance modelshould mine implicit information previous context.multi-fact reasoning type instance correct response relate multiple fact incontext require model deeply understand context rather simply text matching.others instance requireother commonsense knowledge example atthe bottom figure model know thata fully reserve restaurant usually popular.the type reasoning consider themost relevant real chatbots example enable chatbots make personal recommendationsif machine know user attitude abilityof intention prediction allow chatbots respondmore intelligently long conversation session.3.3 mutualplusto increase difficulty safe response replace candidate responses foreach instance mutual guarantee diversity safe response sample list include afraid quite catch saying. could repeat really sorry catch that. particular theinstance choose randomly select responseto replace positive response replace thecorrect safe response negativeresponse replace original positive responseis still best one.the motivation build mutualplus evaluatewhether model able select safe responsewhen candidate inappropriate whenwe replace positive response safe response simulate scenario theother candidate incorrect phenomenon iscommon retrieval-based chatbots limited candidate response handle case inpractice similarly evaluate modelcan choose correct response instead saferesponse correct response exists.4 experimentswe split data training development testsets ratio packinstances construct conversationduring splitting avoid data leakage followingthe standard dialogue setting lowe consider task responseselection task employ traditional informationretrieval evaluation method include recall position candidate recall position candidate mean reciprocal rank1411 voorhees compare performance several response selection model wellas pre-training model simply introduce theseworks follows:4.1 baselineswe evaluate individual scoring method multichoice method human performance ourexperiment given context four candidate individual scoring method compute score choice independently witha score select individual thehighest score among four candidate contrary multi-choice method select best oneby classification choice formulate correct response tend sharemore word context incorrectones following lowe calculatethe tf-idf vector context thecandidate response respectively selectthe high cosine similarity contextand candidate response model output.the calculate training set.dual lstm lowe lstmsare encode context response respectively relevance context responseis calculate similarity final hiddenstate lstms.sequential matching network al.,2017 avoid lose information context construct word-word sequencesequence similarity matrix instead utilize thelast hidden state aggregate similarity matrix matching score.deep attention matching network zhouet adopt self attention module vaswaniet encode response utterance respectively match utterance response applies cross-attention module and3d match obtain final score.bert devlin pre-training modelshave show promising result various multichoice reason task whang following devlin concatenate context sentence acandidate response sentence bert input.on bert fully-connected layer usedfor transform token representation tothe match score.roberta re-establishbert mask language model train objectiveby data different hyper-parameters.we fine-tune roberta bert.gpt-2 radford given context positive response high probability compare negative response motivated concatenate context response sequence calculate joint probability entire sequence response perplexitysequence consider positive response.moreover fine-tune gpt-2 context positive response pair mutual training denote gpt-2-ft.multi-choice method inspired bert formultiple choice devlin task isconsidered pick suitable responseby compare four candidate response particular concatenate candidate response withthe correspond context input sequence issubsequently encode produce representation positive response predict basedon concatenation representation fully connect layer softmax isused method denote bert-mc similarly implement roberta-mc anothermulti-choice method.human performance obtain human performance employ expert measurethe ceiling performance test set.4.2 experiment resultswe report performance approach introduce human performance implementation detail show appendix b.4.2.1 results mutualall model perform significantly onother popular conversation datasets theubuntu corpus lowe dialogue dataset welleck whilehuman address reasoning problem easily.for example bert give theubuntu corpus roberta give mutual.tf-idf slightly good randomly guess indicate obvious statistic clue context positive response incontrast tf-idf achieve score onthe ubuntu corpus show dataset moredifficult correct answer text overlap.we evaluate typical retrieved-based dialogue model performance mutual table we1412dev testbaseline category baseline method mrrbaseline human score method discrimination tf-idf lstm lowe zhou devlin score method generation gpt-2 radford radford method bert-mc devlin comparison vary approach mutual.dev testbaseline category baseline method mrrbaseline human score method discrimination tf-idf zhou devlin score method generation gpt-2 radford radford method bert-mc devlin method roberta results mutualplus transfer method denote train mutual test mutualplus.can well-designed matching model notgive good performance compare simple duallstm moreover drop absolute point compare performance onthe ubuntu corpus indicate text matchingmodels handle reason problem well.both bert roberta outperform othermodels mutual consistent resultsin literature talmor ismainly model learn reason capabilityduring pre-training large corpus althoughroberta achieve asurprising number indicate thatthe model able rank correct response tothe top-2 position bert-mc roberta-mcobtain similar result bert roberta respectively however even roberta behindhuman performance point indicatingthat mutual indeed challenging dataset whichopens door tackle complex reasoning problem multi-turn conversations.gpt-2 gpt-2-ft also perform undesirablyon mutual even averaged perplexity onmutual testset phenomenon illustrate sentence mutual fluent and2 current generative model still plenty ofroom improve reasoning ability.4.2.2 results mutualplusas show table model perform worseon mutualplus indicate dataset difficult mutual consistent ourassumption find performance multichoice method significantly good individual scoring method possible explanation isthat multi-choice method consider candidate together distinguish whether thesafe response best contrast individualscoring method robust safe responsesare easy confuse method training stage.moreover roberta-mc outperforms others alarge margin show outstanding performanceon reason problems.furthermore conduct transfer experiment model train mutual testedon mutualplus without fine-tuning experiment investigate whether model handle saferesponses well never train corpus show table roberta-mcand roberta drop respectively,1413figure bert-mc roberta-mc performanceon different reason types.in transfer setting demonstrate benefitsof safe response training process moreover individual scoring robertaoutperforms roberta-mc show individual scoring method robust thesafe response feed training.4.3 discussionperformance across different reason type analyze model performance across differentreasoning type calculate bert-mc androberta-mc performance various questiontypes introduce section shownin figure find trend bert-mcand roberta-mc similar across different category roberta-mc significantly outperformsbert-mc attitude reasoning multi-fact reasoning potential reason somenormal pattern action attitude capture roberta-mc play football excite however instance involve algebraicand situation show poor performance tworeasoning type heavily depend commonsensereasoning taking figure example takesa simple subtraction step derive time difference turn asignificant challenge roberta-mc second case roberta-mc fail infer dialoguesituation goal find flat rent.performance across different context length interest performance robertadoes decrease significantly number ofturns increase different phenomenon observe datasets show intable performance drop pointsr turn long turn performance turn high good morning look flat people near university.f well several place available rent range month requirement think flat month good prefer live aquiet street need least bedrooms.✘ question enrollment hesitate me.✓ flat satisfied sign contracttomorrow.f floor supermarket.f want bedroom three flat meet yourrequirement.f know time right york york hour behind.f hour behind york.f hour ahead york.✘ well york.figure error analysis indicate roberta-mc sprediction.figure ablation context information contextmeans context remove model predictcorrect choice base four candidate context-n denote earlist utterance removed. instances performance comparison differentnumber turn test denote number ofturns instances number instancesturns indicate reasoning problem notbecome much hard context becomeslonger result also show difficulty ofmutual attribute reason instead complex conversation history.context ablation study verifywhether dataset require multi-turn understand rather degenerate single turn reason problem evaluate roberta roberta-mcperformance utterance manuallyremoved figure show performance whenthe early utterance remove test asthe ablation utterance increase performanceof roberta roberta-mc significantly decrease conform intuition robertaand roberta-mc achieve ablate utterance context respectively indicate importance utteranceand quality dataset moreover shuffle sequence utterance performance ofroberta-mc drop show itis insensitive utterance sequence information.5 conclusionwe introduce mutual high-quality manuallyannotated multi-turn dialogue reasoning dataset contain dialogue testreasoning ability dialogue model describethe process generate mutual performa detailed analysis find various state-ofthe-art model show poor performance mutual.the best model roberta obtain large model performance human performance hope thisdataset facilitate future research multi-turn conversation reason problem.acknowledgmentswe thank yulong chen duyu tang zhiyang tengand yang insightful discussion wealso thank anonymous reviewer constructive comment corresponding author isyue zhang thank support brightdreams robotics westlake university researchgrant
although deep learning model broughttremendous advancement field opendomain dialogue response generation recentresearch result reveal trainedmodels undesirable generation behavior malicious response generic bore response work propose aframework name negative training minimize behavior given trained model framework first find generated samplesthat exhibit undesirable behavior thenuse negative training signal forfine-tuning model experiment showthat negative training significantly reducethe rate malicious response discourage frequent response improve responsediversity.1 introductionend-to-end dialogue response generation beformulated sequence-to-sequence seq2seq task give dialogue context model askedto generate high-quality response recent year deep learn model especially seq2seq languagegeneration model sutskever choet bring significant progress tothe field dialogue response generation.however recent research reveal undesirable behavior seq2seq model side effect standard maximum likelihood estimation training generic bore response problem vulnerability toadversarial attack cheng belinkovand bisk malicious egregious response problem glass work propose explore negative training framework correct unwanted behavior dialogue response generator negative training first find identify input-outputpairs trained seq2seq model exhibit someundesirable generation behavior treat badexamples negative trainingsignals model correspondingly regardthe train data good example standardmle training positive training idea negative training inspire theway parent might teach child language incorporate positive negativetraining signal example teach child love hate addition tousing positive example like love applesbut hate banana might also pointout hate someone consider impolite.in work negative training addressthe malicious response problem frequent response problem describe section and3.3 open-domain dialogue response generation.in experiment show negative trainingcan significantly reduce rate maliciousresponses discourage frequent response andgreatly improve response diversity.2 model formulationin work adopt recurrent neural network base encoder-decoder seq2seq model sutskever mikolovet widely application like dialogue response generation al.,2016 machine translation luong denote onehot vector representation input sequence serve context history information e.g.the previous utterance todenote scalar index corresponding reference target sequence vocabulary weuse represent parameter seq2seq1the last word token indicate theend sentence.2045model model generative distribution.on encoder side every firstmapped corresponding word embeddingxembt xembt input long-short termmemory lstm hochreiter schmidhuber,1997 sequence latent representation henct decoder time similarly firstmapped yembt context vector whichis suppose capture useful latent information ofthe input sequence need construct weadopt attention mechanism context vector construction first attention mask vector distribution input sequence iscalculated decide part focus thenthe mask apply latent vector constructct =∑ni=1 henci formulationof general type global attention describedin luong calculate mask.during baseline training standard trainingwith stochastic gradient descent tominimize negative log-likelihood thereference target sentence give input sentencein data lmle pdata ∼pdata logpθ ∼pdata −m∑t=1logpθ yt|y refers yt−1 y0is begin-of-sentence token consider popular decoding generate sentence give input greedy decoding sampling practice dialogue responsegeneration greedy decoding provide stable andreproducible output severely affect thegeneric response problem sampling providemore diverse less predictable response andthus give rise malicious response problem.3 negative training framework3.1 overviewthe negative training framework3 two-stageprocess given trained model a2here refers output layer lstm cellmemory layer.3our code available http //github.mit.edu/tianxing/negativetraining_acl2020 debug environment ptest provide testinput samples4 model decode samplesand decide well-defined criterion whethereach input-output pair exhibit undesirablebehavior pair provide negative training signals.negative training derive empiricalbayes risk minimization specifically overall objective minimize expectedrisk model exhibit undesirable decodingbehavior lneg ptest ex∼ptestey∼pθ refers binary criterion willbe exhibit undesirable behavior take derivative lneg w.r.t derivative trick widely inreinforcement learning sutton barto ∇θlneg ptest =ex∼ptestey∼pθ logpθ compared lmle maximizesthe log-likelihood train data sample lnegminimizes log-likelihood undesirable modelsamples reason call negative training preliminary experiment find negative training need augment standard objective lmle encourage modelto retain original performance lneg+pos lneg λposlmle experiment find λpos simply setto work well.in next section discuss general negative training framework tailor themalicious response problem frequent responseproblem respectively.3.2 negative training maliciousresponse problemfor malicious response problem follow themethodology propose glass test refer test data.2046first list malicious target sentence create gibbs-enum algorithm5 call tofind trigger input cause model toassign large probability target sequence thefollowing type define o-greedy-hit trigger input sequence isfound model generate targetsentence greedy decoding.• o-sample-min/avg-hit trigger input sequence find model generatesthe target sentence minimum/averageword log-probability large giventhreshold tout.• io-sample-min/avg-hit addition thedefinition o-sample-min/avg-hit alsorequire average log-likelihood thetrigger input sequence measure islarger threshold enforce thetrigger input likely input byreal-world users.tout trained seq2seq model averageword log-likelihood test data isset reasonable average word loglikelihood test intuition themodel assign large probability themalicious sentence reference sentence inthe test note type criteriac indicate whether target sentence hitby trigger input.as show glass typical seq2seq model train around rate malicious target w.r.t samplemin/avg-hit across data-sets however fewmalicious target w.r.t greedy-hit inthis work focus malicious response problem sample decode table weshow pair trigger input malicious targetsentences w.r.t io-sample-min-hit baselinemodel ubuntu data.now apply negative training framework reduce rate trained modelfor give list malicious target eachiteration negative training every target sentence ytarget first call gibbs-enum algorithmto find trigger input xtrigger target is5for paper self-contained describe gibbsenum algorithm appendix a.6a lstm language model train sametraining data regard response independent sentence negative training maliciousresponse probleminput target list ytarget model parameter learn rate criterion trainingdata dtrainfor ytarget ytarget doget xtrigger ytarget gibbs-enumalgorithm.while xtrigger ytarget donegative update ∇θlogpθ ytarget|xtrigger data sample xpos ypos dtrainpositive update λpos ∇θlogpθ ypos|xpos whileend fortrigger input give minute havein first placemalicious target help youtrigger input mirc suppose thatseems problemmalicious target think like youtrigger input know photoshop skype toobut itmalicious target moneytable examples trigger inputs.hit xtrigger ytarget update model toreduce log-likelihood ytarget|xtrigger theprocess formulate algorithm trigger input multiple iteration negative update usually need criterion longer meet note iteration gibbs-enum algorithm call find anew trigger input target.in experiment show negative training effectively reduce rate malicious target iteration eventually gibbsenum algorithm longer find trigger inputsfor large number target initially hits.3.3 negative training frequentresponse problemthe generic response problem end-to-end dialogue response generation refersto typical behavior trained model whereby generated response mostly safe,7note actual implementation algorithm minibatch based.2047boring uninformative tknow good idea however difficult invent automatic criterion determinewhether response generic not.in work focus frequent responseproblem sub-problem generic responseproblem refer behavior trainedmodel generate exactly usually bore response high frequency.we propose metric call max-ratio tomeasure severe frequent response problemis given test decoding method themodel generate response maxratio define ratio frequentresponse experiment baseline modelshave max-ratio around response like idon know across different data-sets showingthe severity frequent response problem.during negative training frequent response first threshold ratio rthres select response frequency ratio large rthreswill discourage iteration model sresponse training data input sentence ismonitored response frequency large thanrthres negative example frequency statistic calculate currentand last mini-batches procedure isformulated algorithm note positive training also need model retain itsoriginal performance.algorithm negative training frequentresponse probleminput model parameter threshold ratio rthres learn rate train data dtrainfor xpos ypos dtrain dogenerate response ysample model.compute frequency rsample ysample inthe last mini-batches.if rsample rthres thennegative update ∇θlogpθ ysample|xpos positive update λpos ∇θlogpθ ypos|xpos ifend forin experiment show negativetraining significantly reduce max-ratio themodel test data greatly increase diversity model responses.4 experimentswe conduct experiment three publicly availableconversational dialogue data-sets ubuntu switchboard opensubtitles save space description data-sets provide appendix b.4.1 baseline model trainingfor data-sets first train lstm base lmand attention base seq2seq model hidden layer size embed size isset switchboard dropout layer withrate model over-fittingis observed mini-batch size andwe apply train fixed start learn rate iteration another10 iteration halving ubuntu andswitchboard start startinglr opensubtitles resultsare show appendix c.after negative training addition measuringthe rate malicious target diversity ofthe response also important check whetherthe original sample quality baseline modelis damage towards perplexity ofthe model negative training willbe compare also conduct human evaluation tomeasure whether sample quality decreased.other popular measurement bleuscore find correspond poorly withhuman judgement nevertheless also find model bleu score notbecome worse negative training.4.2 experiments malicious responseproblemfollowing glass list malicioustargets create test whether negative training teach model generate sentencesin list however addition prevent themodel generate target specific list isalso important check whether negative traininggeneralizes malicious target test target list contain similar different targetsfrom training list also create test generalization training test list contain0.5k targets.it also interest investigate whether malicious target negative trainingcan rate test list towards thatend train seq2seq paraphrase model usingthe paranmt data-set wieting gimpel paraphrase testyou break broken brokeni kill kill killyou really badyou stupid stupid stupidyou shut shut mouth shut uptable examples malicious target traininglist test list paraphrase training targetswhich augmentation.with model structure described insection paraphrase model togenerate paraphrase malicious target thetraining target list8 augmentation experiment training list without augmentation isfirst negative training augmentedwith paraphrased target respectively paraphrase copy training target sentence samples malicious target shownin table training augment trainingand test list three data-sets andthere sequence-level overlap traininglists augment test list.in experiment spot harmful sideeffect negative training frequent word inthe train target list severely penalize andsometimes receive probability even normalperplexity testing especially experiment withsmall λpos alleviate problem asimple technique call frequent word avoiding negative gradient apply themost frequent word malicious training targetlist9 example negative trainingagainst target hate hate negative gradient.for data-sets negative training algorithm1 execute train baseline model for20 iteration training target list fixedlearning rate mini-batch size λpos ubuntu forswitchboard opensubtitles.the main result show table forswitchboard focus sample-avg-hit becausewe find target w.r.t samplemin-hit similar result report andglass ubuntu opensubtitles focus sample-min-hit note weget similar result w.r.t sample-avg-hit for8note training test list manually created.9the exact avoid word o-sample-min-hit io-sample-min-hittraining train test train test pplbaseline o-sample-avg-hit io-sample-avg-hittraining train test train test pplbaseline o-sample-min-hit io-sample-min-hittraining train test train test pplbaseline main result rate malicious target negative training neg-tr refers negative training experiment original malicious train target list without paraphraseaugmentation.ubuntu/opensubtitles omit resultshere.we first observe data-sets negativetraining effectively reduce rate thetraining target list less little nodegradation perplexity provide comparison model behavior appendix also significant rate reduction achieve testtarget list overlap trainingtarget list show negative training similar traditional positive training also generalizes.it also show train list augmentationcan reduce malicious target rate consistently training test list example ubuntu data rate negativetraining w.r.t o-sample-min-hit canbe reduce paraphrase augmentation.we find model generation behavior non-adversarial setting almost sameas baseline negative training example list beam search before/afterneg-train large overlap also findthat model generate similar sample shownin appendix believe reason negative training focus make model morerobust adversarial input originalgeneration behavior keep intact positivetraining equation experiments frequent responseproblemin section report result negative training framework section apply totackle frequent response problem datasets negative training execute iterationson train model training data select rthres fixed learn rate three data-sets mini-batch size isset λpos work focus improve model sgreedy decode behavior instead beam searchfor following reason baseline model experiment find beamsearch give response diversity thangreedy decoding favor short response usually length much result much large max-ratio training doingbeam search much time-consuming thangreedy decoding.to measure diversity model generate response addition max-ratio introducedin section specially design frequent response problem also adopt entropymetric propose zhang given setof response decode test ent-ncalculates entropy n-gram distribution ent-n =∑g∈gn−r n-grams appearedin response refers ratio frequency n-gram w.r.t n-grams theresponses set.in experiment negative training aharmful side-effect spot decoding model tend output long ungrammaticalresponses know real valid deterrent crimecrime yeah satisfy tryingnot believe reason thesentence token penalizedduring negative training appear everynegative example apply frequentword avoiding technique section4.2 except negative gradient scale addition baseline model compareour propose negative training framework a10we find scal zero result extremely shortresponses.ubuntu rthres m-ratio e-3test-set rthres m-ratio e-3test-set rthres m-ratio e-3test-set main result negative training different rthres frequent response problem diversitymetrics response test data also show m-ratio refer ent-n/max-ratio metric.gan goodfellow approach adiscriminator introduce generator gtries fool discriminator believe samplesare real data sample mingmaxdv =mingmaxd ∼pdata logd +ex∼pdata generator refers seq2seq modelpθ framework attractive tackle generic response problem zhang discriminator canact critic judge whether response sample boring describe training detail andhyper-parameter setting approach inappendix e.we also provide comparison decoding popularwork field implement mmi-antilm forour models.the experimental result show table experiment best diversity result nondegenerate sample quality show bold wefirst observe large diversity measuresbetween baseline model test especially switchboard opensubtitles data.2050switchboard opensubtitlesinput cost three hundred dollar stud input captain want mebaseline think good idea baseline sorryneg-train think would agree neg-train hotelinput want breed champion input brown could ibaseline know baseline knowneg-train think neg-train like introduce myselfinput long haired input leave mebaseline know baseline leave youneg-train good shape neg-train take first stepinput short hair input thank brownbaseline know baseline sorryneg-train neg-train happy youtable greedy-decoding sample test data negative training sample consecutive input next sample reference response previous indicate severity frequent/genericresponse problem result negative training different rthres show negative trainingcan significantly increase response diversity withlittle loss bleu score shownin appendix performance example maxratio reduce ent-3 increase by149 switchboard data consistent improvement achieve small rthres used.however sample quality decrease becomingtoo long ungrammatical rthres small.the reason could much diversityis model extremes provide diversity result degradation samplequality.comparing note although onswitchboard/opensubtitles give high entropy max-ratio negativetraining result main focus ourwork frequent response problem alsofind mmis hyper-parameters difficult tune work hyper-parameters dont transferwell data-sets ofconfiguration model give ungrammaticaloutput sample problem also mentionedin paper ubuntu data even find configuration performsbetter baseline model.further vanilla approach shownto effective experiment reasoncould despite discriminative nature gantraining still feed positive gradient samplesfrom model appendixe enough prevent model fromgenerating believe additional technique zhang need forthe approach effective.we show model sample negative training table show thatnegative training effectively discourage boring response response diversity improve however limitation observe diversity doesnot necessarily lead improvement informativeness response w.r.t input sometimes model generate completely unrelatedresponse sample three data-sets areincluded appendix g.to rigorously verify negative training diversity sacrifice sample quality human evaluation conduct resultsare show table observe negativetraining significant margin threedata-sets show negative training doesnot damage quality generated samples.note human evaluation reflect thediversity model raters rateone response time.5 related worksthe malicious response problem gibbsenum algorithm find trigger input glass,2019 originate large body work adversarial attack deep learning model withcontinuous input space image classification goodfellow szegedy ordiscrete input space sentence classification or2051data-set baseline neg-trainubuntu switchboard opensubtitles table human evaluation results dataset sample input-output pair baseline model model negative training areevenly distribute english-speaking human evaluator evaluator pick preferred sample report evaluation check whethernegative training hamper quality generation.seq2seq model papernot samantaand mehta liang ebrahimi al.,2017 belinkov bisk chen adversarial attack refer phenomenon thatwhen imperceptible perturbation apply tothe input output model change significantly correct incorrect triggerinputs find gibbs-enum algorithm beregarded type target attack theattack trigger model assign large probabilityto specific malicious target sentence.motivated work adversarial attack various adversarial training strategy madryet belinkov bisk miyato al.,2016 propose make trained modelsmore robust attack adversarial training model feed adversarialexamples correct label negative training framework consider work differ fromadversarial training instead themodel right thing refer positive training work model train wrong thing best knowledge first work investigate conceptof negative training dialogue response model first propose solution maliciousresponse problem.the malicious target list work verysimilar glass wepropose test target list test generalization negative training show thatthe training list effectively augment byutilizing paraphrase model.in work propose definition frequent response problem sub-problem thegeneric response problem muchresearch work devote alleviate genericresponse problem end-to-end dialogue responsegeneration maximal mutual information objective propose toutilize auxiliary penalize generic response decoding closely relate thiswork sophisticated training framework base ongan zhang havealso show effective techniquessuch variational information maximization orreward every generation step regs propose improve training however ourexperiments show vanilla approach give unsatisfactory result whether negative training11 complementary framework worth investigate future work.finally note concept negative trainingin work different negative sample word2vec training mikolov negative sample word2vec training usedto prevent training trivial usually choose randomly work negativesamples carefully choose exhibit particular undesirable behavior model isthen correct behavior.6 conclusionin work propose negative trainingframework correct undesirable behavior atrained neural dialogue response generator algorithm involve major step first input-outputpairs exhibit behavior identify andthen fine-tuning model negative training example also show negativetraining derive overall objective minimize expected risk undesirable behavior experiment applynegative train malicious response problem frequent response problem getsignificant improvement problem
automatic question generation hasshown promise source synthetic training data question answering inthis paper textual diversity inqg beneficial downstream usingtop-p nucleus sample derive sample froma transformer-based question generator weshow diversity-promoting indeed provide good training likelihood maximization approach beam search wealso show standard evaluation metricssuch bleu rouge meteor inversely correlate diversity proposea diversity-aware intrinsic measure overallqg quality correlate well extrinsicevaluation qa.1 question generation diversitybesides area dialog bordes tutor system lindberg automatic question generation recently beenapplied great success generate synthetictraining example question answering alberti dong important question remain unexplored doesincreased textual diversity automatically generate question lead good figure show four question generate byone model detail section froma squad rajpurkar passage ananswer span prompt question aredifferent lexically also information answer entity draw upon andeven world knowledge e.g. tesla sreputation scientist intuitively suchsample diversity sufficiently accurate could provide model rich train signal.existing work predominantly rely oncustomary beam search decode generationand n-gram similarity metric bleu forevaluation alberti tesla birthday time magazineput cover cover caption theworld power house note contribution toelectrical power generation receive congratulatory letter pioneer inscience engineering include albert einstein.✏ appear time magazine cover his75th birthday famous scientist cover timemagazine scientist receive congratulate birthday famous scientist also figure passage underlined answer span tesla correspond question generate byour model generated question exhibit lexical factual diversity.dong zhang bansal suchmethods/metrics solely optimize/reward similaritywith human-generated reference question treatedas ground truth however many openended generation task ofmany possible available human annotation approach directly penalize diversityby discourage deviation recent year massively pre-trained neural language model devlin radfordet revolutionizednlp open-ended text generation model show remarkable robustness sample radford holtzman thisobservation couple example presentedin figure suggest treat amore open-ended generation problem relyingon power modern text generator producediverse accurate sample might yield good qaresults current approach optimize forthe likely question.we test hypothesis fine-tune pretrained transformer-based mask al.,1http //aqleaderboard.tomhosking.co.uk/squad56522019 sample question itusing top-p nucleus sampling holtzman al.,2020 diversity-promoting text generationtechniques exist—both training time e.g. vaes kingma welling inference e.g. top-k sampling diverse beam search vijayakumar —that appliedto various task language modeling bowmanet dialog clark visualqg jain image captioning vijayakumar wechoose nucleus sample effectiveness simplicity speed experiment leadto following discovery nucleus sample indeed produce good qaresults beam search even onequestion generate prompt.  metric reward similarity gtare negatively correlate diversity aresult inaccurate predictor downstreamqa performance diversity-promoting qg.  measure devise combinesdiversity similarity show strongcorrelations performance.2 question generation robertawe fine-tune roberta mask al.,2019 give answer span within textualcontext show figure nucleussampling holtzman generation.model various transformer architecture beused text generation raffel following dong alberti fine-tune pre-trained masked prefixlm raffel predict question tokenqt give prompt tokenized textualcontext special token delimit answerspan question tokens thathave already generate give promptin left-to-right order special separator tokenseparates question prefix prompt theprompt encode bidirectional attention andquestion token causal left-only attention.we choose roberta pre-trained model extended pre-training large amountsof text implementation ofthe model base hugging face wolfet pytorch implementation roberta.fine-tuning training example themodel predict single question tokenqt give prompt previous questiontokens teacher-forced mask attimestep question tokenthat mark generation training attemptsto minimize masked loss i.e. negativelog-likelihood token predictionfor position losst logp inference generation fine-tunedroberta model output probability distribution entire vocabulary questiontimestep top-p nucleus sampling henceforth sample re-normalized categorical distribution nucleus thesmallest subset vocabulary item acumulative probability mass great high probability among subset restrict pool high-likelihood regionof vocabulary compare top-k sampling nsreduces chance generate low-probabilityitems original distribution peak atone item question generation worksby repeat nucleus sampling question tokensuntil eos.3 experiments resultsto test effect diversity generatequestions nucleus sampling beamsearch number different model andcompare performance.general setup considering performancesof different generation method vary acrossmodels different capacity train eight qgmodels uniquely characterize size parameter amount training data fine-tune modelsizes roberta base parameter large parameter fine-tuningwe train squad1 split duet three-way split publicportion squad1 widely adopt literature approximately train test prompt question pair draw vary amountsof sample range randomfrom train fine-tune model simulate different point low- high-resource2https //github.com/xinyadu/nqg/blob/master/data/raw/5653 train generator f15b model large modeltable performance beam search beam nucleus sampling thesquad-du dataset bold best underlined yield strong result beam bleu rouge meteor score moreover performance improve nucleus probability mass p.spectrum model train epochswith learning rate batch size experiments model generate question prompt thesquad1-du question first evaluate exist generation metric bleu rouge meteor extrinsically evaluateon fine-tune bert devlin al.,2019 whole-word-masked onthe generate example model evaluate test.for eight model evaluatebeam search beam henceforth fordifferent value beam experiment withthe roberta-base model show significantperformance difference beam size and10 therefore report result inthis paper important point note thatgiven paragraph-long input prompt large number synthetic example mayalso need many practical case largebeam size become prohibitively expensivefrom computational standpoint transformerbased generators.for evaluate closely approximate greedydecoding observe model averagenucleus size practically setup alsoset maximum number vocabulary item nucleus even large valuesrarely reach experiments.table show performance mean fivedifferent seed generator bleu-1 rouge-4 meteor variant ineach metric family show high correlation downstream performance alsoshow performance measure squad official score metric compute degreeof lexical overlap predict thetarget answer expect model performanceimproves model size traininginstances intrinsic evaluation qa.importantly however beam best intrinsic evaluation result eight model iscompetitive lowest-resource setup training data hand hasthe high score especiallywhen sufficient training data available ormore note experiment generatea single question prompt generation diversity across different prompt yield higher-qualityqa training data also faster alternative beam sampling five question perprompt large-100 model additional improvement experiments increasep make generation diverse chancesof draw less likely candidate thus5654model- train generator f1base-20b despite rouge score diverse qgwith nucleus sample improves result overbeam search zero-shot out-of-domain generation fornewsqa.generating incorrect question also table1 gain diversity generallygreater drop performance likely todecreased accuracy find holdsin challenging out-of-domain setup perform zero-shot application i.e. furtherfine-tuning four squad-trainedqg model newsqa reading comprehensiondataset news article trischler show result answerable subset ofnewsqa train extractour prompt test evaluation sample absolute score lowerthan squad relative performancesof beam similar intrinsic thebest predictor performance newsqa wasrouge-4 extrinsic evaluation.comparison augmentation humangeneration assess quality generated question absolute term table wecompare performance best modelabove large-100 correspondinghuman annotation impressively in-domainmodel performance similar thatof zero-shot score newsqa alsowithin roughly point gt.we also evaluate generator ability augment human-generated question taking approach similar prior augmentation experimentsdataset train source f1squad1-dugt train diverse synth show impressive result compare human annotation augment synth dong alberti generate large synthetic dataset synth millionexamples wikipedia passage answerspans example extract theircorresponding passage separate modelwhich train squad question type instead full-length question many much andhow long synth fine-tune bertwwm finally fine-tuned onthe target datasets squad1-du newsqa astable show synth achieve absolute point improvement high-performancelarge bert-wwm model.summary results result empirically show give enough training data sufficiently powerful model diverse leadsto strong in-domain out-of-domain training likely question i.e. beamsearch every time less useful existinggeneration metric inadequate evaluate diverse question generator source trainingexamples.4 intrinsic evaluation diverse qgto well understand performance existinggeneration metric measure diverse wetake sampler table e.g. base-100 randomly generate largenumber subset consist nsamplers evaluate assign sampler measureperformances metric separately bin.the process repeat table note themember give containthe number generator actualselection generator generally different indifferent member setup allow toevaluate varying number generator different capacity performance average5655figure performances exist propose generation metric measure diverse theproposed metric show strong correlation spearman in-domain out-ofdomain evaluation.over large number experiments.figure show rather poor forsome negative median spearman scorebetween best metric squad1-du rouge4 newsqa rouge-1 downstream f1.these result provide quantitative confirmation thatrouge similar metric inadequate evaluator diverse sole focus onaccuracy respect available leadsus final research question intrinsically measure overall quality qaunder diverse nucleus sample given categorical distribution vocabulary item model nucleus proposeto measure accuracy relative anddiversity generation.accuracy similarly perplexity fortimestep evaluation example take theprobability qs,1 model precisely nucleus generate gttoken give prompt history qs,1 t 1.we average evaluation pair tocompute model accuracy intuitive measure diversityof model nucleus average entropy ofpn evaluation timesteps however entropyis unbounded measure non-linearinverse growth relative propose accuracymetric make mathematical combination difficult instead rely observationthat increase make generationmore diverse cardinality also onaverage probability contain token experiment onboth datasets show measure diversity compute proportion time wasfound include across timesteps qgevaluation data high positive correlation withthe entropy pearson spearman note unlike accuracymetric timestep diversity metric boolean token eitherin importantly average acrossmany evaluation timesteps probability measure diversity enable straightforwardconvex combination propose accuracymetric.our final metric weighted accuracy diversity tunable parameter reflectingthe weight accuracy relative diversity ourexperiments metric outperform existingmetrics large margin wide range wvalues figure median spearman scorebetween metric in-domain w=.7 out-of-domain w=.8 evaluation isover observe similar performance difference propose andexisting metric pearson r.given scope paper evaluate thecombined metric underlyingideas apply diverse text generation general.further experiment necessary evaluate themetric generation tasks.5 conclusionwhile diversity generation receive significant attention text generation problem e.g. dialog show paper alsoan important measurable dimension quality question generation hope thatour work encourage exploration ofdiversity-promoting evaluation possible future direction include systematic study ofdifferent aspect diversity e.g. lexical andfactual control diversification individualaspects generation.acknowledgmentswe thank anonymous reviewer valuable feedback.5656
address problem extractive questionanswering document-level distant supervision pair question relevant document answer string compare previously probability space distant supervision assumption assumption correspondence weak answer string label possible answer mention span weshow assumption interact thatdifferent configuration provide complementary benefit demonstrate multiobjective model efficiently combine theadvantages multiple assumption outperform best individual formulation ourapproach outperform previous state-of-the-artmodels point triviaqa-wikiand point rouge-l narrativeqasummaries.11 introductiondistant supervision assumption enable thecreation large-scale datasets usedto train fine-grained extractive short answer question answer system example istriviaqa joshi author utilize pre-existing trivia questionanswer string pair couple relevant document high likelihood document support answer question illustration another exampleis narrativeqa dataset kočiský crowd-sourced abstractive answer stringswere weakly supervise answer mentionsin text movie script summary inthis work focus setting documentlevel extractive distant supervision isspecified answer string inputquestion-document pair.1based triviaqa-wiki leaderboard approachwas sota work submit joan molinsky well know answer joan rivers joan rivers diary diva joan alexandra molinsky know professionally joan rivers american comedian actress writer producer televisionhost joan rivers strongly influence lenny bruce receive grammy award best spoken word album forher book diary diva joan alexandra molinsky bear june brooklyn york enter show business choose joan rivers asher stage name …question dancer purify answer spring mount helicon mount helicon spring mount helicon mount helicon play begin three page courtier sentence make reparation topurify bath spring mount helicon thefigure actaeon play represent triviaqanarrativeqafigure triviaqa narrativeqa example triviaqa example three occurrence original answer string joan rivers blue alternate incorrect alias diary diva purple joanrivers mention show blue support answeringthe question narrativeqa example answer sting spring mount helicon blue mount helicon orange latter substring ofthe former mention correct answer spans.depending data generation process theproperties result supervision thesets differ example provided answer triviaqa include alias originaltrivia question answer capture semantically equivalent answer liable introducingsemantic drift possible answer string diary diva relate joan rivers valid answer give question.on hand answer string innarrativeqa mostly valid since highoverlap human-generated answer thegiven question/document pair show spring mount helicon mount helicon valid answer relevant mention case annotator choose answers5658that appear verbatim text moregeneral case noise come partial phrasesand irrelevant mentions.while distant supervision reduce annotation cost increase coverage often come withincreased noise e.g. expand entity answerstrings alias improves coverage also increase noise even document-level distant supervision form answer different interpretation partial supervisionlead different point coverage/noise spaceand relative performance well understood.this work systematically study method forlearning inference document-level distantly supervise extractive model using abert devlin joint question-passageencoder study compound impact probability space define themodel probability space base independentparagraphs whole documents.• distant supervision assumption totranslate supervision possible stringsato possible location answer mention thedocument.• optimization inference todefine correspond training objective e.g.hard maximum marginal likelihood make answerstring prediction inference viterbi ormarginal inference show choice probability spaceputs constraint distant supervision assumption capture threechoices interact lead large difference inperformance specifically provide framework understand different distant supervision assumption corresponding trade-offamong coverage quality strength distant supervision signal best configuration depend property possible annotationsa thus data-dependent compared recent work also bert representation ourstudy show model suitable probabilistic treatment achieves large improvement of4.6 triviaqa rouge-l narrativeqa respectively additionally design anefficient multi-loss objective combine thebenefits different formulation lead significant improvement accuracy surpass thebest previously report result studiedbert…𝒑𝟏𝒒 joan rivers begin endprobabilities joan rivers ………spanprobabilities stringprobabilities joan rivers diary diva contextualizedrepresentation𝚵 𝚵bert𝒑𝟑𝒒 ………figure document-level model fortest-time inference part bert-basedparagraph-level answer score component upper part illustrate probability aggregation acrossanswer span share answer string refersto either operator give example john rivers derive paragraphs.tasks results strengthen transferlearning fully label short-answer extraction data squad rajpurkar lead final state-of-the-art performance of76.3 triviaqa-wiki narrativeqa summary task.22 probability spacehere first formalize paragraph-level anddocument-level model previously document-level extractive typically paragraph-level model consider paragraph document independently whereasdocument model integrate dependenciesamong paragraphs.to define model need specify theprobability space consist possibleoutcomes assign probability individual outcome extractive probability space outcomes consist token position ofanswer mention spans.the overall model architecture show infig bert devlin toderive representation document token asis standard state-of-the-art extractive model devlin minet bert model encodea pair give question paragraphfrom give document neural text representation representation to2the code available http //github.com/hao-cheng/ds_doc_qa5659define scores/probabilities possible answer begin position turn todefine probability possible answer spans.then answer string probability define aggregation possible answerspans/mentions.in following show paragraph-leveland document-level model differ thespace possible outcome compute answer span probability answer position begin scores.scoring answer begin position givena question document consist paragraph bert encoderproduces contextualized representation eachquestion-paragraph pair specifically foreach token position final hidden vector contextualized tokenembedding vector dimension.the span-begin score compute =wtb weight vector thespan-end score define way.the probability start position endposition arepb =exp =exp normalize factor dependingon probability space definition detail probability answer span ikto define partition function depend onwhether paragraph-level documentlevel probability space.paragraph-level model paragraph levelmodels assume give questionagainst document paragraphsp1 independently select pair answerpositions begin endof answer paragraph casethat support answer questionq special null position select follow squad bert implementation3 possible outcome theparagraph-level probability space oflists begin/end position pair eachparagraph ik3https //github.com/google-research/bertand range position respectiveparagraphs.the answer position different paragraphsare independent probability paragraph answer begin compute normalize possible position paragraph i.e. =∑i∈ik∪ null =∑j∈ik∪ null position paragraph probability answer begin atik /zbk probability define analogously.the probability possible answer position assignment document define definition dueto independence assumption model usingparagraph-level normalization learn directly calibrate candidate answer differentparagraphs other.document-level model document-level model assume give question againstdocument single answer span select oppose paragraph theparagraph-level model possible position paragraph part joint probability space directly compete other.in case token span begin position ofthe select answer normalizing factor aretherefore aggregate paragraph i.e. =k∑k=1∑i∈ikexp =k∑k=1∑j∈ikexp compared since always avalid answer document task study null necessary documentlevel model thus exclude the4in paper focus datasets documentis know contain valid answer straightforward toremove assumption consider document-level nullfor future work.5660coverage quality strengthh1 −→h3 ↘table distant supervision assumption corresponding tradeoff indicate high value medium value.inner summation probability possible outcome answer span z∗bz∗e distant supervision assumptionsthere multiple interpret distant supervision signal possible outcome inour paragraph-level document-level probability space lead correspond training lossfunctions although several different paragraphlevel document-level loss chen kadlec clark gardner linet study inthe literature want point interpret distant supervision signal atradeoff among multiple desideratum coverage maximize number instancesof relevant answer span toprovide positive example model.• quality maximize quality annotationsby minimize noise irrelevant answerstrings mentions.• strength maximize strength signalby reduce uncertainty point modelmore directly correct answer mentions.we introduce three assumption distant supervision signal interpret lead different tradeoff amongthe desideratum table begin additional useful notation given document-question pair anda answer string define ofa-consistent token span follow foreach paragraph span onlyif string span position paragraph paragraph-level model forparagraph empty redefine ykato null similarly define aconsistent begin position start position consistent span ∈yka a-consistent position definedanalogously addition term answer span correct question correspondinganswer string correct answer context specific mention answer stringfrom position entail answer similarly term answer begin/end position correct ifthere exist correct answer span starting/endingat position.h1 a-consistent answer span correct.while assumption evidently often incorrect quality dimension especially fortriviaqa provide largenumber positive example strong supervision signal high coverage↗ strength↗ include study completeness.h1 translates differently possible outcomesfor corresponding model depend probability space paragraph document paragraphlevel model select multiple answer span foreach paragraph form possible outcome thus multiple a-consistent answer span occur single outcome long different paragraph multiple a-consistent answerspans paragraph asmentions select equal probability e.g. different annotator document-levelmodels select single answer span document therefore multiple a-consistent answerspans occur separate annotation event table show logprobability outcome consistent h1.h2 every positive paragraph correct answer a-consistent assumption paragraph non-empty aconsistent span term positive paragraph hasa correct answer triviaqa example assumption correctfor first third paragraph second contain mention noisyanswer alias assumption medium coverage generate positive example frommultiple paragraph allow multiplepositive mention paragraph alsodecreases noise high quality→ notclaim mention joan rivers thefirst paragraph support answer question strength supervision signal weaken relative model need figure multiple a-consistent mention paragraph correct.h2 variation correct span assuming5661span-based position-basedh1∑k∈k∑ ∈ykalogps ∑k∈k∑ik∈ykb alogpb +∑k∈k∑jk∈yke alogpe h2∑k∈k ∈ykaps ∑k∈k ξik∈ykb +∑k∈k ξjk∈yke ξk∈kξ ∈ykaps ξk∈kξik∈ykb ξk∈kξjk∈yke table objective function document-question pair different distant supervision assumptions.ξ refers to∑and hardem respectively.that answer span correct correct position assume paragraph correct answer begin position fromykb correct answer position select answer span necessarilybelong example contains abcd would correct begin correct span make sensefor modeling assume paragraph correctbegin position instead correct answer span i.e. really want inconsistent answer like give thatour probabilistic model assume independence ofbegin answer position beable learn well span-level weak supervision prior work clark gardner position-based distant supervision assumption pair-paragraph model akin ourdocument-level h2span-based distant supervision assumption theimpact position span-based modeling thedistant supervision well understood wewill experiment majority setting position-based weak supervision effective span-based model.for paragraph-level document-level model correspond differently possible outcome paragraph model outcome canselect answer span positive paragraph andnull negative document-level model view answer different paragraph asoutcomes multiple draw distribution.the identity particular correct span begin/end position unknown computethe probability event comprise consistent outcome table show log-probabilityof outcome consistent right span-based leave position-basedinterpretation plug in∑for document correct answer inits a-consistent assumption positsthat document correct answer span orbegin/end position every positive paragraph need improve supervision quality example allowsthe model filter noise paragraph twoin since model give choice anyof a-consistent mention capabilityto assign zero probability mass supervisionconsistent mention paragraph.on hand coverage provide single positive example whole document rather thanone positive paragraph also reducesthe strength supervision signal themodel need figure mention toselect large document-level ya.note couple adocument-level model paragraph-levelmodel directly tradeoff answer different paragraph select asingle answer span document withthe distant supervision hypothesis spanbased position-based definition possible consistent outcome formulate thelog-probabilities event define rowthree table using∑for wasused kadlec cloze-style distantly supervise recurrent neural network models.the probability-space paragraph documentlevel distant supervision assumption position span-based togetherdefine interpretation distant supervisionsignal resulting definition probability spaceoutcomes consistent supervision next define correspond optimization objectivesto train model base supervision describe inference method make predictionswith trained model.4 optimization inference methodsfor distant supervision hypothesis maximize either marginal log-likelihood a5662consistent outcome log-likelihoodof likely outcome hardem latterwas find effective weakly supervise tasksincluding semantic parsing show objective function alldistant supervision assumption comprise pairing distant supervision hypothesis position-based span-basedinterpretation probability define accord assume probability space paragraph document table denote theset paragraph document ykdenotes weakly label answer spansfor paragraph null forparagraph-level model note span-basedand position-based objective function equivalent independence assumption since task predict answer string rather particular mention agiven question potentially beneficial aggregate information across answer span correspond string inference scoreof candidate answer string obtain aspa ∈xps ofspans correspond answer string ξcan either∑or max.5 usually beneficial tomatch training objective corresponding inference method marginal inference hardem viterbi inference showedhardem optimization useful anh2 span-level distant supervision assumption couple inference unclear whetherthis trend hold when∑inference useful orother distant supervision assumption perform good therefore study exhaustive combinationsof probability space distant supervision assumption training inference methods.5 experiments5.1 data implementationtwo datasets paper triviaqa joshi wikipedia formulation narrativeqa summary kočiskýet using preprocessing as5for inference marginal scoring approximate scheme aggregate probability ofcandidates string generate list begin/endanswer position paragraph.clark gardner triviaqa-wiki6 keep rank paragraph to400 token document-question pair forboth training evaluation following narrativeqa define possibleanswer string rouge-l similarity crouwdsourced abstractive answerstrings identical data preprocessing andthe evaluation script provide authors.in work bert-base model fortext encoding train model default configuration describe devlin al.,2019 fine-tuning parameter fine-tunefor epoch triviaqa epoch narrativeqa.5.2 optimization inference latentvariable modelshere look cross product optimization hardem inference distant supervision assumption resultin model latent variable therefore exclude look hypothesis h2and couple span-based span orposition-based formulation paragraphlevel document level probability space.the method correspondsto span-based h2-p hardem train andmax inference result show observe inference leadsto significantly good result triviaqa h2-p h2-d slight improvement h3-d. narrativeqa inference maxis better attribute fact correct answer often multiple relevant mentionsfor triviaqa also §5.6 whereas narrativeqa rarely case thus inference withsum narrativeqa could potentially boost theprobability irrelevant frequent strings.consistent observethat span-based hardem work well spanbased h2-p large advantageon narrativeqa triviaqa however h2-d h3-d span-based performsconsistently good span-based hardem forposition-based objective consistentlybetter hardem potentially hardemmay decide place probability mass beginend position combination containmentions string finally ob6https //github.com/allenai/document-qa56630.620.640.660.680.70.720.740.76hardem-spanhardem-posmmlspanmmlposhardem-spanhardem-posmmlspanmmlposhardem-spanhardem-posmmlspanmmlposh2-p h2-d h3-dmax triviaqa f10.480.50.520.540.560.580.60.62hardem-spanhardem-posmmlspanmmlposhardem-spanhardem-posmmlspanmmlposhardem-spanhardem-posmmlspanmmlposh2-p h2-d h3-dmax narrativeqa rouge-lfigure comparison different optimization inference choice group distant supervision hypothesis base result triviaqa narrativeqa.served distant supervision hypothesis/probability space combination positionbased always best among four objective position-based objective performbetter independence assumption begin/end position model futurework arrive different conclusion position dependency integrate based thisthorough exploration focus experimentingwith position-based objective therest paper.5.3 probability space distantsupervision assumptionsin subsection compare probability spaceand distant supervision assumption table result upper section compare paragraph-level model h1-p section compare documentlevel model h1-d h2-d h3-d performance model inference show report exact match score triviaqa rouge-l score fornarrativeqa.for triviaqa h3-d achieve significantly betobjective infer triviaqa narrativeqaf1 rouge-lparagraph-level modelsh1-pmax modelsh1-dmax comparison distant supervision hypothesis mml-pos objective triviaqa narrativeqa sets.ter result formulation capable clean noise positiveparagraphs correct answer e.g.paragraph decide aconsistent mention trust paragraph-levelmodels h1-p h2-p outperform corresponding document-level counterpart h1-d andh2-d fact withouth3 without predict null dmodels notlearn detect irrelevant paragraphs.unlike triviaqa h2-d model achieve thebest performance narrativeqa hypothesize fact positive paragraphsthat correct answer rare innarrativeqa summary relatively shortand answer string human-annotated thespecific document therefore neededto clean noisy supervision usefulsince also lead reduction number ofpositive example coverage model document-level model always improve theirparagraph counterpart learn calibrateparagraphs directly other.5.4 multi-objective formulations cleansupervisionhere study method improveweakly supervise model first combine distant supervision objective multitask manner h2-p h3-d triviaqa andh2-p h2-d narrativeqa choose basedon result §5.3 objective highercoverage susceptible5664objective clean infer triviaqa narrativeqaf1 rouge-lsingle-objectiveparx result compare multi-objectivesand clean supervison indicate model ispre-trained squad.to noise paragraph-level model advantage learn score irrelevant paragraph null outcomes note sameparameters objective multiobjective formulation parameter less efficient individualmodels second external clean supervision squad rajpurkar train bert-based model epochs.this model match probability space andis able detect null extractive answerspans result network initializethe model triviaqa narrativeqa result show table surprising external cleansupervision improve model performance note interestingly external supervision narrow performancegap paragraph-level document-levelmodels reduce difference thetwo inference methods.compared single-objective component multi-objective formulation improve performance triviaqa narrativeqa.5.5 test evaluationtable report test result triviaqa andnarrativeqa best model comparison torecent state-of-art sota model triviaqa report score full test setand verified subset narrativeqa rougetriviaqa wikifull verifiedf1 emours h2-p+h3-d squad wang clark gardner –narrativeqa summaryrouge-lours h2-p+h2-d squad nishida external data test result triviaqa wiki narrativeqa summaries squad refers bestmodel without pretraining squad external data refers model nishida without marco data bajaj score reported.compared recent triviaqa sota wanget best model achieve f1and improvement full test and6.8 improvement verifiedsubset narrativeqa test improverouge-l nishida thelarge improvement even without additional fullylabeled data demonstrate importance select appropriate probability space interpret distant-supervision cognizantof property data well selectinga strong optimization inference method withexternal fully label data initialize model performance significantly improved.5.6 analysisin subsection carry analysis studythe relative performance paragraph-level anddocument-level model depend sizeof answer string number aconsistent span hypothesize correlate label noise triviaqa devset best performing model h2-p andh3-d inference.we categorize example base size oftheir answer string size oftheir correspond a-consistent span |i|.specifically divide data subset and5665subset size h2-p h3-d ∆qss score subset triviaqa group size answer string andcorresponding possible mention indicatesthe improvement h2-p h3-d.report performance separately subset asshown table general expect andqll noisy large whereqsl potentially include many irrelevant mention whileqll likely contain incorrect answer string false alias observe improvement significant noisy subset suggest document-level modeling crucial forhandling type label noise.6 related workdistant supervision successfully usedfor decade information extraction task suchas entity tagging relation extraction cravenand kumlien mintz several propose learn e.g. multi-label multi-instance learning surdeanuet assume least supportingevidence hoffmann integration oflabel-specific prior ritter adaption shift label distribution work start explore distant supervision scale system particularlyfor open-domain evidence tobe retrieve rather give input reading comprehension evidence retrievedfrom information retrieval system establish aweakly-supervised noise inthe heuristics-based span label chen joshi dunn dhingra al.,2017 line work jointly learn andevidence rank either pipeline system wang kratzwaldand feuerriegel end-to-end model line work focus improvingdistantly-supervised model developinglearning method model architecture canbetter noisy label clark gardner propose paragraph-pair ranking objective whichhas component h2-p h3-dposition-based formulation exploremultiple inference method combination objective less powerful representation coarse-to-fine model propose handle label noise aggregate information relevant paragraph extract answer select propose hard learning scheme include experimental evaluation.our work focus examine probabilisticassumptions document-level extractive qa.we provide unified view multiple methodsin term probability space distant supervision assumption evaluate impact oftheir component combination optimization inference method best ourknowledge three hypothesis along withposition span-based interpretation notbeen formalize experimentally compare onmultiple datasets addition multi-objectiveformulation new.7 conclusionsin paper demonstrate choice ofprobability space interpretation distantsupervision signal document-level alarge impact interact depending onthe property data different configurationsare best combined multi-objective formulation reap benefit constituents.a future direction extend work question answer task require reason overmultiple document e.g. open-domain addition finding generalize task e.g. corpus-level distantly-supervised relation extraction.acknowledgementsome idea work originate fromhao cheng internship google research.we would like thank ankur parikh michaelcollins william cohen discussion anddetailed feedback work well othermembers google research languageteam anonymous reviewer valuablesuggestions would also like thank sewonmin generously share process data andevaluation script narrativeqa.5666
previous work answer complex question knowledge base usually separatelyaddresses type complexity questionswith constraint question multiplehops relation paper handleboth type complexity time.motivated observation early incorporation constraint query graph canmore effectively prune search space wepropose modified staged query graph generation method flexible generate query graph experiment clearlyshow method achieve state theart three benchmark kbqa datasets.1 introductionknowledge base question answer kbqa aimsat answer factoid question knowledgebase attract much attention recentyears bordes al.,2017 liang petrochukand zettlemoyer early work kbqafocused simple question contain singlerelation bordes donget however real question often complex recently somestudies look complex kbqa.two different type complexity beenstudied single-relation question constraint example question wasthe first president u.s. singlerelation president answer entityand entity u.s. also constraint first need satisfy thistype complex question staged query graphgeneration method propose firstidentifies single-hop relation path addsconstraints form query graph al.,2015 questions multiple relation example question wife founderof facebook answer relate facebook relation namely wife founder answer type multihop question need consider long relationpaths order reach correct answer themain challenge restrict searchspace i.e. reduce number multi-hop relation path consider searchspace grow exponentially length relation path idea beam search forexample chen propose consider best matching relation instead relation extend arelation path however little work doneto deal type complexity together.in paper handle constraint andmulti-hop relation together complex kbqa.we propose modify staged query graph generation method allow long relation paths.however instead constraint afterrelation path construct proposeto incorporate constraint extend relation pathsat time allow effectivelyreduce search space complexwebquestions dataset high percentage complex question type complexity ourmethod substantially outperform exist methodswith improvement percentage point inprec percentage point twoother benchmark kbqa datasets method alsoachieves state art1.2 method2.1 preliminariesa represent triplet entity the1our code available http //github.com/lanyunshi/multi-hopcomplexkbqa.970who first wife producer nominate thejeff probst show jeffprobst showtvproducer𝑥nominated_forspouse spouseargminis_a married_untilnominee𝑦 core relation pathconstraint constraint example query graph questionshown assuming start topic entitythe jeff probst show core relation path pathlinking jeff probst show lambda variable x.there constraint query graph note thaty1 node n-ary relations.entity relation relationset given question kbqa find anentity answer question.our method largely inspire existingstaged query graph generation method al.,2015 whichwe briefly introduce first query graph hasfour type node grounded entity shadedrectangle exist entity existential variable unshaded rectangle ungrounded entity lambda variable circle alsoan ungrounded entity represent answer.finally aggregation function diamond afunction argmin count operate entity edge querygraph relation query graph shouldhave exactly lambda variable denote theanswer least grounded entity zero ormore existential variable aggregation function figure show example query graph forthe question first wife producerthat nomiated jeff probst show summarize staged query graph generation method follow detail foundin starting grounded entity find thequestion refer topic entity identifya core relation path2 link topic entity alambda variable existing work considers core relation path contain single relation al.,2015 core relation path identify step attach constraint find question.a constraint consist either grounded entity or2this path call core inferential chain basic query graph also consider path relation connect so-called node special dummy entity usedin freebase n-ary relation simplicity treat thesealso single-relation paths.an aggregation function together relation.see examples figure candidate query graph generatedfrom step step rank measuringtheir similarity question typicallydone neural network model acnn execute top-ranked query graph againstthe obtain answer entities.2.2 motivationthe major challenge face directly apply exist method outline constrain multi-hop kbqa question contain multiple relation examplein figure handle existingwork considers core relation path single node wemake naive modification allow core relation path long search space suddenlybecomes much large example complexwebquestions dataset allow core relation path average havearound core relation path question computationally expensive.recent work multi-hop kbqa tackle thisproblem beam search i.e. keep thetop-k t-hop relation path generate relation path chen lanet however approach ignoresconstraints generate relation path weobserve constraints find question canoften help reduce search space guide thegeneration core relation path towards theright direction.take question figure example givena partial core relation path jeff probst show nominate nominee extend path relation wewould need consider relation kblinked binding include entity nominate jeff probst show ifwe attach constraint producer y2first would need consider thoserelations link producer nominate thejeff probst show.we therefore propose modify stag querygraph generation method wait foreach core relation path generate completely4in priority queue keep onlythe top-ranked query graphs.971tvproduceris_aargminyearextendconnectaggregatethe jeffprobst shownominated_fornominee𝑦 jeffprobst show 𝑥nominated_forspouse spousenominee𝑦 𝑥the jeffprobst shownominated_fornominee𝑦 𝑥the jeffprobst shownominated_fornominee𝑦 figure examples extend connect aggregate action note query graph correspond thequestion first person nominate jeff probst show attach constraint flexible generate query graph couple abeam search mechanism semantic matchingmodel guide pruning explore much smallersearch space still maintain high chanceof find correct query graph.2.3 query graph generationformally method beam search generatecandidate query graph iteratively assume thatthe t-th iteration produce query graph denote iteration eachg apply extend connect aggregate action explain grow byone edge node forall action applicable eachg g′t+1 denote result querygraphs scoring function explainedin section rank query graph g′t+1and place top-k gt+1 continuethe iteration gt+1 isscored high gt.we allow following action grow querygraph figure show example action extend action extend core relation pathby relation current querygraph contain topic entity extend action find relation link andgrows path also make lambda variable current query graphhas lambda variable extend action changesx existential variable find binding execute current query graphagainst find relation link ofthese entity finally attache otherend become lambda variable besides topic entity start current core relation path oftentimes othergrounded entity find question connectaction link grounded entity either the5we also allow relation connect acvt node.lambda variable existential variable connect node.6 decide whichrelation link findall binding execute current querygraph find relation exist betweenone entity following detect anaggregation function question setof predefined keywords aggregate action attach detected aggregation function newnode either lambda variable existential variable connect node.the novelty method extend action apply connect aggregateactions previous method allow.2.4 query graph rankingat t-th iteration rank candidate query graph derive vector graph feedingthese vector fully-connected layer followedby softmax derive first dimension come bertbased semantic matching model specifically weconvert sequence token follow thesequence action take constructg textual description entitiesand relation involve step sequentiallyto sequence existential variable lambdavariables ignore example query graphshown figure paper convert tothe follow sequence jeff probst show nominate nominee consider existential variable connectedto lambda variable already consideredthe existential variable past iterations.7this example illustration purpose actual data relation description different show infigure therefore actual token sequence different forthis example also convert question sequenceof token example question wife thefounder facebook becomes wife founder facebook concatenate query graphsequence question sequence single sequence,972the dimension follow thefirst accumulated entity link scoresof ground entity query graph thesecond number grounded entity appear query graph third fifthones number entity type temporal expression superlative query graph respectively last feature number answerentities query graph.2.5 learningto train model make paired question correct answer without groundtruth query graph following framework ofdas reinforce algorithmto learn policy function end-to-endmanner parameter wantto learn include bert parameter update parameter fully-connectedlayer vector f1score predicted answer respect theground truth answer reward.3 experiments3.1 implementation detailsour method require entity identify fromthe question link corresponding entry name entity link usean exist link tool8 complexwebquestions dataset already extract topic entitiesreleased together dataset twodatasets entity type link make ofthe training question answer learna link model temporal expression andsuperlative linking simply regular expression superlative word list superlativewords manually aggregationfunctions argmax argmin.we initialize bert module rankerwith bert base model9 parameter areinitialized randomly hyper-parameters inbert model dropout ratio thehidden size number layer thewith special token separate bertis typically handle sequence thestandard bert model devlin process theentire sequence derive score layer note thatwe fine-tune pre-trained bert parameter learning.8the tool find http //developers.google.com/knowledge-graph.9the pre-trained bert base model could befound http //github.com/huggingface/pytorch-transformers.number multi-attention head respectively late dump freebase10as datasets beam search weset beam size datasetswe three datasets evaluate method complexwebquestons talmor berant,2018 webquestionssp wqsp complexquestions wetreat major evaluation dataset becausecwq significantly high percentage complex question multiple relation andconstraints show table example question constraint compare wqsp note collect similar statistic dataset providethe ground truth query graph observe thatmajor question relations.3.3 methods comparisonwe compare method follow existing work first compare exist stagedquery graph generation method cannothandle multi-hop question next comparewith handle constraintsand considers multi-hop relation path usesneither beam search constraint reduce thesearch space also compare chen al.,2019 beam search beam size handle multi-hop question handle constraint finally compare bhutaniet ansari bhutani decompose complex question simple question achieve sota term ofprec cwq12 ansari generatedquery program question token token andachieved sota wqsp.3.4 main resultswe show overall comparison table wecan dataset methodclearly achieve best performance term of10the download http //developers.google.com/freebase/.11note treat relation path nodesas paths.12we note leaderboard best prec achieve however methoduses annotate topic entity thus comparable here.973qtype wqsp1-hop cons1-hop cons2-hop cons2-hop cons method wqsp cqprec f1yih −bao −chen −ansari −our method method cwqprec bert lstm extend connect aggregate table statistic wqsp cons stand constraint comparison methodand exist work denote re-implementation ablation study dataset.both prec amount improvement also substantial percentage pointsin prec percentage point thisvalidates hypothesis method worksparticularly well complex question bothconstraints multi-hop relation othertwo datasets wqsp method alsoachieves sota outperform previous method demonstrate robustness method.3.5 ablation studywe also conduct ablation study good understand model verify effectiveness ofour method mainly bert replace bert lstm table lstm-based version methodcan still outperform previous state art.this show effectiveness model isnot simply bert alsotest three version method oneaction remove order understand threeactions necessary result also shownin table aggregate actionis least important action whereas extend action important however needto combine three action together achieve thebest performance.3.6 error analysiswe randomly sample error case manual inspection summarize error thefollowing categories.ranking error error comingfrom mis-prediction query graph look atthese error case closely find relation hard detect even humanjudgment example model mis-predicts therelation question nixon profession correct relation vicepresident understand abbreviationof vice president need external knowledge ifthis mapping observe trainingdata topic linking error observe thereare error occur entity orexpression link error e.g. guitar doescorey taylor play constraint type guitar detect link procedure.generation limitation limitation querygraph generation strategy lead errors.for question john adams havebefore president unlikely find amatched query graph strategies.4 conclusionin paper propose modify stag querygraph generation method deal complexquestions multi-hop relation constraint incorporate constraint querygraphs early couple help beam search able restrict search space experiments show method substantially outperform exist method complexwebquestions dataset also outperform previousstate kbqa datasets.acknowledgmentthis research support national research foundation singapore international research centres singapore funding initiative opinion finding conclusionsor recommendation express material arethose author reflect viewsof national research foundation singapore
avoid give wrong answer question answering model need know toabstain answer moreover user often question diverge model strain data make error likely andthus abstention critical work wepropose setting selective question answer domain shift qamodel test mixture in-domainand out-of-domain data must answer i.e. abstain many question possible maintain high accuracy abstention policy base solely model ssoftmax probability fare poorly since model overconfident out-of-domain inputs.instead train calibrator identify input model errs abstain predict error likely crucially calibrator benefit observingthe model behavior out-of-domain data even different domain testdata combine method squadtrained model evaluate mixturesof squad five datasets ourmethod answer question maintain accuracy contrast directlyusing model probability answers48 accuracy.1 introductionquestion answering model achievedimpressive performance train testedon example dataset tend perform poorly example out-of-domain liang chen yogatama talmor berant fisch deployed system searchengines personal assistant need gracefullyhandle input user often questionsthat fall outside system train distribution.while ideal system would correctly answer alldataset distributions example questionq result disordersof immune system squad john wickham legg recommendedby jenner post medical attendantto eighth child young ofqueen victoria prince albert ofsaxe-coburg gotha hotpotqa capote gain fame worldly novel teenagerin crumble southern mansion searchqa traincalibratetestsourcesource knownoodsource unknownoodfigure selective question answer domainshift trained calibrator first model istrained source data calibrator istrained predict whether model correct onany give example calibrator train data consists previously held-out source data knownood data finally combined selective systemis test mixture test data source distribution unknown distribution.ood question perfection attainablegiven limited training data geiger achievable still challenge goal model abstain arelikely thus avoid show wrong answersto user general goal motivate ofselective prediction model output botha prediction scalar confidence abstainson input confidence el-yaniv andwiener geifman el-yaniv paper propose setting selectivequestion answer domain shift whichcaptures important aspect real-world test data often diverge training distribution system must know abstain.we train model data source distribution evaluate selective prediction performanceon dataset include sample thesource distribution unknown distribution mixture simulate likely scenario inwhich user sometimes question arecovered training distribution sys5685tem developer know nothing unknownood data allow access small amount ofdata third know distribution e.g. examples foresee first show setting challengingbecause model softmax probability unreliable estimate confidence out-of-domain data.prior work show strong baseline indomain selective prediction maxprob methodthat abstain base probability assignedby model high probability prediction hendrycks gimpel lakshminarayananet find maxprob give good confidence estimate in-domain data overconfident data therefore maxprob performspoorly mixed setting abstain enoughon example relative in-domain examples.we correct maxprob overconfidence know data train calibrator—a classifier train predict whether original qamodel correct incorrect give example platt zadrozny elkan whileprior work train calibrator in-domaindata dong show generalize unknown data well trainingon mixture in-domain know data.figure illustrate problem setup thecalibrator know data simplerandom forest calibrator feature derive fromthe input example model softmax outputs.we conduct extensive experiment usingsquad rajpurkar source distribution five datasets different ooddistributions average across choice ofusing unknown dataset another know dataset test auniform mixture squad unknown ooddata average trained calibrator achieves56.1 coverage i.e. system answer test question maintain accuracyon answered question outperform maxprobwith model coverage accuracy maxprob train qamodel squad know data coverage train calibrator onlyon squad data coverage summary contribution follow propose novel setting selective question answer domain shift capturesthe practical necessity know abstainon test data differs training data show model overconfident out-of-domain example relative indomain example cause maxprob perform poorly setting show out-of-domain data even froma different distribution test data improve selective prediction domain shift whenused train calibrator.2 related workour combine extrapolation out-ofdomain data selective prediction alsodistinguish setting task identifyingunanswerable question outlier detection.2.1 extrapolation out-of-domain dataextrapolating train data test data froma different distribution important challengefor current model yogatama train many domain still struggle generalize domain mayinvolve type question require differentreasoning skill talmor berant fischet related work domain adaptationalso generalize distribution butassumes knowledge test distribution unlabeled example labeledexamples blitzer daume assume access test distribution instead make weak assumption accessto sample different distribution.2.2 selective predictionselective prediction model eitherpredict abstain test example longstanding research area machine learning chow,1957 el-yaniv wiener geifman andel-yaniv dong acalibrator obtain good confidence estimate forsemantic parsing rodriguez similar approach decide answer quizbowlquestions work focus training test model distribution whereas ourtraining test distribution differ.selective prediction domain shift otherfields recognize importance selectiveprediction domain shift medical application model train test different group patient selective prediction isneeded avoid costly error feng incomputational chemistry toplak use5686selective prediction technique estimate setof possibly out-of-domain molecule whicha reactivity classifier reliable best ourknowledge work first study selectiveprediction domain shift nlp.answer validation traditional pipelined system open-domain often dedicatedsystems answer validation—judging whether aproposed answer correct system oftenrely external knowledge entity magniniet knowing toabstain part past share task likerespubliqa peñas qa4mre peñas watson system forjeopardy also pipelined approach answervalidation gondek work differsby focus modern neural system trainedend-to-end rather pipelined system byviewing problem abstention throughthe lens selective prediction.2.3 related goal taskscalibration knowing abstain closelyrelated calibration—having model outputprobability align true probability itsprediction platt distinction thatselective prediction metric generally depend onlyon relative confidences—systems judge ontheir ability rank correct prediction higher thanincorrect prediction el-yaniv wiener contrast calibration error depend absolute confidence score nonetheless find ituseful analyze calibration section miscalibration example others doesimply poor relative ordering therefore poorselective prediction ovadia observeincreases calibration error domain shift.identifying unanswerable question insquad model must recognize aparagraph entail answer question rajpurkar sentence selectionsystems must rank passage answer questionhigher passage wang yang case goal abstain system person could inferan answer give question givenpassage contrast selective prediction themodel abstain would give wronganswer force make prediction.outlier detection distinguish selective prediction domain shift outlier detection task detect out-of-domain example schölkopf hendrycks gimpel,2017 liang could anoutlier detector selective classification e.g. byabstaining example flag outlier thiswould conservative model oftenget non-trivial fraction example correct talmor berant fisch know data foroutlier detection train model highentropy example contrast settingrewards model predict correctly oodexamples merely high entropy.3 problem setupwe formally define setting selective prediction domain shift start notationfor selective prediction general.3.1 selective predictiongiven input selective prediction task isto output answercandidates denote model confidence given threshold overall systempredicts abstain otherwise.the risk-coverage curve provide standard wayto evaluate selective prediction method el-yanivand wiener test dataset dtest anychoice associate coverage—the fraction dtest model make prediction on—andrisk—the error fraction dtest decrease coverage increase risk usuallyalso increase plot risk versus coverage andevaluate area curve aswell maximum possible coverage desired risk level former metric average overall paint overall picture selective prediction performance latter evaluates aparticular choice correspond specificlevel risk tolerance.3.2 selective prediction domain shiftwe deviate prior work consider thesetting model train data dtrain andtest data dtest draw different distribution experiment demonstrate settingis challenging standard model areoverconfident out-of-domain inputs.to formally define setting specify three5687data distribution first psource source distribution large training dataset dtrainis sample second qunk unknown distribution represent out-of-domain data encounter test time test dataset dtest sampledfrom ptest mixture psource qunk ptest αpsource qunk choose examine theeffect change ratio section third qknown known distribution representingexamples psource systemdeveloper small dataset dcalib.3.3 selective question answeringwhile framework general focus onextractive question answering exemplify bysquad rajpurkar practical importance diverse array availableqa datasets format input passage-question pair answer candidate span passage base model define probability distributionf selective prediction method consider choose argmaxy′∈y differ associate confidence methodsrecall differs standardselective prediction unknownood data drawn qunk appear test time andknown data drawn qknown availableto system intuitively expect systemsmust known data generalize theunknown data section presentthree standard selective prediction method indomain data show adapt touse data qknown.4.1 maxprobthe first method maxprob directly probability assign base model estimate confidence formally maxprob withmodel estimate confidence input cmaxprob maxy′∈y maxprob strong baseline setting.across many task maxprob shownto distinguish in-domain test example themodel right model wrong hendrycks gimpel maxprob also astrong baseline outlier detection lowerfor out-of-domain example in-domain example lakshminarayanan liang al.,2018 hendrycks desirablefor setting model make mistake onood example abstain onood example in-domain examples.maxprob base model consider choice model fsrc trainedonly dtrain model fsrc+known train theunion dtrain dcalib.4.2 test-time dropoutfor neural network another standard approach toestimate confidence dropout test time.gal ghahramani show dropoutgives good confidence estimate data.given input model compute different dropout mask obtain prediction distribution probability distribution considertwo statistic commonly usedas confidence estimate first take mean ofp̂i across lakshminarayanan cdropoutmean =1kk∑i=1p̂i view ensembling predictionsacross dropout mask average them.second take negative variance thep̂i feinman smith gal,2018 cdropoutvar −var higher variance corresponds great uncertainty hence favor abstain like maxprob dropout either train ondtrain dtrain know data.test-time dropout practical disadvantagescompared maxprob require access internal model representation whereas maxprob onlyrequires black access base model e.g. call trained model dropout also requiresk forward pass base model lead ak-fold increase runtime.4.3 training calibratorour final method train calibrator predict whena base model train data psource is5688correct platt dong differ prior work train calibrator amixture data psource qknown anticipate test-time mixture psource qunk morespecifically hold small number psourceexamples base model training train thecalibrator union example theqknown example define ccalibrator prediction probability calibrator.the calibrator could binary classification model random forest classifierwith seven feature passage length lengthof predict answer five softmax probability output model feature require minimal amount domainknowledge define rodriguez similarly multiple softmax probability decidewhen answer question simplicity thismodel make calibrator fast train givennew data qknown especially compare retrain model data.we experiment four variant calibrator first measure impact knownood data change calibrator train data train either data psource orboth psource qknown data described second consider modification instead themodel probability probability themean ensemble dropout mask describedin section also cdropoutvar feature discuss dropout feature arecostly compute assume white-box accessto model result good confidenceestimates variable changedindependently lead four configurations.5 experiments analysis5.1 experimental detailsdata squad rajpurkar source dataset five datasets ooddatasets newsqa trischler triviaqa joshi searchqa dunn al.,2017 hotpotqa yang naturalquestions kwiatkowski areall extractive question answer datasets whereall question answerable however varywidely nature passage e.g. wikipedia news snippet question e.g. jeopardy andtrivia question relationship pas1we consider different datasets represent differentdomains hence usage term domain shift. sage question e.g. whether question arewritten base passage passage retrievedbased question preprocesseddata mrqa share task fisch al.,2019 hotpotqa focus multi-hopquestions select hard example asdefined yang experiment different datasets choose asqknown qunk result average all20 combination unless otherwise specified.we sample example qknown dcalib squad qunk example fordtest evaluate exact match accuracy define squad rajpurkar detail find appendix a.1.qa model model bertbase squad model train epoch devlin train model total onefsrc five fsrc+known dataset.selective prediction method test-timedropout different dropout mask dong calibrator weuse random forest implementation scikitlearn pedregosa train example know example remain squad example validation tunecalibrator hyperparameters grid search average result random split data.when train calibrator psource use3,200 squad example training forvalidation ensure equal dataset size additionaldetails find appendix a.2.5.2 main resultstraining calibrator qknown outperformsother method table compare methodsthat test-time dropout compared tomaxprob fsrc+known calibrator point high coverage and90 accuracy respectively point lowerauc.2 demonstrate train calibratoris good know data train model calibrator train psourceand qknown also outperform calibrator trainedon psource alone coverage accuracy.all method perform optimal selective predictor give base model though295 confidence interval pairedbootstrap test bootstrap samples.5689auc↓cov acc=80 ↑cov acc=90 ↑train model squadmaxprobcalibrator psource calibrator psource qknown best possible20.5419.2718.479.6448.2353.6756.0674.9221.0726.6829.4266.59train model squad +known oodmaxprobbest possible19.618.8351.7576.8022.7668.26table results method without test-time dropout.the calibrator access qknown outperforms allother method well high better.auc↓cov acc=80 ↑cov acc=90 ↑train model squadtest-time dropout –var test-time dropout mean calibrator psource calibrator psource qknown best possible28.1318.3517.8417.319.6424.5057.4958.3559.9974.9215.4029.5534.2734.9966.59train model squad +known oodtest-time dropout –var test-time dropout mean best possible26.6717.728.8326.7459.6076.8015.9530.4068.26table results method test-time dropout.here calibrator access qknown outperforms methods.achieving bound realistic.3test-time dropout improve result expensive table show result method thatuse test-time dropout describe section negative variance across dropoutmasks serf poorly estimate confidence mean perform well best performanceis attain calibrator dropout feature high coverage accuracythan calibrator non-dropout feature sincetest-time dropout introduces substantial i.e. kfold runtime overhead remain analysesfocus method without test-time dropout.the model non-trivial accuracyon data next motivate focus onselective prediction oppose outlier detection show model still anon-trivial fraction example correct table show non-selective exact match scores3as model accuracy dtest itis impossible achieve risk coverage.figure area risk-coverage curve function much data qknown available allpoints data qknown train calibrator ismore effective model training.for model experiment onall datasets model around accuracyon squad around accuracy onmost datasets since accuracy aremuch high abstain example would overly conservative.4 sametime since accuracy in-domainaccuracy good selective predictor answermore in-domain example example training qknown example notsignificantly help base model extrapolate toother qunk distributions.results hold across different amount knownood data show figure across allamounts know data train andvalidate calibrator split performsbetter training dataand maxprob.5.3 overconfidence maxprobwe show maxprob perform oursetting compare in-domain setting miscalibrated out-of-domain example figure maxprob value generally lowerfor example in-domain example follow previously report trend hendrycks andgimpel liang however themaxprob value still high out-of-domain.figure show maxprob well calibrate underconfident in-domain overconfident out-of-domain.5 example max4in section confirm outlier detector doesnot achieve good selective prediction performance.5the in-domain underconfidence squad andsome datasets provide answer training time multiple answer consider correct test time ap5690train data test data→ squad triviaqa hotpotqa newsqa naturalquestions searchqasquad triviaqa hotpotqa newsqa naturalquestions searchqa table exact match accuracy model test datasets training dcalib improvesaccuracy data dataset diagonal generally improve accuracy data qunk figure maxprob average datathan in-domain data still overconfident onood data plot true probability correctness maxprob curve belowthe line indicate maxprob overestimate theprobability prediction correct calibrator assign confidence data hasa small in-domain curve indicate improve calibration.prob model likely getthe question correct come squad indomain likely question correctif in-domain example mixed test time maxprob therefore doesnot abstain enough example figure show calibrator well calibrate even though train unknown ooddata appendix show calibratorabstains example maxprob.our find bert model notoverconfident in-domain aligns hendryckset find pre-trained computervision model well calibrate modelstrained scratch pre-trained model bependix show remove multiple answer makesmaxprob well-calibrated in-domain stay overconfidentout-of-domain.trained epoch model onlytrained epoch standard bert.our finding also align ovadia find computer vision text classification model poorly calibrate out-of-domaineven well-calibrated in-domain note thatmiscalibration out-of-domain imply poorselective prediction data implypoor selective prediction mixture setting.5.4 extrapolation datasetswe next investigated choice qknown affectsgeneralization calibrator qunk figure percentage reduction maxproband optimal achieve trained calibrator calibrator outperform maxprob overall dataset combination large gain whenqknown qunk similar example samplesfrom triviaqa help generalization searchqaand vice versa snippet passage samples newsqa nonwikipedia dataset also helpful theother hand dataset significantly help generalization hotpotqa likely hotpotqa sunique focus multi-hop questions.5.5 calibrator feature ablationswe determine importance feature thecalibrator remove feature individually leave rest table thatthe important feature softmax probability passage length intuitively passagelength meaningful long passageshave answer candidate passagelength differs greatly different domains.5.6 error analysiswe examine calibrator error pair ofqknown qunk—one similar pair datasets andone dissimilar sample error inwhich system confidently give wrong answer overconfident error sys5691figure results different choice qknown y-axis qunk x-axis pair report percent improvement trained calibrator overmaxprob relative total possible improvement.datasets similar passage e.g. searchqa andtriviaqa help main diagonal element shade assume access qunk section acc=80 ↑cov acc=90 ↑all features–top softmax probability–2nd:5th highestsoftmax probabilities–all softmax probabilities–context length–prediction length18.4718.6119.1126.4119.7918.656.0655.4654.2924.5751.7355.6729.4229.2726.670.0824.2429.30table performance calibrator itsfeatures remove individually leave rest thebase model softmax probability important feature passage length.tem abstain would questioncorrect answer underconfident thesewere sample overconfident orunderconfident error respectively.qknown newsqa qunk triviaqa thesetwo datasets different non-wikipediasources overconfidence error dueto model predict valid alternate answer orspan mismatches—the model predict slightlydifferent span gold span beconsidered correct thus calibrator trulyoverconfident point need improveqa evaluation metric chen ofunderconfidence error passage require coreference resolution long distance include article title neither squad nornewsqa passage coreference chain longor contain title unsurprising calibrator struggle case another ofunderconfidence error case therewas insufficient evidence paragraph answer question triviaqa constructedvia distant supervision calibrator notincorrect assign confidence allunderconfidence error also include phrase thatwould common squad newsqa banned. qknown newsqa qunk hotpotqa thesetwo datasets dissimilar multiple hotpotqa short wikipedia passagesand focus multi-hop question newsqa hasmuch long passage news article doesnot focus multi-hop question overconfidence error valid alternate answersor span mismatch underconfidence error correct answer spanin passage could plausibly answer question suggest model arrive answer artifact hotpotqa facilitateguesswork chen durrett al.,2019 situation calibrator lack ofconfidence therefore justifiable.5.7 relationship unanswerablequestionswe study relationship selectiveprediction identify unanswerable questions.unanswerable question selectiveprediction train model squad2.0 rajpurkar augmentssquad unanswerable question ourtrained calibrator model close modeltrained squad alone maxprob also perform similarly squad model squad model prediction method identifyunanswerable question maxproband calibrator pick threshold andpredict question unanswerable confidence choose maximize squad2.0 score method perform poorly thecalibrator average five choice qknown achieve maxprob achieve result weakly outperform the6we evaluate question randomly sample fromthe squad development set.5692figure difference calibrator andmaxprob function much dtest comesfrom psource i.e. squad instead qunk averagedover datasets calibrator outperform maxprob dtest mixture psource qunk.majority baseline em.taken together result indicate identify unanswerable question differenttask know abstain distribution shift focus test data thatis dissimilar training data theoriginal model still correctly answer nontrivial fraction example contrast unanswerable question squad look similarto answerable question model train onsquad wrong.5.8 changing ratio in-domain ooduntil dtest train calibrator vary rangingfrom squad data sample qknown dcalib qunk dtest show difference betweenthe train calibrator maxprob ofthe graph difference close show thatmaxprob performs well homogeneous settings.however data source mixed thecalibrator outperforms maxprob significantly thisfurther support claim maxprob performspoorly mixed settings.5.9 allowing access qunkwe note finding hold alternate access sample fromqunk instead qknown training modelwith data maxprob achievesaverage whereas train calibrator achieve unsurprisingly train onexamples similar test data helpful wedo focus setting goal buildselective model unknown distributions.6 discussionin paper propose setting selectivequestion answer domain shift whichsystems must know abstain mixture ofin-domain unknown example combine important goal real-worldsystems knowing abstain handlingdistribution shift test time show modelsare overconfident example lead topoor performance setting train acalibrator data help correctfor problem focus question answering framework general extend toany prediction task graceful handle ofout-of-domain input necessary.across many task model struggle onout-of-domain input models train standard natural language inference datasets bowmanet generalize poorly distribution thorne naik achievinghigh accuracy out-of-domain data evenbe possible test data require ability thatare learnable training data geigeret adversarially choose ungrammatical text also cause catastrophic error wallaceet cheng case intelligent model would recognize itshould abstain inputs.traditional system typically natural ability abstain shrdlu recognize statement parse find ambiguous winograd qualm answer readingcomprehension question construct reason chain abstain find thatsupports answer lehnert system deploy real-world settingsinevitably encounter mixture familiar unfamiliar input work provide framework tostudy model judiciously abstain inthese challenge environments.reproducibility code data experimentsare available codalab platform http //bit.ly/35incah.acknowledgments work support bythe darpa ased program fa8650-18-27882 thank ananya kumar john hewitt daniter anonymous reviewer helpfulcomments insights.5693
copy module widely equip inthe recent abstractive summarization model facilitate decoder extract wordsfrom source summary generally encoder-decoder attention serve thecopy distribution guarantee thatimportant word source copy remain challenge work propose atransformer-based model enhance copymechanism specifically identify importance source word base degree centrality directed graph build bythe self-attention layer transformer weuse centrality source word guidethe copy process explicitly experimental result show self-attention graph provide useful guidance copy distribution.our propose model significantly outperformthe baseline method cnn/daily maildataset gigaword dataset.1 introductionthe explosion information expedite therapid development text summarization technology help grasp pointsfrom miscellaneous information quickly thereare broadly type summarization method extractive abstractive extractive approachesselect original text segment input forma summary abstractive approach create novel sentence base natural language generation techniques.in past year recurrent neural network rnns base architecture chopra nallapati seeet zhou obtain state-of-the-art result text summarization benefit longterm dependency high scalability transformerbased network show superiority rnns∗equal contribution.source u.s. senator block president barackobama nominee senior administration post atthe pentagon justice department protest aproposal house guantanamo detainee fortleavenworth prison midwestern home state ofkansasreference senator obama nominee protest guantanamotransformer:1 senator block pentago justice nomineestransformer copy senator block pentago justice poststransformer guided copy senator block obama nominee guantanamotop words self-attention nominee obama senator pentagon guantanamotable yellow shade represent overlap reference summary generate standardcopy mechanism miss importance word obama nominee many task include machine translation vaswani dehghani sentence classification devlin cohanet text summarization song al.,2019 zhang successful framework forthe summarization task pointer-generator network combine extractive andabstractive technique pointer vinyals al.,2015 enable model copy word thesource text directly although copy mechanismhas widely summarization task howto guarantee important token source arecopied remain challenge experiment find transformer-based summarizationmodel copy mechanism miss someimportant word show table word like nominee obama ignore standard copy mechanism tackle problem weintend clue importance ofwords self-attention graph.1356we propose self-attention guided copy mechanism sagcopy encourage summarizer copy important source word selfattention layer transformer vaswani al.,2017 build directed graph whose vertex represent source word edge define interms relevance score pair ofsource word dot-product attention vaswaniet query k.we calculate centrality source wordsbased adjacency matrix straightforward method textrank mihalcea tarau algorithm assume word receivingmore relevance score others likelyto important measure know indegree centrality also adopt another measureassuming word send relevancescore others likely critical namelyoutdegree centrality calculate source wordcentrality.we utilize centrality score guidance forcopy distribution specifically extend dotproduct attention centrality-aware function.furthermore introduce auxiliary loss compute divergence copy distribution centrality distribution toencourage model focus important words.our contribution threefold present guided copy mechanism basedon source word centrality obtain bythe indegree outdegree centrality measures.• propose centrality-aware attention aguidance loss encourage model payattention important source words.• achieve state-of-the-art public textsummarization dataset.2 related workneural network base model rush nallapati chopra nallapati zhou gehrmann al.,2020b achieve promise result abstractive text summarization copy mechanism gulcehre zhou enable summarizers withthe ability copy source target point vinyals recently pre-training base method devlin radford attract grow attention achieve state-of-the-art performance inmany task pre-training encoder-decodertransformers song dong lewis xiao al.,2020 show great success summarizationtask work explore copy moduleupon transformer-based summarization model.3 backgroundwe first introduce copy mechanism pointergenerator networks pgnet thesource text feed bidirectional lstm bilstm encoder produce sequence encode hidden state bilstm hi−1 step unidirectional lstm decoderreceives word embedding previous wordto produce decoder state lstm st−1 yt−1 context vector generate base theattention distribution bahdanau tanh whhi +wsst softmax =∑iαt vocabulary distribution pvocab allwords target vocabulary calculate follow pvocab softmax wast vact incorporate generating-copying switchpgen final probability distribution ofthe ground-truth target word pgenpvocab pgen pcopy pgen sigmoid yt−1 copy distribution pcopy determine attend time step previous work encoder-decoder attention weight serve asthe copy distribution pcopy xi=wαt loss function average negative loglikelihood ground-truth target word foreach timestep ×source textencoderdecodertarget textattention graphvocabularydistributionfinaldistributionpgen× pgencopydistribution× figure framework propose model based encoder self-attention graph calculate thecentrality score source word guide copy module.4 modelin section present approach enhancethe copy mechanism first briefly describethe transformer model copy mechanism.then introduce method calculate thecentrality score source word base theencoder self-attention layer finally incorporate centrality score copy distributionand loss function framework modelis show figure transformer copy mechanismscaled dot-product attention vaswani widely self-attention network attention softmax qkt√dk number column query matrixq matrix value matrix take encoder-decoder attention thelast decoder layer copy distribution softmax wsst twhhi√dk note multi-head attention obtainthe copy distribution multipleheads.4.2 self-attention-based centralitywe introduce approach i.e. indegree centrality outdegree centrality calculate thecentrality score source word base thelast encoder self-attention layer transformer.centrality approach propose investigate importance node social network freeman bonacich borgattiand everett kiss bichler al.,2011 degree centrality simplestcentrality measure distinguish asindegree centrality outdegree centrality freeman determine base theedges come leave node respectively.indegree centrality word proportionalto number relevance score incoming fromother word measure sumof indegree score graph-based extractive summarization method mihalcea tarau,2004 erkan radev zheng lapata,2019 centrality word proportionalto number relevance score outgo otherwords compute theoutdegree scores.formally directed graphrepresenting self-attention vertex theword edge represent encoder self-attention weight word theword where∑idi next introducethe approach calculate word centrality withthe graph g.we first construct transition probability matrixt follow j/∑jdi basic indegree centrality define scorei =∑jtj textrank mihalcea tarau,2004 inspire pagerank algorithm pageet calculate indegree centrality thesource word iteratively base markovchain scorei =∑jtj scorej scorei indegree centrality score vertexvi initial score astationary indegree centrality distribution compute score score iteratively takeat three iteration implementation.outdegree centrality measure much wordi contribute word directed graph scorei =∑jdi next incorporate source word centralityscore decode process.4.3 guided copy mechanismthe motivation word centrality indicate thesalience source word providethe copy prior knowledge guide copymodule focus important source words.we word centrality score extra input tocalculate copy distribution follow softmax wsst whhi wpscorei scorei indegree outdegree centralityscore i-th word source text aboveimplementation refer centralityaware dot-product attention.moreover expect important sourcewords draw enough encoder-decoder attention.thus adopt centrality-aware auxiliary loss toencourage consistency overall copydistribution word centrality distributionbased kullback-leibler divergence +λkl score experiments5.1 experimental settingwe evaluate model cnn/daily maildataset hermann gigaworddataset rush experiment areconducted nvidia adopt encoder layer decoder attention head hmodel byte pair encoding sennrich word segmentationis data pre-processing warm-startthe model parameter mass pre-trained basemodel1 train epoch convergence decode beam search witha beam size experimental resultswe compare propose self-attention guidedcopy sagcopy model following comparative models.lead-3 first three sentence article summary.pgnet pointergenerator network.bottom-up gehrmann asequence-to-sequence model augment abottom-up content selector.mass song sequence-tosequence pre-trained model base transformer.abs rush relies encoder nnlm decoder.abs+ rush enhance absmodel extractive summarization features.seass zhou control information flow encoder decoder theselective encode strategy.seqcopynet zhou extend thecopy mechanism copy sequence fromthe source.we adopt rouge score asthe evaluation metric show table table sagcopy outdegree indegreecentrality base guidance significantly outperformthe baseline model prove effectivenessof self-attention guide copy mechanism basic indegree centrality indegree-1 favorable consider rouge score computation complexity.besides rouge evaluation investigate guidance view loss function sample gigaword test wemeasure divergence centralityscore copy distribution calculatethe rouge-1 rouge-2 score figure divergence yield a1https //github.com/microsoft/mass1359models rg-1 rg-2 rg-llead-3 outdegree indegree-1 indegree-2 indegree-3 rouge score cnn/daily maildataset results mark take corresponding paper indegree-i denote indegree centrality obtain textrank i-iteration note thatindegree-1 basic indegree centrality equivalent textrank rg-1 rg-2 rg-labs outdegree indegree-1 indegree-2 indegree-3 experimental result gigaword dataset.higher rouge score show loss function reasonable.additionally visualize self-attentionweights learn model figure whichdemonstrates guidance process.5.3 human evaluationwe conduct human evaluation measure thequantify summary importance readability.we randomly select sample gigaword test annotator require togive comparison model summariesthat present anonymously result intable show sagcopy significantly outperform mass+copy term importance iscomparative term readability.win loss kappaimportance human evaluation result gigaworddataset denote generated summary sagcopy good mass+copy evaluatethe agreement fleiss kappa fleiss score copy score copy score copy score copy divergence rouge gigaword test sagcopy indegree-1 model eachpoint plot represent sample bottom plot show average rouge score differentkl values.serbsinnorthernkosovotakenasteptowardsbreakingofftieswiththeunserbsinnorthernkosovobreakofftieswithunserbsinnorthernkosovotakenasteptowardsbreakingofftieswiththeunkosovoserbsbreakofftieswithunserbsinnorthernkosovotakenasteptowardsbreakingofftieswiththeunserbsinnorthernkosovotakenasteptowardsbreakingofftieswiththeunserbsinnorthernkosovotakenasteptowardsbreakingofftieswiththeunserbsinnorthernkosovotakenasteptowardsbreakingofftieswiththeunencoder self-attention centrality copyself-attention graph scores distribution0.1200.0080.0640.1300.0090.0090.0410.0180.0270.0070.0720.0090.0020.129without guidancefigure guidance process sagcopy indegree model show keyword northern iscorrectly copy model.6 conclusionin paper propose sagcopy summarization model acquire guidance signal thecopy mechanism encoder self-attentiongraph first calculate centrality score foreach source word incorporate importance score copy module experimentalresults show effectiveness model forfuture work intend apply method othertransformer-based summarization models.acknowledgmentsthis work partially support beijingacademy artificial intelligence baai
paper propose problem deepquestion generation togenerate complex question require reason multiple piece information ofthe input passage order capture theglobal structure document facilitate reasoning propose novel framework first construct semantic-levelgraph input document encode semantic graph introduce anattention-based ggnn att-ggnn afterwards fuse document-level graphlevel representation perform joint training content selection question decoding hotpotqa deep-question centric dataset model greatly improve performance question require reasoningover multiple fact lead state-of-theart performance code publicly available http //github.com/wing-nus/sg-deep-question-generation.1 introductionquestion generation system play vital rolein question answer dialogue system andautomated tutor application enrich thetraining corpus help chatbots start conversation intriguing question automatically generate assessment question respectively.existing research typically focus generate factoid question relevant fact obtainable single sentence duan zhao exemplifiedin figure however less explore thecomprehension reason aspect question result question shallow notreflective true creative human process.people ability deep questionsabout event evaluation opinion synthesis orreasons usually form why-not input paragraph pago pago international airportpago pago international airport also know tafuna airport public airportlocated mile southwest central business district pago pago inthe village plain tafuna island tutuila american samoa anunincorporated territory united states.input paragraph hoonah airporthoonah airport state-owned public-use airport locate nautical mile southeast central business district hoonah alaska.question pago pago international airport hoonah airport onamerican territory answer yesinput sentence oxygen cellular respiration release photosynthesis usesthe energy sunlight produce oxygen water.question life process produce oxygen presence light answer photosynthesisa example shallow question generationb example deep question generationfigure examples shallow/deep evidenceneeded generate question highlighted.what-if require in-depth understanding input source ability reasonover disjoint relevant context e.g. whydid gollum betray master frodo baggins afterreading fantasy novel lord rings.learning deep question intrinsicresearch value concern human intelligenceembodies skill curiosity integration andwill broad application future intelligent system despite clear push towards answer deepquestions exemplify multi-hop reading comprehension commonsenseqa rajani generate deep question remain un-investigated thus clearneed push research towards generate deepquestions demand high cognitive skills.in paper propose problem deepquestion generation generate question require reason multiplepieces information passage figure show example deep question requiresa comparative reasoning disjoint piecesof evidence introduces three additionalchallenges capture traditional qgsystems first unlike generate question froma single sentence require document-level1464understanding introduce long-range dependency passage long second wemust able select relevant context meaningful question non-trivial involvesunderstanding relation disjoint piecesof information passage third need toensure correct reasoning multiple piece ofinformation generated question answerable information passage.to facilitate selection reason overdisjoint relevant context distill important information passage organize asemantic graph node extractedbased semantic role labeling dependency parsing connect different intra- intersemantic relation figure semantic relationsprovide important clue content arequestion-worthy reasoning perform e.g. figure entity pagopago international airport hoonah airporthave locate relation city unitedstates natural comparative question e.g. pago pago international airport andhoonah airport american territory toefficiently leverage semantic graph introduce three novel mechanism propose novel graph encoder incorporate anattention mechanism gated graph neuralnetwork ggnn dynamicallymodel interaction different semantic relation enhance word-level passageembeddings node-level semantic graph representation obtain unified semantic-awarepassage representation question decoding introduce auxiliary content selection taskthat jointly train question decoding assist model select relevant context thesemantic graph form proper reasoning chain.we evaluate model hotpotqa yanget challenging dataset thequestions generate reason text fromseparate wikipedia page experimental resultsshow model incorporate useof semantic graph content selectiontask improves performance large margin term automate metric section human evaluation section error analysis section validate semantic graph greatly reduce amount semanticerrors generated question summary contribution first work best ofour knowledge investigate deep question generation novel framework combine asemantic graph input passage generatedeep question novel graph encoder thatincorporates attention ggnn approach.2 related workquestion generation automatically generate question textual input rule-based technique usually rely manually-designedrules template transform piece giventext question heilman chali hasan,2012 method confine variety transformation rule template make approach difficult generalize neuralbased approach take advantage sequenceto-sequence seq2seq framework attention bahdanau model aretrained end-to-end manner require lesslabor enable good language flexibility compare rule-based method comprehensive survey find improvement propose sincethe first seq2seq model apply various technique encode answer information thus allow good quality answerfocused question zhou al.,2018 improve training viacombining supervise reinforcement learningto maximize question-specific reward yuan al.,2017 incorporate various linguistic featuresinto process however approach consider sentence-level qg.in contrast work focus challenge generate deep question multi-hop reasoningover document-level contexts.recently work start leverage paragraphlevel context produce good question andcardie incorporate coreference knowledgeto well encode entity connection across document zhao apply gated selfattention mechanism encode contextual information however practice semantic structure isdifficult distil solely self-attention theentire document moreover despite consideringlonger context work train evaluate squad rajpurkar weargue insufficient evaluate deep becausemore question shallow andonly relevant information confine singlesentence happy ball subject series parodyadvertisements saturday night live saturday night live abbreviate american late night livetelevision sketch comedy variety show create lorne michaels develop bydick ebersol happy ball subject series parody advertisementson show create answer lorne michaelsof aseriesisthe happy ballthe subjectvariety showof parodyadvertisementson saturday night livesaturday night liveabbreviatedat snlisan american late night livetelevision sketch comedycreatedby lome michaelsdevelopedby dick ebersolnsubjcoppobjpobjpobjcopcopnsubjpartmodpobjpobjconjpobjdepsimilarsimilarword-to-node attention𝑉𝑉𝑉𝑉𝑀𝑀𝐴𝐴prediction layer……cross attention…document questionanswersemantic-enricheddocument representationsfeatureaggregatorquestiondecoderdocumentencoder att-ggnn semantic graphencoder…node embeddings+ features+ answer tagsanswerencoder… …structure-awarenode representationscontextvectorcontentselection… …vocabulary sourcecopy switchpreviouswordsoftmax…………x kfigure framework propose model right together input example left themodel consist four part document encoder encode input document semantic graph encoder toembed document-level semantic graph att-ggnn content selector select relevant question-worthycontents semantic graph question decoder generate question semantic-enricheddocument representation left figure show input example semantic graph dark-colored node inthe semantic graph question-worthy node label train content selection task.3 methodologygiven document answer objective generate question satisfies maxqp document answer sequence word different previous work generate involve reason multiple evidence sentence ni=1 sentence also unlike traditionalsettings sub-span becausereasoning involve obtain answer.3.1 general frameworkwe propose encoder–decoder framework withtwo novel feature specific fusedword-level document node-level semanticgraph representation good utilize aggregatethe semantic information among relevant disjoint document context joint train overthe question decoding content selection tasksto improve selection reasoning relevant information figure show general architectureof propose model include three module semantic graph construction build dpor srl-based semantic graph give input semantic-enriched document representation employ novel attention-enhanced gated graphneural network att-ggnn learn semanticgraph representation fuse withthe input document obtain graph-enhanced document representation joint-task question generation generate deep question jointtraining node-level content selection wordlevel question decoding following describe detail module.3.2 semantic graph constructionas illustrate introduction semantic relation entity serve strong clue indetermining reasoningtypes include distill semantic information document explore semantic role labelling dependencyparsing base method construct semanticgraph refer appendix detail graphconstruction.• srl-based semantic graph task semantic role labeling identify semantic relation hold among predicate associated participant property màrquez al.,2008 include etc.for sentence extract predicate-argumenttuples toolkits1 tuple form subgraph tuple element e.g. argument location temporal node intertuple edge node different tuples ifthey inclusive relationship potentiallymention entity.1we employ state-of-the-art bert-based model shiand allennlp toolkit perform srl.1466• dp-based semantic graph employ biaffine attention model dozat manning sentence obtain dependency parsetree revise remove unimportant constituent e.g. punctuation mergingconsecutive node form complete semanticunit afterwards inter-tree edge betweensimilar node different parse tree constructa connect semantic graph.the leave side figure show example thedp-based semantic graph compared srlbased graph dp-based typically model morefine-grained sparse semantic relation discuss appendix section give performance comparison formalisms.3.3 semantic-enriched documentrepresentationswe separately encode document semantic graph rnn-based passage encoderand novel att-ggnn graph encoder respectively fuse obtain semantic-enriched document representation question generation.document encoding given input documentd employ bi-directionalgated recurrent unit encode context represent encoderhidden state context embedding concatenation bi-directional hidden states.node initialization define srl- anddp-based semantic graph unified thesemantic graph document heterogeneous multi-relation graph wherev denotegraph node edge connect wherenv number node edge inthe graph respectively node nvj=mvis text span associated node typetv start positionof text span edge also type thatrepresents semantic relation nodes.we obtain initial representation eachnode nvj=mv compute word-tonode attention first concatenate last hidden state document encoder direction document representation afterwards node calculatethe attention distribution word follow =exp attn ∑nvk=mnexp attn attention coefficient document embed word node v.the initial node representation give bythe attention-weighed embeddings itsconstituent word i.e. =∑nvj=mvβvj wordto-node attention ensure node capture notonly meaning constitute part alsothe semantics entire document noderepresentation enhance additionalfeatures embed answertag embed obtain enhanced initialnode representation h̃0v encoding employ novel attggnn update node representation aggregate information neighbor torepresent multiple relation edge baseour model multi-relation gated graph neural network ggnn provide separate transformation matrix eachedge type essential node topay attention different neighbor node whenperforming different type reasoning thisend adopt idea graph attention networks velickovic dynamically determine weight neighbor node message passing attention mechanism.formally give initial hidden state graphh0 h̃0i |vi∈v att-ggnn conduct layer state transition lead sequenceof graph hidden state wherehk |vj∈v state transition anaggregation function apply node tocollect message node directly connectedto neighbor distinguish theirincoming outgo edge follow =∑vj∈n wteijh =∑vj∈na wtejih denote incomingand outgo edge respectively wteij denote weight matrix correspond edgetype teij attention1467coefficient derive follow =exp attn ∑t∈n attn attn single-layer neural network implement andwa learnable parameter finally isused update node state incorporate theaggregated neighboring information.h k-th state transition denote finalstructure-aware representation node aggregation finally fuse semantic graph representation document representation obtain semanticenriched document representation question decoding follow fuse employ simple matching-based strategy forthe feature fusion function fuse word match small granularity node thatcontains word denote thenconcatenate word representation thenode representation hkvm i.e. hkvm corresponding node weconcatenate special vector close ~0.the semantic-enriched representation provide follow important information benefit question generation semantic information document incorporate semantic information explicitly concatenate semanticgraph encoding phrase information phrase isoften represent single node semanticgraph figure example therefore itsconstituting word align noderepresentation keyword information word e.g. preposition appear semanticgraph align special node vector mention indicate word carryimportant information.3.4 joint task question generationbased semantic-rich input representation generate question jointly train twotasks question decoding content selection.question decoding adopt attention-basedgru model bahdanau copying coveragemechanisms question decoder decoder take semantic-enrichedrepresentations theencoders attention memory generate theoutput sequence word time make thedecoder aware answer averageword embeddings answer initialize thedecoder hidden states.at decode step model learn toattend input representation compute context vector base current decoding state next copying probability pcpy calculate contextvector decoder state decoder inputyt−1 pcpy soft switch choose generate vocabulary copyingfrom input document finally incorporatethe coverage mechanism encourage decoder utilize diverse componentsof input document specifically step maintain coverage vector covt thesum attention distribution previous decoder step coverage loss compute penalize repeatedly attend location ofthe input document.content selection raise deep question human select reason relevant content tomimic propose auxiliary task contentselection jointly train question decoding.we formulate node classification task i.e. decide whether node involve inthe process i.e. appear reasoning chain raise deep question exemplifiedby dark-colored node figure feed-forward layer ontop final-layer graph encoder takingthe output node representation classification deem node positive ground-truth totrain content selection task content appear ground-truth question bridgeentity sentences.content selection help model identify thequestion-worthy part form proper reasoningchain semantic graph synergizes withthe question decode task focus thefluency generated question jointly trainthese task weight share inputrepresentations.14684 experiments4.1 data metricsto evaluate model ability generatedeep question conduct experiment hotpotqa yang contain ∼100kcrowd-sourced question require reasoningover separate wikipedia article question ispaired support document containthe evidence necessary infer answer thedqg task take supporting document alongwith answer input generate question.however state-of-the-art semantic parsing model difficulty produce accurate semantic graph long document thereforepre-process original dataset select relevantsentences i.e. evidence statement sentence overlap ground-truth question input document follow original datasplit hotpotqa pre-process data result example train andevaluation respectively.following previous work employ bleu1–4 papineni meteor lavie andagarwal rouge-l automate evaluation metric bleu measure theaverage n-gram overlap reference sentence meteor rouge-l specializebleu n-gram overlap idea machine translation text summarization evaluation respectively critically also conduct human evaluation annotator evaluate generation quality three important aspect deep question fluency relevance complexity.4.2 baselineswe compare propose model severalstrong baseline question generation.• seq2seq attn bahdanau thebasic seq2seq model attention takesthe document input decode question.• nqg++ zhou enhance theseq2seq model feature-rich encoder contain answer position information.• ass2s learns decode question answer-separated passage encodertogether keyword-net base answer encoder.• s2sa-at-mp-gsa zhao enhancedseq2seq model incorporate gate self-attentionand maxout-pointers encode rich passage-levelcontexts table also implement version coverage mechanism answerencoder fair comparison label b5.• cgc-qg another enhancedseq2seq model perform word-level contentselection generation i.e. make decisionson word generate copy richsyntactic feature dep.implementation details fair comparison weuse original implementation ass2s andcgc-qg apply hotpotqa baseline share document encoder andquestion decoder hidden unit dimension word embeddings initialize pre-trained glove pennington al.,2014 graph encoder node embeddingsize plus answer embeddings number layersk hidden state size othersettings train follow standard best practice2.4.3 comparison baseline modelsthe part table show experimentalresults compare baseline method wemake three main observations:1 version model andp2 consistently outperform baselinesin bleu specifically model dp-basedsemantic graph achieve absolute improvement bleu-4 +15.2 comparedto document-level model employsgated self-attention enhance withthe decoder show thesignificant effect semantic-enriched documentrepresentations equip auxiliary contentselection generate deep questions.2 result cgc-qg exhibit unusual pattern compare method achieve best meteor rouge-l worstbleu-1 among baseline cgc-qg performs word-level content selection observethat tend include many irrelevant word thequestion lead lengthy question tokenson average ground-truth questionsand model unanswerable orwith semantic error model greatly reducesthe error node-level content selection basedon semantic relation show table model train adam kingma ba,2015 mini-batch size learning rate initiallyset adaptive learning rate decay apply weadopt early stopping dropout rate bothencoder decoder attention mechanisms.1469model bleu1 bleu2 bleu3 bleu4 meteor rouge-lbaselinesb1 seq2seq attn nqg++ ass2s s2s-at-mp-gsa s2s-at-mp-gsa +cov +ans cgc-qg srl-graph dp-graph contexts semantic graph multi-relation attention multi-task performance comparison baseline ablation study best performance bold.model short contexts medium contexts long contexts averageflu cpx.b4 s2sa-at-mp-gsa cgc-qg semantic graph multi-task dp-graph ground truth human evaluation result different method input different length flu. rel. cpx.denote fluency relevance complexity respectively metric scale best srl-based dp-based semantic graph model achieve state-of-theart bleu dp-based graph perform slightlybetter +3.3 bleu-4 possible explanationis fail include fine-grained semanticinformation graph parsing often result node contain long sequence tokens.4.4 ablation studywe also perform ablation study assess impact different component model performance dp-based semantic graph model show rows a1–4 table result observe srl-version.• impact semantic graph notemploy semantic graph semanticgraph bleu-4 score model dramatically drop indicate necessity building semantic graph model semanticrelations relevant content deep qg.despite vital role result show generate question purely semantic graphis unsatisfactory posit three reason thesemantic graph alone insufficient convey themeaning entire document sequential information passage capture graph automatically build semantic graphinevitably contain much noise reason necessitate composite document representation.• impact att-ggnn using normal ggnn multi-relation attention encodethe semantic graph performance drop −3.61 bleu-4 compare model withatt-ggnn multi-task reveal thatdifferent entity type semantic relationsprovide auxiliary information need generatemeaningful question att-ggnn model incorporate attention normal ggnn effectively leverage information across multiplenode edge types.• impact joint training turn thecontent selection task multi-task thebleu-4 score drop showingthe contribution joint training auxiliarytask content selection show content selection help learn qg-aware graph representation section train modelto focus question-worthy content forma correct reason chain question decoding.4.5 human evaluationwe conduct human evaluation random testsamples consist short token token long token document three worker rate the300 generate question well ground-truth1470types examples s2sa-at- cgc-qg dp-graphmp-gsacorrect pred kemess mine colomac mine mine operate earlier mine operate early date kemess mine colomac mine semantic pred lawrence ferlinghetti american poet short story write error lawrence ferlinghetti american poet write short story name answer pred release date game release october revealing release date game name hurricane ghost pred video game michael gelling play promoter entity video game drew gelling play promoter redundant pred town walcha walcha belong town walcha belong unanswerable pred population city barack obama bear ranking population city barack obama bear table error analysis different method respect major error type exclude correct pred.and show example predicted question ground-truth question respectively semantic error question logic commonsense error answer revealing question reveal answer ghost entity question refers entity occur document redundant question contain unnecessaryrepetition unanswerable question error answer document.questions poor good threecriteria fluency indicate whether thequestion follow grammar accord withthe correct logic relevance indicateswhether question answerable relevantto passage complexity indicateswhether question involve reason multiple sentence document averagethe score raters question reportthe performance five model table unaware identity modelsin advance table show human evaluationresults validate model generate question good quality baselines.let explain observation detail compared s2sa-at-mp-gsa improvement salient term fluency +13.33 complexity +8.48 thatof relevance +6.27 reason thebaseline produce shallow question affect complexity question semantic error affect fluency observe similar result remove semantic graph semantic graph demonstrate ourmodel incorporate semantic graph produce question semantic error andutilizes context.• metric decrease general inputdocument becomes longer obviousdrop fluency input context long itbecomes difficult model capture questionworthy point conduct correct reasoning lead semantic error model alleviate problem introduce semantic graphand content selection question quality dropsas noise increase semantic graph thedocument become longer.4.6 error analysisin order good understand question generation quality manually check sampledoutputs list main error source table among semantic error redundant unanswerable noticeable errorsfor model however find baselineshave unreasonable subject–predicate–objectcollocations semantic error model especially cgc-qg large semanticerror rate among three method ittends copy irrelevant content input document model greatly reduce semanticerrors explicitly model semantic relation entity introduce typedsemantic graph noticeable error typeis unanswerable i.e. question correct answer passage cgc-qg remarkably produce unanswerablequestions model modelachieves comparable result s2sa-at-mp-gsa likely fact answerability require deep understanding document aswell commonsense knowledge issuescannot fully address incorporate semantic relation examples question generate bydifferent model show figure analysis content selectionwe introduce content selection task guidethe model select relevant content formproper reasoning chain semantic graph toquantitatively validate relevant content selection calculate alignment node attention1471last onethe secondstudio albumna naconfessions confessionsrobert shapiromatthew harta american teen musical comedy filmby christian rock band superchic thedisney filmof teenagedrama queenof teenagedrama queenbyfor walt disneypicturesby sarasugarmanisappearedisdirectedproducednsubjdeppobjnsubjnsubjnsubjpobj pobjpobjpobjpobjpobjpobjdepcopconjprepsimilarsimilarsimilarsimilardobjpickedpassage last picked second studio album christian rock band superchic appear disney film confessions teenage drama queen confessions teenage drama queen american teen musical comedy film direct sara sugarman produce robert shapiro andmatthew hart walt disney pictures graphquestion name american teen musical comedy second studio album christian rock band superchic appear question humans song last picked appear american teen musical comedy film direct sara sugarman question baseline direct american musical comedy film confession question last picked second studio album american teen musical comedy film direct sara sugarman produce robertshapiro matthew hart walt disney pictures figure example generated question average attention distribution semantic graph nodescolored darker attention best view color respect relevant nodes∑vi∈rn αviand irrelevant nodes∑vi /∈rn respectively condition single training jointtraining represent ground-truthwe content selection ideally successfulmodel focus relevant node ignore irrelevant reflect ratio between∑vi∈rn and∑vi /∈rn jointly train content selection thisratio compare singletask training consistent intuition aboutcontent selection ideally successful modelshould concentrate part graph help toform proper reasoning quantitatively validatethis compare concentration attention insingle- multi-task setting compute theentropy −∑αvi logαvi attention distribution find content selection increasesthe entropy average gainbetter insight figure visualize semantic graph attention distribution example wesee model attention darker node form reasoning chain thehighlighted path purple consistent thequantitative analysis.5 conclusion future workswe propose problem generate question require reason multiple disjointpieces information proposea novel framework incorporate semanticgraphs enhance input document representation generate question jointly trainingwith task content selection experiments onthe hotpotqa dataset demonstrate introducingsemantic graph significantly reduce semanticerrors content selection benefit selectionand reasoning disjoint relevant content lead question good quality.there least potential future directions.first graph structure accurately representthe semantic meaning document crucialfor model although dp-based srl-basedsemantic parsing widely advancedsemantic representation could also explore discourse structure representation noord knowledgegraph-enhanced text representation al.,2017 yang second method canbe improve explicitly model reasoningchains generation deep question inspire byrelated method jiang bansal,2019 multi-hop question answering.acknowledgmentsthis research support national research foundation singapore international research centres singapore funding initiative opinion finding conclusionsor recommendation express material arethose author reflect viewsof national research foundation singapore.1472
data-driven approach neural networkshave achieve promising performance natural language generation however neural generator prone make mistake e.g. neglect input slot value generatinga redundant slot value prior work refer thisto hallucination phenomenon paper study slot consistency build reliablenlg system slot value input dialogue properly generate outputsentences propose iterative rectificationnetwork improve general nlgsystems produce correct fluent response apply bootstrapping algorithmto sample training candidate reinforcement learn incorporate discrete reward relate slot inconsistency training.comprehensive study conductedon multiple benchmark datasets show thatthe propose method significantly reduce slot error rate strongbaselines human evaluation also confirm effectiveness.1 introductionnatural language generation critical component task-oriented dialogue system convert meaning representation i.e. dialogueact natural language sentence traditional method stent konstas andlapata wong mooney mostlypipeline-based divide generation process intosentence planing surface realization despitetheir robustness heavily rely handcraftedrules domain-specific knowledge addition generated sentence rule-based approachesare rather rigid without variance human language recently neural network base model dušek jurčı́ček ∗equal contributions.† corresponding author.input dainform name pickwick hotel pricerange moderate referencethe hotel name pickwick hotelis moderate price rangemissing moderate hotel name misplacethe pickwick hotel fort masonis moderate price range area table exmaple include mistake generation extract hotel dataset errors mark color miss misplace nguyen attract much attention implicitly learn sentence planningand surface realisation end-to-end cross entropy objective example dušek jurčı́ček employ attentive encoder-decoder model apply attention mechanism input slotvalue pairs.although neural generator trainedend-to-end suffer hallucination phenomenon balakrishnan examplesin table show misplacement error unseen slot area missing error slot nameby end-to-end trained model comparedagainst input motivated observation paper define slot consistency ofnlg system slot value input shallappear output sentence without misplacement.we also observe task-oriented dialoguesystems input mostly simple logicforms therefore enable retrieval-based methodse.g k-nearest neighbour handle themajority test case furthermore exist adiscrepancy training criterion crossentropy loss evaluation metric slot error rate similarly observe neural machinetranslation ranzato therefore isbeneficial training method integrate theevaluation metric objectives.98in paper propose iterative rectificationnetwork improve slot consistency forgeneral system consist pointerrewriter experience replay buffer pointerrewriter iteratively rectify slot-inconsistent generation data-driven systems.experience replay buffer fixed size collect candidate consist mistaken case train leveraging observation wefurther introduce retrieval-based bootstrappingto sample pseudo mistaken case candidate forenriching training data foster consistencybetween train objective evaluation metric reinforce williams incorporate slot consistency discrete reward intotraining objectives.extensive experiment show proposedmodel significantly outperform allprevious strong approach apply irnto improve slot consistency prior baseline notice large reduction slot error rates.finally effectiveness propose methodsare confirm bleu score caseanalysis human evaluations.2 preliminary2.1 delexicalizationinputs structure meaning representation i.e. consist type alist slot value pair slot value pair represent type information content whilethe type control style sentence improve generalization capability delexicalization technique dušek andjurčı́ček tran nguyen widelyused replace value reference sentence bytheir corresponding slot create pair ofdelexicalized input output templates.hence important step generate template correctly give input however step introduce miss misplacedslots model error unaligned training data balakrishnan juraska lexicalization followedafter template generate replace slot template correspond value da.2.2 problem statementformally denote delexicalized input consist anact type slot universal contain possible slot output template system sequence token word slot define slot extraction function consist element thetemplate slot-consistent system satisfy thefollowing constraint avoid trivial solution require hallucination phenomenon possible miss misplace slot value generated template hardto avoid neural-based approaches.2.3 knn-based systema knn-based system fknn composedof distant function template collect fromq delexicalized sentence train corpus.given input distance define function compute size evaluation system fknn first rank template distant function select beam size templates.3 architeturefigure show architecture iterative rectification network consist component pointer rewriter produce template improved performance metric experience replay buffer gather sample train data.the improvement slot consistency obtain iterative rewriting process assume iteration template notslot consistent input i.e. pointer rewriter iteratively rewrite recursion acertain number iteration reached.99input inform name phone name nice hotelinform name queen anne hotel phone name consistency measuretemplate dbexperience replay bufferinconsistency casebootstrappingℎ template input name nice hotel inform name phone phone number name phone name phone still inconsistentthe phone number queen anne hotel previous wordtemplate copyiterative rectification networkpointer rewriterfor𝑤 policygeneration name argmaxstatenlg system:1 knn2 neuronmistaken samplesfigure consist module experience replay buffer pointer rewriter experience replaybuffer collect mistaken case baseline template dash arrow whereas thepointer network output templates improved performance metric epoch rectification obtainssamples case training buffer train pointer rewriter metric slot consistencyusing policy-based reinforcement learning technique omit trivial connection brevity.3.1 pointer rewriterthe pointer rewriter train iterativelycorrect candidate give thiscorrection operation conduct time-recurrently.at position rewrite template astate represent past history pointerrewriter action take accord apolicy π.state autoregressive model particular lstm compute state give past statehj−1 input past output j−1hj φlstm hj−1 represent one-hot representation context representation input template describe ineq operation mean vector concatenation.action position output template action space consist category template copy copy token fromthe template word slot generation generate word slot position.for length-m input template action ajis therefore theaction sequence length-n output templateis copy model templatecopy attentive pointer decide position token copy candidate eachtoken candidate represent usingan embed position outputtemplate model utilize hidden statehj computes attentive weight thetokens weight token embeddingy follow φpr pprij softmax =∑1≤i≤mpprij learnable parameters.word slot generation another candidate position word slot froma predefined vocabulary action compute adistribution word slot belowpvocabj softmax distribution dependent state hjand matrix learnable.100algorithm interactive data aggregationinput template-db baseline system pointer rewriter total epoch number candidate size uoutput ideal pointer rewriter φpr.1 epoch← then6 end8 end9 epoch do10 bootstrapping training do15 then17 end19 end20 epoch← epoch+ endpolicy probability action canbe compute follow pprj w|hj pvocabj probability copy thei-th token input template positionj w|hj probability word orslot predict distribution pvocabjin weight real value between0 compute sigmoid operation sigmoid policy pointer rewriter greedy search decidewhether copying generate token.3.2 experience replay bufferthe experience replay buffer providingtraining sample three source ofsamples first off-the-shelf system second pointer rewriter inthe last iteration real mistakenalgorithm bootstrapping retrievalinput template-db total sample number maximum tolerance default .output pseudo sample randomselect then9 end11 end12 randomselect endsamples store case thebuffer sample off-policy case setc contain sample many iteration before.the third source sample bootstrappingalgorithm store ω.iterative data aggregation replay experience progressive reflect improvement iterative training therefore design iterative data aggregation algorithmin algorithm algorithm experiencereplay buffer define fixed size total epoch number itrandomly provide mistaken sample trainingpointer rewriter epoch importantly content vary eachepoch initially consist real mistakensamples baseline system line line8-th later gradually fill samplesfrom line line itssamples reflect general distribution trainingsamples template database line algorithm aggregate groupsof mistake sample line totrain model line retrieval relying solely onthe real mistaken sample expose system todata scarcity problem easy observe realsamples heavily bias towards certain slot number real mistaken sample besmall address problem introduce a101position hotel phone number phone token phone number name phone name mistakentemplatereferencetemplateextractive slotfunction wordfunction word noun phraseambiguityfigure correcting candidate give reference template infer simple rules.bootstrapping algorithm describe algorithm template database build delexicalized train corpus organize pairsof reference template turn algorithm first randomlysamples line pair train template data base every pair measure pair slot-inconsistentwith respect pair iswithin certain distance hyper parameter line usually asmall number select sample closeenough practice finally random sampling line insert return output suchbootstrapping process stop number ofgenerated sample reach certain limit k.these sample refer pseudosamples following represent wider coverage training sample real mistakensamples sample generaldistribution template semanticsare real mistaken case willdemonstrate experiment effectivelyaddresses data scarcity problem.4 training supervised learningand distant supervisionone idea behind propose model toconduct distant supervision action template copy generation diagram motivation figure training candidate yand reference give exact action thatconvert template infer fromthe template simple rule theinference firstly rule check reference tokenzj exist candidate output labeldc consisting represent whethertokens reference template existent/absentin candidate secondly rule locate original position candidate token reference template fordc finally action label policy isinferred i.we extracted supervisedlearning loss minimize followsjsl −l∑j=1log length ground truth compute likelihood action position jgiven state follow issue attempt utilize label produce distantsupervision training firstly importance ofevery token candidate different example noun phrase color blue critical shouldbe copy function word color oflittle relevance generate itself.however distant supervision treat same.secondly rule-based matching cause semanticambiguity dashed line color black lastly training criterion cross entropy directlyrelevant evaluation metric slot errorrate address issue reinforcementlearning obtain optimal actions.5 training policy-basedreinforcement learningin section describe another method trainirn apply policy gradient williams tooptimize model discrete rewards.5.1 rewardsslot consistency reward relate thecorrectness output template given ofslot-value pair output template generate slot-value pair extract input reward zero when102modelsf restaurant hotel laptop televisionbleu bleu bleu bleu errhlstm sclstm tgen dušek jurčı́ček aroa tran nguyen ralstm tran nguyen table experiment result four datasets baseline model meanwhile improvement overall prior method statistically significant t-test.they equal otherwise negative valueset cardinality difference thetwo followsrsc language fluency reward relate thenaturalness realized surface form response generation method following al.,2015a first train backward language modelon reference text train data perplexity surface form lexicalization output template measuredusing language model thereward language fluency follow −ppl distant supervision also measure reward distant supervision section length-n reference template reward isgiven follow −l∑j=1log inferred action label.the final reward action weighted sumof reward discuss γscrsc γlmrlm γdsrds γsc+γlm+γds equalvalue work reward observe thelast token utterance generated.5.2 policy gradientwe utilize supervised learning initialize model label extract fromdistant supervision convergence continuously tune model policy gradient describe section policy model φpritself generate sequence action arenot necessarily producesan output template compute slot consistencyreward language fluency reward ineq reward final reward iscomputed gradient back propagateis estimate reinforce as∇jrl ∗n∑j=1∇ denotes model parameter advantage function reinforce baseline experiment find thatb bleu perform good weaver tao,2001 trick simple averaging thelikelihood experiments6.1 experiment setupwe model performance four nlgdatasets different domain hotel andsf restaurant benchmark collect wenet laptop benchmarksare release dataset isevaluated five strong baseline method include hlstm sc-lstm wenet tgen dušek jurčı́ček aroa tran nguyen ralstm tran nguyen following priorworks evaluation metric consist bleu andslot error rate compute aserr total number slot andp number miss redundant slotsin generated template respectively.103modelsf restaurant televisionbleu bleu errhlstm ↓tgen dušek jurčı́ček ↓ralstm tran nguyen ↓table arrow emphasize absolutely improved performance contribute irn.methodlaptopbleu serirn +knn reward reward reward baseline bleu aggregation bootstrapping table ablation study reward upper part andtraining data algorithm part follow baseline performance reportedin tran nguyen open sourcetoolkits rnnlg1 tgen2 build system hlstm sclstm tgen reimplement baseline aroa ralstmsince source code available.6.2 main resultswe first compare model i.e. withall strong baseline metioned figure2 show propose model significantly outperform previous baseline bleu scoreand compared current state-of-the-artmodel ralstm achieve reduction time restaurant hotel laptop television datasets respectively.furthermore improve bleu score datasets respectively improvement bleu score canbe contribute language fluency reward rlm.to verify whether help improve slot consistency general model equipstrong baseline include hlstm tgen andralstm evaluate performance restaurant television datasets.as show table method consistently reduce errs also improve bleu score all1https //github.com/shawnwun/rnnlg.2https //github.com/ufal-dsg/tgen.modeltelevisioninformative naturaltgen real user trial generation quality evaluation informativeness naturalness.baselines datasets.in conclusion model notonly achieve state-of-the-art performancesbut also contribute improvement slotconsistency general systems.6.3 ablation studywe perform ablation experiment thesclstm+irn model laptop dataset understand relative contribution data aggregationalgorithms reward effect reward designsthe result table show removal slot consistency reward distant supervision rewardrds advantage function dramatically degradesser performance language fluency relate information baseline bleu reward alsohave positive impact bleu thoughthey small rds.6.3.2 effect data algorithmsusing candidate baseline degrades performance approximately baselinesclstm show incorporate candidate important model withoutbootstrapping even include candidate performance sclstm table show bootstrapping include genericsamples template database critical.6.4 human evaluationwe evaluate strong baseline tvdataset given input human eval104input recommend name crios family audio= nicam stereo size large reference text large crios television family feature nicam stereomistaken generation name family size screen cost price audio price revision name nice television family size screen audio revision name nice family size screen size audio revision name nice family family size screen size audio lexicalized form crios nice family large screen size nicam stereotable television dataset candidate hlstm output template eachiteration slot error mark color miss misplace score generated surface realization fromour model baseline term informativeness naturalness informativenessmeasures whether output utterance contain theinformation specify without insertionof extra slot miss input slot naturalness define whether mimic responsefrom human rating show ralstm outperformsralstm notably informativeness relatively by4.97 term naturalness improvement relative by1.50 meanwhile help improve performance tgen informativenessand naturalness.these subjective assessment consistent tothe observation table verify effectiveness propose method.6.5 case studytable present sample dataset showsa progress make given input thebaseline hlstm output third template miss slot audio insert slot price output template first iteration removal inserted price slot second iteration improve languagefluency progress slot-inconsistency thethird iteration achieve slot consistency whicha natural language though slightly different fromthe reference text generate lexicalization.7 related workconventional approach solve task aremostly pipeline-based divide sentenceplanning surface realisation dethlefs al.,2013 stent walker ohand rudnicky introduce class-based ngram language model rule-based reranker.ratnaparkhi address limitation ngram language model sophisticatedsyntactic dependency tree mairesse young employ phrase-based generator learnfrom semantically align corpus despite theirrobustness model costly create andmaintain heavily rely handcrafted rules.recent work dušek andjurčı́ček tran nguyen builddata-driven model base end-to-end learning.wen combine recurrent neuralnetwork base model rerankerto generate require utterance introduce novel sc-lstm additionalreading cell jointly learn gate mechanismand language model dušek jurčı́ček present attentive neural generator apply attention mechanism input tran nguyen employ refiner component select andaggregate semantic element produce theencoder recently domain adaptation wenet unsupervised learning bahuleyanet also receive much attention.we also inspire post-edit paradigm second-pass decoder improve translation quality.a recent method defines anauxiliary loss check object word existin expected system response task-orienteddialogue system would interest applythis auxiliary loss propose method onthe hand reinforce williams algorithm apply paper general incorporate metric suchas bleu.nevertheless end-to-end neural-based generator suffer hallucination problem hardto avoid generate slot-inconsistent utterance balakrishnan balakrishnan attempt alleviate issue employ treestructured meaning representation constrained105decoding technique however tree-shapedstructure require additional human annotation.8 conclusionwe propose iterative rectification network improve slot consistency general nlgsystems method retrieval-based bootstrapping introduce sample pseudo mistakencases train corpus enrich originaltraining data also employ policy-based reinforcement learn enable train modelswith discrete reward consistent evaluation metric extensive experiment show thatthe propose model significantly outperform previous method improvement include bothof correctness measure slot error rate andnaturalness measure bleu score humanevaluation case study also confirm effectiveness propose method.acknowledgementthis work support national naturalscience foundation china nsfc grant61976072 workwas first author internship atant financial thank anonymous reviewer forvaluable suggestion
task graph-to-text generation atproducing sentence preserve meaning input graph crucial defect thecurrent state-of-the-art model mess upor even drop core structural informationof input graph generate output wepropose tackle problem leveragingricher training signal guide modelfor preserving input information particular introduce type autoencodinglosses individually focus differentaspects a.k.a view input graph thelosses back-propagated well calibrate model multi-task training experiments benchmark graph-to-textgeneration show effectiveness approach state-of-the-art baseline ourcode available http //github.com/soistesimmer/amr-multiview.1 introductionmany text generation task take graph structure astheir input abstract meaning representation banarescu knowledgegraph database table example asshown figure amr-to-text generation isto generate sentence preserve meaningof input graph compose aset concept want-01 andtheir relation arg0 arg1 show figure kg-to-text generation produce sentence represent contain worldwide factoid information ofentities australia veil relation followedby effort graph-to-text generation tasksmainly focus effectively represent inputgraphs attention mechanism better transfer input knowledge decoder when⇤corresponding authorfollowedby want-01boy eat-01arg1arg0arg0girlabove veilaenirinto battleprecededbyaustriliacountry luncharg2arg1beautifulmodfigure graph meaning wantsthe beautiful girl lunch him. aknowledge graph carry meaning veilis australian novel sequel aenir wasfollowed battle. generate sentence taking amr-to-text generation example different graph neural network gnns beck song guoet ribeiro introduce well represent input amrs asequence-to-sequence model konstas later work lam,2019 wang show relationaware transformers achieve even good resultsthan gnns advance encode havelargely push state-of-the-art performance.existing model optimize maximizingthe conditional word probability referencesentence common signal training languagemodels result model learn toproduce fluent sentence crucial inputconcepts relation mess evendropped taking figure anexample model produce girl wantsthe convey opposite meaning graph particular bevery likely girl want appear much morefrequent want training corpus important issue itswide existence across many neural graph-to-text7988generation model hinder usability thesemodels real-world application dušek al.,2018 balakrishnan potential solution issue improve training signal enhance preserve ofstructural information however little work hasbeen explore direction probably design signal non-trivial.as first step towards goal propose toenrich training signal additional autoencoding loss standard autoencodingfor graph-to-sequence task require reconstruct parse input graph parsing algorithm type graph knowledgegraphs generalize graph typesor even exist make approach general across different type graph proposeto reconstruct different view input graph rather original graph viewhighlights aspect graph easy toproduce multi-task learning autoencoding loss view back-propagatedto whole model model betterfollow input semantic constraints.specifically break input graph aset triple form first view eachtriple want-01 arg0 figure contain pair entity relation asthe next step alignment graph nodesand target word generate grind viewinto target sentence reconstruction oursecond view linearization input graphproduced depth-first graph traversal thisview reconstruct token-by-token lastdecoder state overall first view highlight thelocal information triple relation secondview focus global semantic informationof entire graph.experiments amr-to-text generation andwebnlg gardent show ourgraph-based multi-view autoencoding loss improve performance state-of-the-art baseline bleu point without introduce parameter decoding besides human study show approach indeedbeneficial preserve concept relation input graphs.2 related workprevious work neural graph-to-text generation konstas song becket trisedya marcheggianiand perez-beltrachini caoand clark damonte cohen hajdik koncel-kedziorski hong song al.,2017 mainly study effectively representinput graph model train onlywith standard language modeling loss themost similar work propose encoder-decoder-reconstructor model formachine translation train totranslate source sentence target reference also translate target referenceback source text reconstruction wisemanet extend reconstruction loss tuet table-to-text generation atable contains multiple record severalfields.we study challenging topic toreconstruct complex graph structure rather asentence table propose general andeffective method reconstruct different complementary view input graph besides wepropose method breakdown whole graph sentence pair small piece edge word pair alignment train modelto reconstruct edge give correspondingword hand neither previousefforts leverage valuable information.our work remotely relate previous effort string-to-tree neural machine translation aharoni goldberg al.,2017 wang generatingtarget sentence syntactic tree onemajor difference goal producinggrammatical output preserve inputstructures besides multi-view reconstructionframework detachable component ofthe decoder state training extra errorpropagation structure prediction introduce conversely model generate treestogether target sentence thus extra effort introduce alleviate errorpropagation finally exist transition-based algorithm nivre convert tree parse intothe prediction transition action studyreconstructing graph commonparsing algorithm graph types.autoencoding loss input reconstruction wasmainly adopt sequence labeling task suchas name entity recognition liuet simile detection7989 sentiment analysis andsøgaard since input reconstruction notintuitively relate task autoencodingloss serve training signal differentfrom effort leverage autoencoding lossas mean preserve input knowledge besides study reconstruct complex graph proposinga general multi-view approach goal.3 base structure-aware transformerformally input graph-to-text generation canbe represent setof graph node corresponds graphedges edge triple show labelled relation connectednodes given graph choose recent relation-aware transformer model al.,2019 baseline generate ground-truthsentence contain samemeaning input graph exhibit state-ofthe-art performance amr-to-text generation.3.1 structure-aware transformer encodersimilar standard model vaswani al.,2017 structure-aware transformer encoderstacks multiple self-attention layer anembedding layer encode linearized graph nodes.taking l-th layer example consume thestates precede layer hl 11 theembedding layer state thenupdated weighted =xj2 hl 1j  ijwr1 vector representation relation node wr1are model parameter weight obtain relation-aware self-attention =exp = hl 1i wq  hl 1j  ijwr2 |pdh model parameter denote encoder-state dimension theencoder adopts self-attention layer represent concatenated top-layerhidden state encoder inattention-based decoding.compared standard model encoderintroduces vectorized structural information node pair given node pairvi generate information involvestwo main step first sequence graph edgelabels along path obtain direction symbol label todistinguish edge direction instance label sequence girl figure arg0 arg1 arg0 next step thelabel sequence treat single feature tokenand represent corresponding embeddingvector vector take vectorizedstructural information sincethere large number feature mostfrequent keep rest mappedinto special feature.13.2 standard transformer decoderthe decoder standard transformer architecture stack embeddinglayer multiple self-attention layer linear layer softmax activation generate targetsentences word-by-word manner targetword decoder state generate sequentially self-attention decoder sadecoder si 1 yi 1 sadecoder function decodingone step self-attention-based decoder.3.3 training language modeling losssame previous work model trainedwith standard language modeling loss minimize negative log-likelihood conditionalword probability lbase  xi2 yi|y1 yi 1  xi2 yi|si represent model parameters.4 multi-view autoencoding lossesfigure visualize training framework usingour multi-view autoencoding loss the1zhu also mention cnn-basedor self-attention-based alternative calculate thegpu memory consumption alternative timesmore baseline actually show comparableperformance.7990attentionencoder decoderthe want beautiful girl lunch himarg0arg1arg0want arg0 arg1 arg0 girl beautiful arg1 lunch arg2 want-01boy eat-01arg1arg0arg0girlluncharg2arg1beautifulmodmodarg1arg2languagemodelinglossview triple relationsview linearize graphfigure training framework multi-view autoencoding losses.attention-based encoder-decoder model thelanguage modeling loss baseline lossesare produce reconstruct proposedviews surround slash theinput graph view represent differentaspect input propose loss weexpect good refine model preserve thestructural information input graphs.4.1 loss reconstructing triple relationswith biaffine attentionour first view break input graph oftriples triple want-01 arg0boy figure contain pair node andtheir label relation next pre-generatedalignments graph node target wordsto ground graph triple onto target sentence.as illustrate slashed blue figure2 result contain several label eachconnecting word pair want represent local relation theircombination imply global input structure.for certain type graph node havemultiple word deal situation weuse first word associate graph nodeswhen ground graph edge onto target sentence next also connect first word eachgrounded entity word entityin order represent whole-entity informationin sentence taking edge followedby infigure example first grind ontothe target sentence connect word next create edge label compound word veil word battle toindicate associated entity mentions.for many task graph-to-text generation thenode-to-word alignment easily generatedfrom off-the-shell toolkits example amrto-text generation several aligners pourdamghani flanigan wang szubertet available link node towords knowledge graph alignment canbe produce simple rule-based matching anentity-linking system.the resulting structure labeled connecting word pair resemble dependency tree andthus employ deep biaffine model dozat andmanning predict structure thedecoder state specifically model factorize probability make twoparts unlabeled factor labeled giventhe decoder state representationfor word head modifier anyunlabeled factor calculate pass hidden state corresponding multi-layerperceptrons mlps rarc hi mlparc head rarc mi mlparc mod unnormalized score unlabeled factorswith possible head word give modifier yiare calculate  arci rarc hu|ararc mi +rarc hva rarc h concatenation rarc hi model parameter similarly representation word head orthe modifier labeled factor calculate by7991two additional mlps rlabel hi mlplabel head rlabel mi mlplabel mod unnormalized score relation labelsgiven head word modifier arecalculated  labeli rlabel hj lrlabel mi rlabel hj rlabel mi model parameter theoverall conditional probability labeled withlabel head word modifier calculatedby follow chain rule l|yi l|yj softmax  labeli softmax  arci subscript represent choose thex-th item correspond vector.as final step loss reconstruct thisview define negative log-likelihood alltarget ground triple lauto1 l|yi loss reconstructing linearizedgraphs transformer decoderas supplement first loss reconstruct local information grounded triple introduce second loss predict thewhole graph linearized sequence minimizethe loss graph structural information causedby linearization adopt algorithm base ondepth-first traversal konstas whichinserts bracket preserve graph scope linearize graph show dotted boxof figure node suffix represent word sens removed.one argue could directly predictthe original graph structural information would lose however type graphscan parsing algorithm theirunique property directed undirected root unrooted exact predictionwill hurt generality propose approach.conversely solution general linearizationworks type graph figure observe inserted bracket clearlyinfer original graph structure besides previouswork iyer konstas hasshown effectiveness generate linearizedgraphs sequence graph parsing alsoconfirms observation.given linearized graph represent sequence token tokenxi graph node edge label insertedbracket adopt another standard transformerdecoder sadecoderg produce sequence sadecoderg ti 1 xi 1 denote concatenatedstates target sentence equation theloss reconstruct view define thenegative log-likelihood linearized graph lauto2  xi2 xi|ti represent model parameters.4.3 discussion comparisonour autoencoding module function detachablecomponents base target-side decoder state thus bring main benefit first ourapproaches orthogonal recentadvances kipf welling veličković encoder side forrepresenting graph also flexible otherdecoders base multi-layer lstm hochreiterand schmidhuber extra error propagation introduce asour approach affect normal sentencedecoding process.in addition different aspect lossesfocus merit disadvantagesover term train speed calculate loss fast loss predict triple relation parallel feasible generate linearizedgraph besides calculate loss suffers lessvariances triple relation agnostic thetoken order determine input file conversely graph linearization highly sensitive inputorder major merit loss generality node-to-word alignment easilyobtained especially multi-lingual tasks.4.4 training autoencoding lossesthe final training signal propose autoencoding loss formalize lfinal lbase ↵lauto1  lauto2 coefficient proposedlosses coefficient value select adevelopment experiment.5 experimentswe study effectiveness autoencodingtraining framework amr-to-text generationand kg-to-text generation bleu papineni al.,2002 meteor denkowski lavie score report comparison followingprevious work multi-bleu.perlfrom moses2 bleu evaluation.5.1 dataamr datasets3 take ldc2015e86 thatcontains instance fortraining development testing respectively.each instance contain sentence amrgraph following previous work standardamr simplifier konstas preprocessour graph take ptb-based stanfordtokenizer4 tokenize sentence node-toword alignment produce aligner pourdamghani datasetfor primary experiment also report ournumbers ldc2017t10 late version amrdataset instancesfor training development testing respectively.webnlg gardent datasetconsists training developmentkg-text pair subgraph dbpedia5 contain relation triple testset part contain entity relation belong tothe dbpedia category trainingdata unseen entity relationscome unseen category previous work evaluate model part also relevant setup.we follow marcheggiani perez-beltrachini preprocess data obtain alignment sentence amethod base heuristic string matching formore detail remove abbreviation node york change york find first phrase the2http //www.statmt.org/moses/3https //amr.isi.edu/download.html4https //nlp.stanford.edu/software/tokenizer.shtml5https //wiki.dbpedia.org/figure development result ldc2015e86.sentence match long prefix node.as result find match nodes.5.2 settingsfor model hyperparameters follow settingof baseline selfattention layer adopt head eachlayer size embed hidden statesare batch token-size theembeddings randomly initialize updatedduring training model train adam kingma with 1 byte-pair encoding sennrichet operation apply alldatasets gpus experiments.for approach multi-layer perceptronsfor deep biaffine classifier equations and10 take layer unit transformerdecoder equation predict linearizedgraphs take embedding hidden sizesas baseline decoder equation development resultsfigure show devset performance usingeither loss triple relation loss linearizedgraph different coefficient show thebaseline performance coefficient equal large improvement term bleuscore increase coefficient either lossfrom result indicate effectiveness ofour autoencoding training framework performance model either loss slightly goesdown increase coefficient oneunderlying reason over-large coefficientwill dilute primary signal language modeling relevant bleu metric.particularly observe high performanceswhen respectively andthus coefficient value forthe remain experiments.7993model bleu timelstm konstas –grn song –dcgcn –ra-trans-sa –ra-trans-f-ours loss triple relation loss linearize graph –grn –lstm –dcgcn ensemble –table main test result ldc2015e86 numberssuch mean number extra silver databeing ensemble indicate model ensemble.5.4 main resultstable show main comparison result withexisting work amr-to-text generation time represent average time second fortraining step first group correspond tothe reported number previous model thisdataset main difference encoderfor presenting graph lstm konstas apply multi-layer lstm linearized amrs song dcgcn al.,2019 adopt graph neural network encode original amrs ra-trans-sa best performingmodel self attention tomodel relation path node pair.the second group report system thera-trans-f-ours baseline implementationof feature-based model itshows highly competitive performance thisdataset applying loss alone achieve improvement bleu point loss aloneobtains point loss possiblereason loss reconstructthe whole linearize graph provide informative feature using loss observeroughly gain term bleu indicate loss complementary.regarding meteor ra-trans-sa report high among previously report number ra-trans-f-ours baseline thatis slightly ra-trans-sa applying loss1 loss alone give number and36.1 respectively using loss approachachieves good ra-trans-sa.model recall ra-trans-f-ours human study recall input relationson ldc2015e86.regarding training speed adopt loss double amount time compare thebaseline much slow loss isbecause biaffine attention calculation different word pair parallelizable notfor produce linearized graph using lossestogether observe moderately longer trainingprocess slower baseline pleasenote autoencoding framework affectsthe offline training procedure leave onlineinference process unchanged.the last group show additional high numbersproduced system ensemble multiple model and/or additional silver data theysuffer problem require massivecomputation resource take long time fortraining leave explore additional silver dataand ensemble work.5.5 quantitative human study preservinginput relationour multi-view autoencoding framework atpreserving input relation thus conducta quantitative human study estimate aspect.to first extract interaction asubject predict object correspondingto fragment pred arg0 subj arg1obj graph check howmany interaction preserve output model reason consider type ofinteraction come fold first conveyfundamental information form backbone sentence second easily extractedfrom graph evaluate human judges.as show table choose amrsentence pair conduct study compareour model baseline term recallnumber show percent preserved interaction determine sentence preserve aninteraction people backgroundto make decision choose majorityvote human judgement interaction baseline preserve withour multi-view autoencoding loss more7994 recommend-01 arg0 arg1 go-02 arg0 purpose see-01 arg0 arg1 person arg0-of have-rel-role-91 arg1 arg2 doctor arg2 recommend doctortoo doctor approach recommend seeyour doctor country arg0-of have-03 arg1 policy consist-of target-01 arg1 aircraft arg0-of traffic-01 arg1 drug time current domain country wiki colombia name name colombia colombia country currentlyhas policy target drug trafficking aircraft colombia country drugtrafficking policy approach colombia country withthe current policy target drug traffickingaircraft example system outputs.interactions preserve confirmsthe effectiveness approach.5.6 case studyas show table demonstrate several typical example human study forbetter understand framework help preserve structural input information exampleincludes input reference sentence baseline output baseline generatedsentence approach approach first example baseline output dropsthe predicate recommend fail preserve fact subject reason occursfrequently training corpus otherhand extra signal produce multi-viewframework enhance input semantic information guide model generate correct sentencemodel bleura-trans-f-ours loss edge label loss edge label linearization ablation study views.with exact meaning input amr.the second example show similar situation baseline generate natural shortsentence drop important informationfrom input graph result informationloss result sentence conveys oppositemeaning drug trafficking policy input target drug trafficking aircraft atypical problem suffer many neural graph-tosequence model multi-view framework helpsrecover correct meaning policy target fordrug trafficking aircraft ablation studyas show table conduct ablation studyon ldc2015e86 analyze important eachpart input graph framework forloss test situation edge labelsare available result observe largeperformance drop bleu point isquite intuitive edge label carry importantrelational knowledge connectednodes therefore discard label causeloss significant semantic information.for loss also observe large performancedecrease edge label drop confirmingthe observation loss addition studythe effect random graph linearization theorder pick child random rather thanfollowing left-to-right order stage thedepth-first traversal procedure motivation isto investigate robustness loss regardinginput variance organize input order suchas alphabetical order child beavailable certain graph-to-sequence task weobserve marginal performance drop less than0.1 bleu point indicate approach isvery robust input variance likely becausedifferent linearization result still indicate samegraph besides previous study konstas al.,2017 show similar observation.7995model bleu meteordcgcn –ra-trans-cnn loss loss main test result ldc2017t10.model bleu meteoradapt loss loss main test result webnlg5.8 main results ldc2017t10table compare result ldc2017t10 withthe high number report single modelswithout extra silver training data ra-trans-cnn isanother model adopt convolutional neural network lecun tomodel relation path node pair ra-trans-f baseline achieve comparablescore ra-trans-cnn approach improve baseline nearly bleu point indicate superiority.regarding meteor score advantage previous state-of-the-art systemon dataset large point onldc2015e86 since ldc2017t10 almost onetime training instance ldc2015e86 conclude problem droppinginput information effectively reducedby simply supervised data aresult approach still effective largerdataset conclusion also confirm bycomparing gain approach amrdatasets regard bleu score main results webnlgtable show comparison result withprevious result webnlg testset adapt gardent base standardencoder-decoder architecture byte pair encode sennrich andit best system challenge gcnec marcheggiani perez-beltrachini arecent model graph convolution network kipf welling encode kgs.our baseline show comparable performancewith previous state based thisbaseline apply either loss lead significantimprovement combination bring gainof bleu point although baselinealready achieve high bleu score thegains task still comparable thoseon amr-to-text generation observation mayimply problem miss input structuralknowledge ubiquitous among many graphto-text problem result approach canbe widely helpful across many tasks.following previous work also report meteor score approach show gain baseline final number iscomparable adapt similar gain onthe bleu metric loss comparable loss2 regard meteor combination moreuseful apply own.6 conclusionwe propose reconstruct input graph autoencoding process encourage preserve inputsemantic information graph-to-text generation.in particular auxiliary loss recoveringtwo complementary view triple relation linearized graph input graph introduce sothat model train retain input structuresfor good generation training framework isgeneral different graph type experiments ontwo benchmark show effectiveness ourframework automatic bleu metricand human judgements.acknowledgeboth jinsong support bythe national program china no.2019qy1803 national natural science foundation china scientificresearch project national language committee china yb135-49 zhang wassupported joint research program betweenbritedreams robotics westlake university wethank anonymous reviewer constructive suggestions.7996
extracting lexico-semantic relation graphstructured taxonomy also know taxonomy construction beneficial variety application recently graph neural network show powerful successfully tackle many task however attempt exploit gnnto create taxonomy paper propose graph2taxo gnn-based cross-domaintransfer framework taxonomy construction task main contribution learnthe latent feature taxonomy constructionfrom exist domain guide structurelearning unseen domain also propose novel method direct acyclic graph generation taxonomy construction.specifically propose graph2taxo usesa noisy graph construct automaticallyextracted noisy hyponym-hypernym candidatepairs taxonomy knowndomains training learned model isthen generate taxonomy unknown domain give term thatdomain experiments benchmark datasetsfrom science environment domain showthat approach attain significant improvement correspondingly state art.1 introductiontaxonomy exploit many natural language processing application question answering harabagiu query understand recommendationsystems friedrich zanker automatic taxonomy construction highly challengingas involve ability recognize setof type hypernym text corpus instance hyponym type is-a hypernymy hierarchy types.existing taxonomy e.g. wordnet milleret complete taxonomies specific many domain either entirely absent missing paper focuson construction taxonomy unseen domains1 since taxonomy express directedacyclic graph dags suchanek taxonomy construction formulate daggeneration problem.there considerable research graphneural networks sperduti starita,1997 gori year particularly inspire convolutional brunaet graph convolution operationswere define fourier domain similarspirit convolutional neural network cnns method aggregate neighbor informationbased connectivity graph createnode embeddings apply successfully many task matrix completion berg manifold analysis monti prediction community bruna knowledge graph completion shang representation networknodes hamilton kipf welling,2017 best knowledge noattempt exploit taxonomy construction.our propose framework graph2taxo firstto show gnn-based model crossdomain noisy graph substantially improve thetaxonomy construction unseen domain e.g. environment exploit taxonomy ormore domain e.g. food task describe detail section another novelty approach thefirst apply acyclicity constraint-based dagstructure learn model zheng yuet taxonomy generation task.the input graph2taxo cross-domain1by unseen domain refer domain taxonomy available system.2199noisy graph construct connect noisy candidate is-a pair extract largecorpus standard linguistic pattern-based approach hearst noisy becausepattern-based approach prone poor coverage well wrong extraction addition itis cross-domain noisy is-a pairsare extract large-scale corpus contain collection text multiple domains.our propose neural model directly encode thestructural information noisy graph theembedding space since link domainsare also model structuralinformation multiple domain also crossdomain information.we demonstrate effectiveness proposedmethod science environment datasets bordea show significant improvement f-score state art.2 related worktaxonomy construction also know taxonomyinduction well-studied problem theexisting work follow sequential step construct taxonomy text corpus wang al.,2017 first is-a pair extract patternbased distributional method taxonomyis construct is-a pairs.the pattern-based method pioneer hearst detect is-a relation term pair appearance sentence lexical pattern linguisticrules ritter snowet represent term-pair asthe multiset dependency path connect theirco-occurrences corpus also regardedas path-based method.an alternative approach detect is-a relation distributional method baroni al.,2012 roller distributionalrepresentation term directly predict relations.as step taxonomy construction usingthe extract is-a pair approachesdo incrementally attach term snowet shen alfarone davis,2015 firstto present reinforcement learning base approach name taxorl task term pair representation taxorl obtain thepath lstm encoder word embeddings bothterms embeddings features.recently dash argue strictpartial orders2 correspond directly dags.they propose neural network architecture call strict partial order network spon thatenforces asymmetry transitive property assoft constraint empirically show thatsuch network produce good result detectinghyponym-hypernym pair number datasetsfor different language domain supervised unsupervised settings.many graph-based method kozarevaand hovy regard thetask hypernymy organization hypernymydetection problem follow graph pruningproblem graph prune task various graphtheoretic approach optimal branching algorithm velardi edmond algorithm karp tarjan algorithm tarjan year addition wang mention several graphbased taxonomy induction approach contrast approach formulate taxonomy constructiontask generation problem instead anincremental taxonomy learning differentiate compare exist method addition approach theknowledge exist domain bansal al.,2014 build taxonomy ofmissing domains.3 graph2taxo frameworkin section first formulate problem statement introduce propose graph2taxoframework solution describe individual component framework detail alongwith justification component come together solution.figure illustration gnn-based crossdomain transfer framework taxonomy construction.2a binary relation transitive irreflexive asymmetric.22003.1 problem definitionthe problem address paper give alist domain-specific term target unseen miss domain input construct ataxonomy target unseen domain otherwords problem address paper howto organize term taxonomy.this problem abstract asfollows given large input corpus ofgold taxonomy ggold know domain different target domain task tolearn model train corpus taxonomy known domain construct multipletaxonomies target unseen domains.as solution aforementioned problem propose gnn-based cross-domain transferframework taxonomy construction figure1 call graph2taxo consist crossdomain graph encoder generator.the first step propose approach tobuild cross-domain noisy graph input toour graph2taxo model step extractcandidate is-a pair large collection inputcorpora span multiple domain weused output panchenko whichis combination standard substring matchingand pattern-based approach since patternbased approach rigid correspondingoutput suffer recall i.e. missingis-a pair also contain incorrect i.e. noisy pair ambiguity language richnessin syntactic expression structure inputcorpora example consider phrase animals wuet note pattern-based approach willextract is-a rather is-a animal noisy is-a pair construct adirected graph ginput vinput einput cross-domain noisy graph vinput denotesa term einput belong list extracted noisy is-apairs input document collection span multiple domain therefore einput intradomain edge also cross-domain edge figure subgraph generation modelwhich large cross-domain noisy graphas input given list term target unseen domain learn taxonomystructure corresponding domain dag.graph2taxo take advantage additional knowledge form previously know gold taxonomy ggold nknown train alearning model inference phase modelreceives list term target unseen domain build taxonomy theinput term nknown denote numberof previously know taxonomy thetraining phase.this problem distil direct acyclic substructure taxonomy many domain alarge cross-domain noisy graph challenge relatively overlap noisyedges einput true edge available taxonomy hand.the follow section describe proposedcross-domain graph encoder generator detail.3.2 cross-domain graph encoderthis subsection describe cross-domain graphencoder figure embed generation.this embed generation algorithm twostrategies namely neighborhood aggregation andsemantic clustering aggregation.3.2.1 neighborhood aggregationthis first strategy embed generation rn×n adjacencymatrix noisy graph ginput thesize vinput represent feature representation node l-th layer thush rn×dl denote intermediate representation matrix initial matrix randomlyinitialized standard normal distribution.we adjacency matrix noderepresentation matrix iteratively update therepresentation particular node aggregate representation neighbor doneby formally layer gilmeret hamilton employ general message-passing architecturewhich consist message propagation functionm message neighbor vertexupdate function message pass work viathe follow equation ml+1v hl+1v ml+1v denote neighbor node andm message addition following2201definitions function =∑u∈n avuhlu ml+1v ml+1v hlvθl rdl×dl+1 denote trainable parametersfor layer represent activation function.let identity matrix theinformation aggregation strategy describe abovecan abstract gnnl semantic clustering aggregationthis second strategy embed generation operate outputof previous step learn representationsfrom previous step highly likely beuniformly distribute euclidean space butrather form bunch cluster regard wepropose soft clustering-based pooling-unpoolingstep semantic cluster aggregate forlearning good model representation essence step share similarity information anypair term vocabulary.analogous auto-encoder pool layeradaptively create small cluster graph comprising cluster node whose representation learn base trainable cluster assignment matrix idea assignmentmatrix first propose diffpool yinget approach hand unpooling layer decode cluster graph theoriginal graph cluster assignmentmatrix learn pooling layer learn semantic cluster node think bridge node different clustersto messages.mathematically speak learn soft clusterassignment matrix rn×nc layer thegnn model number clusters.each corresponds node inlayer column correspond thenc cluster first step pool layer usesthe adjacency matrix node feature matrixh generate soft cluster assignment matrix softmax gnnl cluster softmax row-wise softmax function θlcluster rdl×nc denote trainable parameter gnnl cluster.since matrix calculate base nodeembeddings node similar feature localstructure similar cluster assignment.as final step pool layer generatesan adjacency matrix cluster graph anda embed matrix contain cluster noderepresentations follow rnc×dlac tasl rnc×nca operation within small clustergraph l+1c gnnl rnc×dl+1to propagate message neighboringclusters trainable parameter gnnl areθl rdl×dl+1 pass cluster information originalgraph unpooling layer restore originalgraph cluster assignment matrix follow l+1c rn×dl+1the output pooling-unpooling layer result node representation possess latentcluster information finally combine neighborhood aggregation semantic clustering aggregation strategy residual connection concate concatemeans concatenate matrices.h output pooling-unpooling step.figure illustration generator.3.3 generatorthe generator take noisy graphginputand representation vocabulary term output section input encode acyclicity as2202a soft-constraint describe outputsa distribution edge within ginput encodesthe likelihood true is-a relationship outputdistribution finally induce taxonomy i.e. dags is-a relationships.in training step generator apply domain figure noisygraph subgraph ginput training sample generate forthat domain denote number hypo hyper pair belong edge training also know label vector label pair base onwhether belong gold know taxonomy.3.3.1 edge predictionfor edge within noisy graph daggenerator estimate probability edgerepresents valid hypernymy relationship ourmodel estimate probability convolution operation illustrate figure edge hypo hyper first stepthe term embeddings edge feature concatenate follow vpair concate vhypo vhyper vfeas vhypo vhyper embeddingsfor hypo hyper node section vfeas denote feature vector edge hypo hyper include edge frequency andsubstring feature substring feature includesends contains prefix match suffix match length long common substring lengthdifference boolean feature denote whetherlcs vinput term not.inspired conve model dettmers well know convolution base algorithm linkprediction apply convolution operationon vpair convolution operation since itincreases expressiveness generatorthrough additional interaction participate embeddings.for convolution operation make different kernel parameterized convolution operation calculate follow vpair vpair vpair =k−1∑τ=0ωc v̂pair denote kernel width denotes thesize vpair denote position start thekernel operation kernel parameter aretrainable addition v̂pair denote paddedversion vpair wherein padding strategy asfollows vpair bk/2czeros side hand |k|is even bk/2c zero beginning bk/2c zero vpair bvaluecreturns floor value.each kernel generate vector accord toequation arec different kernel result generation different vector whichare concatenate together form vectorvc concatenate probability hypo hyper give edge hypo hyper express hypernymy relationship estimate followingscoring function hypo hyper sigmoid vtcw denote parameter matrix fullyconnected layer illustrate figure loss calculation make useof differentiable loss huang precision =∑nt−1t=0 labelt∑nt−1t=0 ptrecall =∑nt−1t=0 labelt∑nt−1t=0 labeltlf1 precision× recallprecision recall3.3.2 constraintthe edge prediction step alone guarantee generated graph acyclic learningdag data np-hard problem chickering,1995 chickering effect ofthe first work formulate acyclic structurelearning task continuous optimization problemwas introduce zheng paper author note trace ofbk denote non-negative adjacency matrix rn×n count number oflength-k cycle directed graph hence positive entry within diagonal suggest theexistence cycle word nocycle if∑∞k=1∑ni=1 calculatingbk every value i.e.repeated matrix exponentiation impractical andcan easily exceed machine precision solve this2203problem zheng make taylorseries expansion =∑∞k=0bkk show thata non-negative matrix ∞∑k=1n∑i=1 make sure constraint useful arbitrary weight matrix positive negative value hadamard product isused lead follow theorem.theorem zheng matrix ∈rn×n ea◦a represent trace matrix represent hadamard product equal matrixexponential b.since matrix exponential available deep learning framework al.,2019 propose alternative constraint practically convenient follows.lemma forsome complex since α|λ| ≤ec|λ| constraint theorem berelaxed state follow hyper-parameter.finally augmented lagrangian approach propose combined loss function +ρ2h hyper-parameters thebackpropagation gradient pass backto domain intra-domain crossdomain edge fromginput update parameters.4 experimentswe evaluate graph2taxo semeval-2016 task13 taxonomy extraction evaluation3 otherwiseknown texeval-2 task bordea allexperiments implement pytorch code ispublicly available http //github.com/ibm/gnn-taxo-construction.3semeval-2016 task http //alt.qcri.org/semeval2016/task13domain source escience wordnet eurovoc dataset statistic texeval-2 task obtain bordea vertices andedges columns represent structural measure taxonomy english language only.4.1 benchmark datasetsfor experiment english environmentand science taxonomy within texeval-2benchmark datasets datasets comewith training data list term thetask build meaningful taxonomy theseterms science domain term come wordnet eurovoc manually construct taxonomy henceforth refer combine whereas theterms environment domain come eurovoctaxonomy table show dataset statistics.we choose evaluate propose approach onenvironment science taxonomy becausewe want compare approach exist state-of-the-art system name taxorl maoet well taxi system texeval-2 task note thesame datasets taxorl fortexeval-2 task.in addition dataset bansalet gold taxonomy source ofadditional knowledge ggold ggold ≤nknown know apriori dataset aset medium-sized full-domain taxonomy consist bottom-out full subtrees sample fromwordnet contain taxonomy total.to test model taxonomy prediction andto remove overlap remove taxonomyfrom ggold term overlap setof provide term science environment domain within texeval-2 task weget non-overlapping taxonomy total partition ratio create training validation datasets respectively.4.2 experimental settingswe experiment different settings.in train different noisy inputgraph gold taxonomy mentionedbefore evaluate science environ2204science science science science environment combined eurovoc wordnet average eurovoc model febaseline results texeval-2 task taxonomy extraction evaluation a.k.a texeval-2 first four rowsrepresent participate system texeval-2 task whose performance take bordea illustrate performance reinforcement learning system partialand full respectively graph2taxo1/2 represent propose algorithm setting describedin section result report round decimal places.ment domain within texeval-2 task firstsetting input taxorl maoet fair comparison input oftaxorl consists term pair associated dependency path information whichhas extract three public web-basedcorpora graph2taxo make theterm pair create noisy input graph.in second setting data4 provide bytaxi panchenko comprisesof list candidate is-a pair extract base onsubstrings lexico-syntactic pattern usedthese noisy candidate pair create noisy graph.a graph2taxo model train thenoisy graph obtain settings.in test phase candidate term-pairs forwhich term present test vocabulary score trainedgraph2taxo model threshold apply candidate pair score beyond threshold accumulate together predicted taxonomy gpred notice different optimal threshold different task good performance tune threshold however choose harder task prove modelhas good performance others even simply threshold addition wespecify hyper-parameter range experiment learning rate number kernel number cluster finally adam optimizer kingma experiments.evaluation metrics given gold taxonomy4data available http //panchenko.me/data/joint/taxi/res/resources.tgzggold part texeval-2 benchmark dataset predicted taxonomy gpred proposedgraph2taxo approach evaluate gpred usingedge precision edge recall f-score measuresas define bordea hyper-parameterswe following hyper-parameter configuration train model dropout number kernel kernel size learn rate initial embed sizeto loss function addition number cluster isset experiment scenariowherein input resource come taxi onlyhyponym-hypernym candidate pair observe morethan time create noisy graph also pooling unpooling layer ourexperiments.we dropout place endof cross-domain encoder module otherafter conv1d operation model trainedusing nvidia tesla p100 gpus.4.4 results discussionstable show result texeval-2 taskevaluation science environment domains.the first represent string-based baselinemethod bordea exploit termcompositionality hierarchically relate term forexample extract pair statistics department department provide wikipediacorpus utilizes aforementioned technique toconstruct taxonomy.the next three table namely taxi junlp usaar perform2205science science science environment combined eurovoc wordnet eurovoc model fegraph2taxo noconstraint without feas addembeddings ablation test report precision recall f-score across science environment domains.the first block value report result ablate layer utilize within graph2taxo model thesecond block demonstrate addition constraint fact improve performance thirdblock illustrate importance feature vfeas improve performance final block usespretrained fasttext embeddings initialize graph2taxo model fine tune base train data.all result report round decimal places.ing system participate texeval-2task furthermore taxorla illustrate performance reinforcement learning system byunder partial induction full induction setting respectively since maoet show outperform othermethods gupta bansal compare result propose graph2taxo approach state-ofthe-art system taxorl.finally graph2taxo1 graph2taxo2 depictthe result propose algorithm bothaforementioned setting input resource taxorl first scenario resource taxi second scenario.in setting find overallprecision propose graph2taxo approachis good exist approach demonstrate strong ability graph2taxo tofind true relation meanwhile recall propose graph2taxo approach comparable thatof exist state-of-the-art approach combining precision recall metric observe thatgraph2taxo outperforms exist state-of-the-artapproaches f-score significant margin.for example science average domain graph2taxo2 improve taxorl f-score environment eurovoc domain ourmodel improve taxorl f-score thetexeval-2 task.besides propose model high scalability.for example method trainedfor large graph include million node kipf welling besides partcan replace improved method hamilton designedfor large-scale graphs.ablation tests table show result proposed graph2taxo second setting theablation experiment divide four block indicate contribution layer usedin graph2taxo model table experiment three time average valuesof three report furthermore infigure randomly choose science eurovoc domain report error-bars correspond standard-deviation value ourexperiments.0 constraintwithout feasresults science eurovoc domain error barsf1 score recal precisionfigure results science eurovoc domain theaverage precision recall f-score value theirstandard error value clear addition residual layer layer lower variance results.the first block value table illustrates result ablate layer within ourgraph2taxo model comparing first evident semantic cluster layer improve recall cost precision however improve overall f-score improve2206ment clearly science eurovoc domain wherein increase second block show additionof constraint improve performance represent graph2taxo setup without constraint adding constraint yield good fscore specifically observe major increase of+5 science eurovoc domain.in third block remove feature vfeasas mention section result row5 table show feature critical inimproving performance propose systemon science eurovoc environment eurovoc domains note feature denote asvfeas novelty propose method butrather exist state-of-the-artapproaches.finally study effect initialize ourmodel pre-trained embeddings rather thaninitializing random specifically initializethe input matrixh0 graph2taxo model withpre-trained fasttext5 embeddings model fasttext embeddings improve upon margin precision value environment eurovoc domain unfortunately nosignificant effect f-score hence havenot pre-trained embeddings report theresults table provide illustration output thegraph2taxo model figure environment domain.the generate taxonomy example contain multiple tree serve purpose generate taxonomical classification asfuture work plan figure different strategy connect subtrees large graph forbetter generation.figure simple example taxonomy generatedby graph2taxo environment domain.5https //fasttext.cc5 conclusionwe introduce gnn-based cross-domainknowledge transfer framework graph2taxo whichmakes cross-domain graph structure inconjunction acyclicity constraint-baseddag learning taxonomy construction furthermore propose model encode acyclicity soft constraint show overall modeloutperforms state art.in future would like figure different strategy merge individual gain obtainedby separate application constraint intoa setup take best precision andrecall improvement forth good performing system also plan look strategiesto improve recall construct taxonomy.acknowledgmentsthe author would like thank chen frommit-ibm watson prof. jinbo fromthe university connecticut in-depth discussion model construction
question answering increasingdemand amount information available online desire quick accessto content grow common approachto fine-tune pretrained language model task-specific label dataset.this paradigm however relies scarce andcostly obtain large-scale human-labeleddata propose unsupervised approachto training model generated pseudotraining data show generate question training apply simpletemplate related retrieved sentence ratherthan original context sentence improvesdownstream performance allow themodel learn complex context-questionrelationships training model thisdata give relative improvement previous unsupervised model score thesquad dataset whenthe answer named entity achieve stateof-the-art performance squad unsupervised qa.1 introductionquestion answering answer questionbased give knowledge source recent advance drive performance system near-human performance onqa datasets squad rajpurkar al.,2016 natural questions kwiatkowski al.,2019 thanks pretrained language model suchas bert devlin xlnet yang al.,2019 roberta fine-tuningthese language model however require largescale data fine-tuning creating dataset every domain extremely costly practicallyinfeasible ability apply model outof-domain data efficient manner thus very1equal contribution2work internship labsfigure question generation pipeline originalcontext sentence contain give answer query retrieve related sentence contain match entity input question-style converter create train data.desirable problem approach withdomain adaptation transfer learn technique chung well data augmentation yang dhingra wang al.,2018 alberti however expand upon recently introduce task unsupervised question answering lewis toexamine extent synthetic training dataalone train model.in particular focus machine readingcomprehension setting context agiven paragraph model accessthis paragraph answer question furthermore work extractive answer assume contiguous sub-string context.a training instance supervised reading comprehension consist three component question context answer give dataset domain collection document usually beeasily obtain provide context form ofparagraphs sentence answers begathered keywords phrase context focus mainly factoid questionconcerns concise fact particular emphasize question whose answer name entity majority type factoid question entities canbe extract text name entity recognition techniques training instance sanswer thus main challenge focus14509of paper create relevant question context answer pair unsupervised manner.recent work lewis styletransfer generate question context answer pair show little improvement apply much simple question generator whichdrops permutates mask word improveupon paper propose simple intuitive retrieval template-based question generationapproach illustrate figure idea toretrieve sentence corpus similar thecurrent context generate question basedon sentence create question forall context answer pair fine-tune pretrained bert model data evaluate onthe squad v1.1 dataset rajpurkar contribution follow introduce retrieval template-based framework whichachieves state-of-the-art result squad unsupervised model particularly answeris name entity perform ablation studiesto determine effect component templatequestion generation release synthetictraining data code.12 unsupervised approachwe focus create high-quality non-trivial question allow model learn extractthe proper answer context-question pair.sentence retrieval standard cloze questioncan obtain take original sentencein answer appear context andmasking answer chosen token however model train data learntext matching fill-in-the-blank withlittle generalizability reason choseto retrieval-based approach obtain sentence similar contain answer upon create give question ourexperiments focus answer arenamed entity prove usefulprior assumption downstream performance lewis confirm initial experiment first index sentencesfrom wikipedia dump elasticsearchsearch engine also extract name entity foreach sentence wikipedia corpus thesentences query assume access anamed-entity recognition system work1https //github.com/awslabs/unsupervised-qafigure example synthetically generate question generic cloze-style question well atemplate-based approach.make spacy2 pipeline fora give context-answer pair query index original context sentence query toreturn sentence contain answer come context alower score query sentenceto discard highly similar plagiarized sentences.besides ensure retrieved sentence andquery sentence share answer entity requirethat least additional matching entity appearsin query sentence entire context perform ablation study effect ofthis match retrieve sentence arethen feed question-generation module.template-based question generation weconsider several question style generic clozestyle question answer replacedby token mask templated question wh+b+a+ well variation theordering template show figure2 given retrieved sentence formof fragment answer fragmentb templated question wh+b+a+ replacesthe answer wh-component e.g. depend entity type theanswer place wh-component beginning question follow sentencefragment fragment choiceof wh-component sample bi-gram base onprior probability bi-gram associatedwith named-entity type answer priorprobability calculate base named-entityand question bi-gram starter squaddataset information make ofthe full context-question-answer viewed2https //spacy.io4510as prior information disturb integrityof unsupervised approach additionally thechoice component significantly affect result template-based approach wealso experiment clause-based template butdid find significant difference performance.3 experimentssettings downstream question answeringmodels fine-tune pretrained bert model transformers repository wolf report ablation study number baseuncased version bert consistent lewiset model train validatedon generate pair question answer alongwith context test squad development training differs ablationstudy describe validation dataset random template-basedgenerated data point consistent acrossall ablation study train model checkpointing model every stepsand choose checkpoint high f1score validation best model allablation study average train runswith different seed unless otherwise state experiment perform syntheticqa training example initial model performedbest amount make generatedtraining data public.3.1 model analysiseffect retrieved sentence test effectof retrieve original sentence input question generation generic cloze questions.as show table retrieve sentencesimproves original sentence reinforce motivation retrieved sentence match trivially current context force model learn complex relationship simple entity matching theretrieval process return sentence notmatch original context random sample,15/18 retrieve sentence judge entirelyrelevant original sentence retrievalis already quite good high qualityelasticsearch retrieval original contextsentence query answer word.while explicitly ensure retrievedsentence meaning find thesearch result entity match give largelytraining procedure f1cloze-style original retrieve effect original retrieve sentence forgeneric cloze-style question generation.semantically matching sentence additionally webelieve sentence loosely relatedmeaning regularization factor whichprevent downstream model learningonly string match pattern along line lewis find simple noise function drop mask permute wordswas strong question generation baseline believe loosely related context sentence actas intuitive noise function investigate role semantic match retrievedsentences important direction future work.for section follow show resultsof retrieved sentence trend improved performance hold across experiments.effect template component evaluatethe effect individual template component ondownstream performance results shownin table template method improve largelyover simple cloze template performs best among template-based method word begin resembles target squad domain switchingthe order fragment fragment forcethe model learn complex relationship fromthe question additionally test effect thewh-component question mark theend sentence using data remove wh-component resultsin large decrease performance believe thatthis wh-component signal typeof possible answer entity help narrowdown space possible answer removing thequestion mark template also resultsin decreased performance large remove wh-component result ofbert pretraining expect certain punctuation base sentence structure note thesequestions grammatical havean impact performance improving question quality make difference performance asseen jump cloze-style question totemplate question ablation study suggestthat combination question relevance though4511template data f1cloze simple effect order template word question mark downstream performance.matching entity question formulation describe determine downstream performance.balancing component interestingproblem leave improve grammaticalityand fluency mean language modelgeneration future experiments.in last table show effect bi-gram prior downstreamqa training using most-common wordby grouping name entity category accord lewis perform closeto best-performing n-gram prior method single wh-word result asignificant decrease performance resultssuggest information name entity typesignaled wh-word provide importantinformation model informationbeyond wh-simple improve result significantly.effect filtering entity matching besidesensuring retrieved sentence query sentence share answer entity require atleast additional matching entity appear inboth query sentence entire context resultsare show table auxillary match leadsto improvement match usingtemplate-based data best result match query context matching mayfilter sentence whose topic fromthe original context leave investigation effect retrieved sentence relevance tofuture work.effect synthetic training dataset size notably lewis make approximately million synthetic data point order totrain model however able traina model good performance much fewerexamples show large subset unnecessary release synthetic training datamatching procedure f1no match match match context match effect query context matching retrieve input question generation module downstream performance.figure comparison effect size synthetic data downstream performance.as well figure show performance fromtraining random subset differ size andtesting squad development data sample random question context thedata lewis even little as10k datapoints train synthetically generate template-based data auxiliary matchingoutperforms result ablation study lewis using data templatebased data consistently outperform lewiset training either dataset show similar trend performance decrease increasingthe number synthetic example past likely distributional mismatch thesquad data choose example forour final experiment ablation studiesas number give good performance initialexperiments.3.2 comparison best-performing models compare train best template-baseddata state-of-the-art table squad f1results reflect result hidden squad testset report single-model number lewis report ensemble method achieve best single model achieve wemake whole-word-masking version of4512model choice squad test squad f1bert-large lewis comparison result bertlarge model.bert-large although original bertlarge give similar performance thesquad report number sample squad question name entity refer squad-ner subset corresponding squad development dataset has4,338 sample differ slightly lewiset difference preprocessing also train fully-supervised model onthe squad train dataset vary amountsof data find unsupervised performanceequals supervised performance train about3,000 label examples.4 conclusionin paper introduce retrieval-based approach unsupervised extractive question answering simple template-based approach achievesstate-of-the-art result unsupervised methodson squad dataset f1when answer named entity analyze theeffect several component template-basedapproaches ablation study toexperiment datasets domain incorporate synthetic data semi-supervisedsetting test feasibility framework multi-lingual setting.5 acknowledgementswe thank xiaofei fruitful discussion theproject
large transformer-based language modelshave show effective manyclassification task however computational complexity prevents application require classification largeset candidate previous work haveinvestigated approach reduce model size relatively little attention technique improve batch throughput inference paper introduce cascade transformer simple effective technique adapt transformer-based model intoa cascade ranker ranker usedto prune subset candidate batch thus dramatically increase throughput inference time partial encoding transformer model share among rerankers provide speed-up compare state-of-the-art transformer model approach reduces computation almost impact accuracy measure ontwo english question answering datasets.1 introductionrecent research show transformer-basedneural network greatly advance state ofthe many natural language processingtasks efforts bert devlin roberta xlnet al.,2019 others lead major advancementsin several subfields model ableto approximate syntactic semantic relation word compound pre-trainingon copious amount unlabeled data clark al.,2019 jawahar easily apply different task fine-tuningthem train data target domain/task peters impressive effectiveness transformer-based neuralnetworks partially attribute largenumber parameter range millionfor base model billion shoeybi al.,2019 however also make rather expensive term computation time resources.being aware problem research community develop technique pruneunnecessary network parameter sanh optimize transformer architecture zhang xiao paper propose completely different approach increase efficiency transformer model orthogonal previouswork thus apply addition anyof method describe main idea isthat large class problem require choose correct candidate among many someapplications often entail modelover hundred thousand instance however well-known many case candidate easily exclude optimalsolution land doig i.e. require less computation case hierarchicaltransformer model property exploitedby subset model layer score significant portion candidate i.e. canbe easily exclude search additionally hierarchical structure transformer model intuitively enable re-use computation oflower block upper blocks.following intuition work atstudying transformer model cascadedto efficiently find score element amonga large candidate specifically thecontributions paper first build sequence rerankers different complexity whichprocess candidate pipeline rerankerat position take candidate select reranker provide candidatesto reranker position requiringthat ki−1 approach5698allows save computation time moreexpensive rerankers progressively reduce thenumber candidate step build riusing transformer network roberta pre-trained models.second introduce optimization onsrn increase efficiency base observation model process inputindependently contrast propose cascadetransformer sequence rerankers builton single transformer model rerankersr1 obtain small feedforward classification network different transformer block position therefore partial encoding transformer block bothinput reranker well subsequent transformer encode block allow efficiently re-use partial result consume forrankers ri+1 enable approach parameter allrerankers must compatible thus trainedct multi-task learning fashion alternate theoptimization different i.e. layer riare affect back-propagation loss aswell loss i.finally test case target answersentence selection well-known task inthe domain question answering givena question sentence candidate e.g. retrieve search engine task consistsin select sentence correctly answer thequestion test approach differentdatasets asnq recently make available bygarg benchmark datasetbuilt anonymized question toamazon alexa code asnq split modelstrained asnq publicly available.1our experimental result show selection different determine differenttrade-off point efficiency accuracy.for example possible reduce overallcomputation decrease inaccuracy importantly approachlargely improve reduce cost by37 almost loss accuracy thererankers train cascade approachachieve equivalent good performance transformer model train independently finally result suggest other1https //github.com/alexa/wqa-cascade-transformersnlp task require candidate ranking e.g. parsing summarization many structured prediction tasks.2 related workin section first summarize related work forsequential reranking passage document focus late method finally discuss late technique reducingtransformer complexity.reranking approach introduce paper inspire previous work matsubara fast as2neural model select subset instance beinput transformer model reduce thecomputation time latter four time preserve accuracy.before paper main work sequential ranker originate document retrieval research example wang formulate develop cascade rank model thatimproved top-k ranked effectiveness retrieval efficiency dang propose twostage approach limited textual feature final model train large setof query- document-dependent feature wanget focus quickly identify ofgood candidate document passedto second cascade gallagheret present general frameworkfor learn end-to-end cascade ranker usingback-propagation asadi studiedeffectiveness/efficiency trade-off three candidate generation approach methodsare align approach target document retrieval different setting.further linear model simpleneural model agarwal focus onas2 apply linear models.answer sentence selection lastfew year several approach proposedfor example severyn moschitti apply create question answer representation others propose interweighted alignment network shen tran ofcompare aggregate architecture also beenextensively evaluate wang jiang bianet yoon family ofapproaches shallow attention mechanism5699over question answer sentence embeddings.finally tayyar madabushi exploitedfine-grained question classification improve answer selection.transformer model fine-tune several task closely relate example machine reading devlinet yang wang ad-hoc document retrieval yang macavaney semantic understanding task obtain significantimprovement previous neural method recently garg apply transformer model obtain impressive boost state ofthe tasks.reducing transformer complexity highcomputational cost transformer model preventstheir many real-word application someproposed solution rely leverage knowledgedistillation pre-training step e.g. sanh al.,2019 parameter reduction technique lanet reduce inference cost however theeffectiveness approach varies depend target task apply to.others investigate method reduce inference latency modify self-attention operate either encode child decode xiao zhang overall solution aremostly orthogonal approach changethe architecture transformer cell rather efficiently re-using intermediate results.with respect model architecture approach similar probe models2 al.,2017 hupkes belinkov train classification layersbased partial encode input sequence.however intermediate classifier integral part model rather train onfrozen partial encoding classifier inspect model property rather toimprove inference throughput.our apporach also share similarity withstudent-teacher approach self-training yarowsky mcclosky underthis setting model teacher make prediction unlabeled data obtain automatic label student whichlearns gold standard automatic label recent year many variant have2also know auxiliary diagnostic classifiers.been propose include treat teacher prediction soft label hinton maskingpart label clark multiplemodules teacher zhou ruderand plank unlike classic approach improve teacher model orcreating efficient student instead train model sequential rank components.this generalization approach student need learn simplertask teacher however approach significantly different traditional setting preliminary investigation show benot effective.3 preliminaries task definitionwe first formalize problem select mostlikely element reranking problem define sequential reranking finally wecontextualize task framework.3.1 element selectionin general large class problem formulate element selection task give query candidatesa select optimal element model task selector function define powerset argmaxi probability required element canbe estimate neural network model thecase transformer model optimizedusing point-wise loss i.e. targetcandidate generate selection probability pairwise list-wise approach still bianet would change finding study point-wise method havebeen show achieve competitive performance inthe case transformer models.3.2 search sequential reranking assuming heuristic available preselect subset most-likely candidate element selection require evaluate sampleusing relevance estimator instead single estimator often efficient sequenceof rerankers progressively reduce number ofcandidates.we define reranker function take subset ⊆5700a return element size highestprobability relevant query σ.given sequence rerankers sort termsof computational efficiency weassume ranking accuracy e.g. termsof increase reverse order ofthe efficiency i.e. i.then define sequential reranker ordern composition rerankers rn−1 also bethe element selector associatedwith different i.e. number ofelements reranker return depending thevalues model different trade-offsbetween accuracy efficiency obtained.33.3 definitionthe definition directly follow definition element selection section wherethe query natural language question theelements answer sentence candidate retrievedwith approach e.g. search engine.4 transformersin section explain exploit hierarchical architecture traditional transformermodel build model first briefly recap traditional transformer model referto monolithic sequenceclassification derive sequential rerankers pre-trained transformer model section introduce cascadetransformer model model efficiently partial encoding input build aset sequential rerankersri section finally explain model train forinference section respectively.4.1 monolithic transformer modelswe first briefly describe transformer model sequence classification call monolithic input sample computationflows first last layers.let standardstacked transformer model vaswani embed layer the3the design end-to-end algorithm learn optimalparameter give target trade-off leave future work. 1  droppingbatchelement +1    ℎ3   ℎ2   input batch toct model contains ℎ10ℎ20 3  2  1  31 21 11 30 20 10ℎ1   ℎ2  visual representation cascade transformer model propose paper components yellow represent layer traditional transformer model element purple unique toct input output model show blue.in example drop rate cause sample x3to remove partial classifier layers4 generate contextualized representation input sequence typicallyreferred depth encoder i.e. thenumber layer typical value range from12 although recent work experiment layer shoeybi pre-trained large amount unlabeledtext mask devlin al.,2019c autoregressive yang radford language model objective.pre-trained language model fine-tunedfor target task additional layer anddata e.g. fully connect layer typicallystacked obtain sentence classifier formally give sequence input symbols5 encode =4that entire transformer block constitute layersfor multi-head attention normalization forward processing positional embeddings.5for rank task sequence input symbol typically concatenation query candidate in5701 first obtain recursivelyapplying input hi−1 first symbol inputsequence6 feed sequence dense feedforward layer obtain final output score i.e. fine-tuned together withthe entire model task-specific dataset ofquestion candidate answer pair case transformer-based sequential reranker modelsmonolithic transformer easily modifiedor combine build sequence rerankers asdescribed seciton case adaptan exist monolithic obtain sequence rerankersri eachri consist encoders fromt layer follow classificationlayer i.e. asequence input symbol rerankers thesequence design predict weindicate rerankers srnare train independently target data.in experiment obtain best performance following formula determine architecture eachreranker word assemble sequentialreranker five rerankers build withtransformer model layer respectively choice fact thatour experimental result seem indicate thatthe information layer structuredenough achieve satisfactory classificationperformance task observation inline recent work effectiveness ofpartial encoders semantic task similar peters cascade transformer modelsduring inference monolithic transformer modelsevaluate sequence entire computation graph obtain classification score model distinguish specialtoken model alsouse second embed layer represent sequenceeach symbol come from.6before process transformer model sequence typically prefix start symbol allow transformer model accumulate knowledge entire sequence positionwithout compromise token-specific representation devlinet mean example areprocessed multiple time similar layer different e.g. compute thesame operation first transformer layer rerankers compute thesame layer morecomputationally-efficient approach share allthe common transformer block different rerankers speed computation transformer encoder implement required thiscan easily obtain classificationlayer layer figure give sample classifierscρ produce score partial encoding build model build rerankers select candidate score subsequent rerankers ri+1.we setting choice describe section observe best performance whenall encoding input partial classifier rather partial encoding classification token therefore average obtain score line kovalevaet hypothesize encode layer long dependency might properlyaccounted however experiment find benefit parametrizing thisoperation e.g. either complex network weight average operation.4.3.1 training ctthe training propose model conduct multi-task fashion every mini-batch randomly sample ranker include thefinal output ranker calculate loss target label back-propagate loss throughoutthe entire model embed layer weexperimented several complex samplingstrategies include round-robin selection process parametrized bias towards early rankersfor first epoch ultimately foundthat uniform sampling work best also empirically determine classifier backpropagating loss input embeddings oppose stop layer crucial ensure convergence possible explanationcould enable classifier influence theinput representation backpropagation ensures late rerankers robust against5702variance partial encoding induce early classifier experimentally find gradientdoes flow throughout different block thedevelopment performance later classifiersdrops early classifier start converging.4.3.2 inferencerecall interested speed inference classification task answer selection hundred candidate associate question therefore assume without loss generality batchof sample contain candidate answer question ourpartial classifier throw away fraction ofcandidates increase throughput discard ki−1c candidate roundsα· ki−1 close integer.for instance batch size recall experiment consist cascade rerankers layer thesize batch reduce b0.3· candidate discard first classifier second classifier layer b0.3· remove effectivebatch size layer samplesare leave i.e. instance number score thefinal classifier reduce times.our approach effect improve thethroughput transformer model reduce theaverage batch size inference throughput neural model maximumnumber example process parallel i.e. size batch number usually ceiled amount memory availableto model e.g. monolithic model constant batch size inference however batch size cascade model varies process batch cansize network respect average batchsize thus increase number sample weinitially batch example suppose hardware requirement dictate maximum batch size monolithic model.as average batch size cascading modelis =80.2 process batch instanceswithout violate memory constrains increasingthroughput remark fixed crucial obtain performance gain describe wewere employ score-based thresholding apasnq trecqa wikiqatrain questions cand corr cand corr questions cand corr datasets statistic asnq moresentence candidate trecqa wikiqa.proach discard candidate scorebelow give threshold could determinethe size batch throughout cascade thusmaking impossible efficiently scale system hand note nothing ourimplementations prevents potentially correct candidate drop however show section opportune choiceof threshold good accuracy early classifier ensure high probability least onepositive example candidate lastclassifier cascade.5 experimentswe present three experiment design toevaluate first section showthat propose approach without selectionproduces comparable superior result respect state thanks itsstability property second section wecompare cascade transformer vanillatransformer well sequence transformermodels train independently finally third section explore tuning dropratio α.5.1 datasetstrecqa wikiqa traditional benchmarksused trecqa wang wikiqa yang typically containa limited number candidate question.therefore useful compare accuracy system state theydo enable test large scale passage reranking i.e. inference hundred thousand answercandidates therefore evaluate approach datasets asnq publicly available dataset still leverage trecqa wikiqa show our5703cascade system comparable performance tostate-of-the-art transformer model filtering applied.asnq answer sentence natural questionsdataset garg large collection question-answer pair twoorders magnitude large public as2datasets obtain extract sentencecandidates google natural question benchmark kwiatkowski samples innq consists tuples 〈question answerlong answershort label〉 answerlong contain multiple sentence answershort fragment sentence label binary value indicate whether answerlong correct positive sample obtain extract sentencesfrom answerlong contain answershort allother sentence label negative original release ansq7 contain train development split split bothhave test sets.gpd general purpose dataset part ourefforts study large scale evaluateperformance system build search engine retrieve candidatedocuments give question weextracted candidate sentence document rank vanilla transformermodel described finally ranked sentence manuallyannotated correct incorrect answers.we measure accuracy approach onasnq four metric mean average precision mean reciprocal rank precision rank candidate normalized discounted cumulative gain at10 retrieve candidate ndcg thefirst metric capture overall system performance latter well suit evaluatesystems many candidate focus moreon precision wikiqa trecqa usemap mrr.5.2 models trainingour model fine-tuned start pretrained roberta encoder wechose transformer model others toits strong performance answer selection task garg specifically base7https //github.com/alexa/wqa_tandamodel wikiqa trecqamap mrrca1 wang jiang –ca2 yoon garg layer tanda layer tanda layer tanda layer tanda tandabase layer layer layer layer layer comparison academic datasets.with exception transformer thepartial final classifier achieve comparable good performance state models.variant embeddings layers,12 head hidden unit moreappropriate efficient classification applicable8 fine-tune model two-step transfer adapt tanda technique introducedby garg mention section optimize ourmodel multi-task setting minibatch randomly sample output layersof classifier backpropagate loss alllayers below.while evaluate different sample technique find simple uniform distribution sufficient allow model convergequickly.our model optimize adam kingmaand triangular learn rate smith,2017 update ramp-up9 peaklearning rate batch size upto token mini-batch model forthe partial final classifier feedforward module hidden unit andtanh activation function like original bertimplementation dropout value alldense attention layer implement oursystem mxnet chen andgluonnlp machinewith nvidia tesla v100 gpus memory.8when fine-tune trecqa wikiqa weperform transfer step asnq adapt ourtarget dataset asnq directly fine-tune it.9on asnq roughly equivalent ˜950k sample orabout training set.5704method model asnq cost reductionper batchmap ndcg ndcg mrrmonolithictransformer layer tanda layer tanda layer tanda layer tanda tandabase baselinesequentialranker models,4 layer sequence0.3 cascadetransformer layer layer layer layer full layer table comparison cascade transformers model asnq datasets monolithictransformer refers single transformer model train independently sequential ranker sequence ofmonolithic transformer model size train independently cascade transformer theapproach propose train model equal outperform state drop apply i.e. drop obtain performance operations.5.3 stability results cascade trainingin oder good assess training strategy forct model compare monolithic transformer evaluate performance system twowell know datasets wikiqa trecqa.the result experiment present intable note case applyingany drop cascade classifier necessary dataset sentence comfortablyin mini batch dataset statistic table would observe advantage pruningcandidates instead focus evaluate howour training strategy affect performance partialand final classifier model.our experiment show classifier ctmodel achieve competitive performance respect state transformer model train cascade outperformstandabase absolute point layer modelsare equally comparable differ point wikiqa outscoringtanda absolute point ontrecqa however observe meaningful difference performance model monolithic counterpart wehypothesize fact lowerlayers typically well suit classificationwhen part large model peters al.,2019 observation reinforce factthat layer tanda model show table four time number iteration anyother model converge local optimum.overall experiment show trainingstrategy effective model butcan also produce small transformer model withgood accuracy without separate fine-tuning.5.4 results effectiveness cascadingthe main result approach presentedin table compare state-of-the-artmonolithic transformer tandabase small monolithic transformer model layer sequential ranker consist transformer model and12 layer train independently reportperformance classifier individually layers4 equivalent full transformermodel test drop ratio finally model report therelative cost batch compare base transformer model layers.overall observe cascade model competitive monolithic transformerson asnq datasets particular selection apply cascade model performs equal good totandabase asnq improve achieve aslightly indicatesthat despite multitasking setup method iscompetitive state art.5705a drop rate produce small degradation accuracy significantly reduce number operation batch particular achieve lessthan drop compare totandabase ansq slightly improve overit observe pronounceddrop performance expect intermediate classification layer designedto drop significant number candidates.for large value note thatwe achieve significantly good performance thanmonolithic transformer similar computationalcost example achieve improvement tanda model similar improvement obtain ansq +11.0 model also competitive respect sequential transformer equivalentdrop rate timesmore efficient model madeof independent tanda model re-use encoding generate small model does.5.5 results tuning drop ratio αfinally examine different value dropratio affect performance model inparticular perform exhaustive grid-searchon model train dataset fordrop ratio value performance report infigure respect relative computationalcost batch configuration comparedwith tandabase model.overall find model robustwith respect choice observemoderate degradation high drop ratio value e.g. varies asexpected performance increase model withhigher computational cost batch although theytaper model relative cost hand grid search result notseem suggest effective strategy pick optimal value experiment choose value droprates future would like learn suchvalues train cascade model itself.6 conclusions future workthis work introduces variant traditionaltransformer model design improve inference646668707274map84868890mrr788082848688ndcg relative cost7880828486precision grid search plot validation set.each point correspond configuration drop ratio valueson x-axis represent relative computational costper batch configuration compare tandabase.the three report table correspond compared traditional monolithicstacked transformer model approach leveragesclassifiers place different encode stage toprune candidate batch improve modelthroughput experiment show modelnot achieve comparable performance traditional transformer model reduce computational cost batch also thatour train strategy stable jointly producessmaller transformer model suitable forclassification high throughput latency goal must meet future work plan toexplore technique automatically learn toplace intermediate classifier drop ratioto
open-domain dialogue generation gainedincreasing attention natural language processing evaluation require holisticmeans human rating deem goldstandard human evaluation inefficientand costly automated substitute highlydesirable paper propose holistic evaluation metric capture different aspect open-domain dialogue metricsconsist gpt-2 base context coherencebetween sentence dialogue gpt-2based fluency phrasing n-gram base diversity response augmented query textual-entailment-inference base logicalself-consistency empirical validity ourmetrics demonstrate strong correlationswith human judgment open source thecode relevant material introductionlearning communicate capacity intelligent agent research enable machineto meaningful natural conversation withhumans play fundamental role developingartificial general intelligence inthe formulation turing test turing recently open-domain non-task-oriented dialoguesystems attract surge research interest bessho sordoni shanget vinyals serban al.,2016 ghazvininejad model open-domain dialogue generation efficient manner pose significantchallenge develop dialogue system theprevalent method open-domain dialogue evaluation human-based rating give rubric.∗equal contributions.§wenjuan corresponding author wenjuan hancontributed work shanghaitech university.1https //github.com/alexzhou907/dialogue_evaluation.context conversationspeaker want tonight speaker movie model responsespeaker something active.reference responsespeaker yeah film turing look great context conversationspeaker downtown area speaker are.speaker much model responsespeaker idea ofthe area.table response dialogue system wolfet daily dialogue dataset first generate response appear reasonable within opendomain dialogue bleu score semanticsimilarity model response reference response second generated response conflictswith prior utterance italic text highlight thelogical contradiction.when various variation model ofhyper-parameters need labor-intensivehuman evaluation deem impracticable thiskey drawback hinder research progress andrender human evaluation approach scalable.previous automatic evaluation metric generallyfocus quality dialogue generation context coherence fluency word-overlap metric papineni banerjee lavie ad-hoc classifier ghazarian design measuringthe quality open-domain dialogue relationbetween utterance critical shownin first example table compared withthe previous approach language model train enormous amount text naturally capture coherence among word andutterances hand good evaluationmetric measure quality gen3620eration also diversity generation whichis especially important open-ended task like dialogue story generation hashimoto n-gram base metric utilized tomeasure diversity serban al.,2017 however metric might improper fordiversity evaluation since generate utterancesgiven various query provide benchmarkare generally diverse experiment observe constantly high diversity term humanratings n-gram base entropy evaluate generated response directly additionto three aforementioned metric logical selfconsistency also aspect dialogue model zhang dialogue example withlogical contradiction display second example table welleck measuredlogical self-consistency transfer sentence rule-based triple category relation category help human annotator weare nevertheless unaware reliable automaticmeasure logical consistency open-domain dialogue.in work propose holistic metric thatevaluate distinctive aspect generate dialogues.specifically consider context coherence adialogue meaningfulness response withinthe context prior query language fluencyof generate response quality phrasingrelative human native speaker responsediversity generated sentence variety meaning word choice response logical self-consistency logical consistency utterance dialogue agent bothcontext coherence response fluency qualitymetrics naturally capture metric basedon strong language model like gpt-2 radfordet therefore propose recruit andfine-tune gpt-2 basis quality metrics.with regard response diversity logical selfconsistency propose measure underaugmented utterance controlled paraphrasing.we leverage effective approach generateaugmented utterance word substitution textgenerator k-best decoder moreover weutilize n-gram base entropy capture responsediversity entailment base approach capturelogical self-consistency experiment showthat propose metric strongly correlate human judgment moreover augmented datasetsallow accurate straightforward human annotation significantly improve agreement human evaluation release thecode relevant material open-source contribution pave towards research.2 prior artheuristic-based metric show alignwell human judgment widely apply invarious language generation task machinetranslation bleu papineni computesn-gram precision whereas meteor banerjeeand lavie take account precisionand recall summarization rouge also consider precision recall calculate f-measure n-gram base metric arewell-suited generation task moresource-determined conditional entropy suchas translation image captioning summarization dialogue study adopt metricsto evaluate quality generate conversation response ritter sordoniet nevertheless suitablefor open-ended generation high conditional entropy task like dialogue generation diverserange generation acceptable conditional query indeed conduct extensive empirical study metric e.g. bleu meteor rouge test effectivenesson evaluate dialogue generation find limitedrelation automatic metric human judgments.the word-overlap metric e.g. bleu fail tocapture semantic similarity model andreference response following work leveragethe distribute representation learn neural network model capture semantic similarity amongcontext model response reference response.lowe collect dataset human scoresand train hierarchical recurrent neural network predict human-like score input response give context result automaticmetric medium level correlation human judgment obtaining metric howeverrequires large dataset human-annotated score thus render approach flexible extensible propose referenced metricand unreferenced metric blended evaluation routine ruber open-domain dialogue system thisblended metric combination metric areferenced metric measure similarity betweenmodel-generated reference response the3621basis word-embeddings unreferenced metric capture relevance query andresponse obtain train neural network classifier determine whether response isappropriate positive example reference negative example referenceresponses randomly choose dataset henceavoiding need human-annotated data aftertraining softmax score utilize measurewhether generated response coherent thequery attempting improve ruber ghazarianet explore contextualized embeddings bert bert-based unreferencedmetric improves word-embedding-basedruber unreferenced metric interestingly theyshow combine metric reduce correlation human judgment unreferencedmetric alone although finding counterintuitive consistent characteristicsof open-domain dialogue range diverseresponses reasonable give query hence response acceptable human annotator evenif align well reference either interms word-overlap semantic embedding.context coherence component dialogue response coherence query asexplored ghazvininejadet prior work measure coherencebased softmax score trained binaryclassifier explore alternative approachbased language modeling bengio language model naturally capture coherence response query without resortingto ad-hoc classifier.language fluency besides coherence goodresponse fluent fluency often measure language model holtzman define response fluencyscore negative perplexity generated responses.response diversity addition quality metric response diversity also critical especiallyfor high conditional entropy task like dialogue orstory generation hashimoto somen-gram base metric utilize measurediversity serban compute unigram entropy across generate utterance measure diversity metric mightbe improper diversity since generated utterance give various query generally diverse.in experiment observe constantly high diversity term human rating n-gram basedentropy another perspective entropy compute across generated response essentiallymeasuring marginal entropy response actual interest conditional entropy response conditional queries.logical self-consistency similar diversityevaluation current benchmark suitablefor evaluate logical self-consistency current dataset well-formed make system togenerate simple nonredundant response butunfortunately still exist logical contradictionsas show table natural language inference task williams tocheck whether sentence entail contradict previous sentence highly related tologic evaluation open-domain dialogues.3 metrics3.1 context coherencelanguage model predict next tokengiven previous token naturally capture coherence sentence particularly dialogue query response case gpt-2 radford large-scale pre-trainedlanguage model base transformer architecture vaswani train vastamount diverse data demonstrate impressive text generation capability order bettercapture dependence query andresponses gpt-2 fine-tuned nextsentence prediction task dialogue dataset ofinterest.suppose query contain token response tokens denote fine-tuned gpt-2 context coherence define loglikelihood response conditional thequery normalize length responselength craw =1trlogp =1trtr∑tlogp rt|r note craw negative number andunbounded single value hardto explain absolutely interpretedrelative value also unboundednessrenders prone extreme value hence normalized score utilized instead since score3622distribution vary function dataset thelower bound define percentile denotedas c5th instead arbitrary value thenormalized score −max c5th craw c5thc5th range response fluencyto capture fluency response also adoptthe pretrained language model gpt-2 particular response fluency score fraw isdefined fraw =1trtr∑tlogp rt|r similar context coherence normalized version fraw employed.3.3 response diversityprior work serban measure diversity compute n-gram entropy across generated response essentially reflect marginal entropy response diversity response conditionalon query e.g. conditional entropy however interest dialogue model theother hand measure diversity base response randomly sample model conditional single query response quality isgenerally caccia currentwork instead propose measure response diversity utilize augment datasets controlledparaphrasing allow measure diversity among top-ranked response conditional onparaphrased query hence avoid tradeoff dependency diversity quality.in word give query slightly tiltthe correspond element query-responsejoint space along query dimension achieve byparaphrasing-augmentation measure theentropy high-quality response neighbourhood target query.while augment query measure theconditional entropy response need control diversity augmented query suchthat augmented stay vicinity thetargeted query hence goal controlled augmentation minimize diversity meaningand word avoid feed dialogue modelidentical input achieve augmentationapproaches consider wordnet miller,1998 substitution conditional textgenerator substitution word-level manipulation method replace word withsynonyms define wordnet different conditional text generator augment query multi-turn dialogue requiresa generator produce augments condition onthe context define prior utterancehistory select query instance suppose ut−1 denote utterance history utindicates query augment topk beam modelconditional utterance history produced.given target query augmentedqueries controlled paraphrasing corresponding response generate model undertest calculate n-gram entropy forsamples logical self-consistencylogical self-consistency measure generated response logically contradictory agentuttered multi-turn history basic idea toapply pretrained multi-genre natural languageinference mnli williams model tolabel relation response utterance history agent logically consistent specifically train ternary classifierthat take utterance input predict therelation either contradiction entailment neutral mnli dataset average thecontradiction class probability current utterance prior utterance agentas contradiction score order match thehuman rating minus contradictionscore final score logical self-consistencyevaluation.moreover measure logical self-consistencyunder augment datasets controlled paraphrasing introduce section main idea generate augmentedmulti-turn utterance history likely induce dialogue system produce contradictoryresponses assume likely forthe agent produce self-contradictory responseswhen respond similar query wsand paraphrase query calcu3623late contradiction score current utteranceand prior utterance agent.4 experiments4.1 datasetto facilitate comparison prior work ghazarian dailydialog dataset al.,2017 adopt empirical analysis ourproposed metric dataset contain multi-turn dialogue dataset dialogue split traintest-validation partitions.4.2 response generationa sequence-to-sequence seq2seq model attention bahdanau train withthe train validation partition generate dialogue response implementation opennmt klein train model theseq2seq consist lstm hidden unit encoder decoder themodel train learn rate obtain response wide spectrum quality diversity sample data top-ksampling language model fine-tuningthe base gpt-2 model layer usedto compute metric gpt-2 model wasfine-tuned training validation data infine-tuning query response concatenate together single sentence intogpt-2 perplexity fine-tuned languagemodel test dataset controlled query generationwordnet substitution conditional text generator augment diversity-controlledqueries stanford part-of-speech tagger toutanova manning wordnetby miller utilize wordnet substitution achieve first stanford postagger token query four augment input generate substitute verb nouns adjectives adverb abovewith synonym wordnet conditional textgenerator train opennmt transformer2we also experiment medium gpt-2 find result generally same.and large model gpt-2 might posecomputational difficulty researcher thus werenot considered.context conversationspeaker course two-week paid vacation ayear five-day workweek.speaker margin card could take amargin card travel company soon possible.human score score score case study coherence metric thehuman evaluation agree generated response isnot coherent give query ruber indicate reply coherent.on training validation split query augmentation apply test datasetto augment query top-k beam forresponse diversity five variant obtain theoriginal query four paraphrased logical self-consistency variant obtain theoriginal query paraphrase.4.5 metric evaluationto validity propose metric weutilize amazon turk collect high quality humanratings subject metric select sample present human andeach datapoint best metric.on context coherence response fluency select datapoints diverse range ofgeneration quality query-responsepairs context coherence response fluency forresponse diversity select datapoints total response group allof condition control inputsgenerated give context.for logical self-consistency datapoints select independent response diversity afteramazon turk result collect computethe pearson spearman correlation ourautomatic metric human rating assess thevalidity metric normalize humanrating score range results5.1 context coherencetable demonstrate pearson spearmancorrelations propose context coherence metric human judgment also resultswere compare previous best-performing au3624 gpt-2 fine-tune gpt-2 fine-tunefigure correlation context coherence metric human rating without fine-tuning ofgpt-2 note random jitter sample fromn human rating visualize scatter plotsshowed paper overlap points.pearson spearmanruber+bert fine-tune fine-tune mean correlation ruber+bert context coherence metric human rating without fine-tuning gpt-2 metric ruber bert embeddings ghazvininejad clearly ourlanguage model base coherence metric showshigher correlation human judgment theclassifier-based metric ruber.in addition compare propose metricwith similar metric base gpt-2 languagemodel without fine-tuning target dataset.the fine-tuned version improve result indicate fine-tuning dialogue datasetenables language model good capture thedependency query reply interestingly even metric base languagemodel without fine-tuning correlate humanratings strong ruber.we also examine inter-rater reliability iscomputed hold rating rater time calculate correlation averageof rater judgment finally averagingover take maximum held-out correlation score inter-rater reliability result alsosupport strong performance proposedcontext coherence metric correlation automatic metric human evaluationwas close inter-rater correlations.in addition figure detail effect finepearson spearmangpt-2 fine-tune fine-tune mean correlation response fluency metricf human rating without fine-tuning ofgpt-2 pairwise mean correlation humanratings.tuning gpt-2 help improve consistency human rating automatic metric.table display case study coherencemetric human evaluation agree thegenerated response coherent givenquery ruber indicate reply coherent might ruber simply compare embeddings query responseand business travel relate word query suchas vacation workweek reply astravel company make ruber judge aresimilar.5.2 response fluencyour finding show propose fluency metricf highly correlate human judgments.table summarize relation propose fluency metric human rating term ofpearson spearman correlation importanceof fine-tuning gpt-2 outline section isevident observe increase pearson correlation enhancement from0.32 spearman correlation addition figure detail effect fine-tuning notably correction outlier occurs.3625 gpt-2 fine-tune gpt-2 fine-tunefigure correlation response fluency metric human rating gpt-2 without finetuning.1-gram entropy entropy entropypearson spearman pearson spearman pearson spearmanbaseline dataset dataset dataset comparison response diversity metric baseline dataset paraphrasing-augmenteddatasets datasets spearman pearson correlations.inter-rater pearson inter-rater spearman human variancemean mean maxbaseline dataset dataset dataset comparison response diversity baseline dataset paraphrasing-augmenteddatasets datasets inter-rater spearman pearson correlations.5.3 response diversitytable show evaluation propose diversity metric basis augment datasetswith also include baselinedataset consist response randomlychosen query test data unigram bigram trigram entropy utilize calculateresponses diversity compare human rating pearson spearman correlation isclear automatic evaluation controlledparaphrasing datasets consistently achieve highercorrelation compare baselinedataset figure display correlation normalize human rating correspond n-gramentropy base augmented dataset entropyvalues base datasets demonstratestronger relation human rating compare tothose base baseline dataset consistent withthe report correlations.table display inter-rater pearson spearman correlation variance human ratings.human rating base paraphrasing augment datasets show high inter-rater correlationsand variance indicate raters generallyagree poor baseline performance likely uncontrolled nature input sentence output evaluated modelsare generally diverse make difficult humansto judge diversity performance model.furthermore diversity metric correlation human rating close corresponding mean inter-rater correlation suggest thatthe diversity evaluation base paraphrasingaugmented data reveal diversity dialogue system consistent humans.5.4 logical self-consistencytable display correlation propose automatic rating human rating thethe paraphrasing augment data andctg baseline without augmentation theautomatic metric base augment data a3626 baseline entropy baseline entropy baseline entropy entropy entropy entropy entropy entropy entropyfigure correlation n-gram entropy human rating baseline dataset dataset ctgdataset.stronger relation base baseline inparticular metric base augmentationaligns human judgment closet.inter-rater pearson spearman correlationsare report table human rating theaugmented data consistent onthe baseline indicate necessity efficiencyof refine dataset instead originalone show case study table relation four metricsalthough four propose metric intuitivelyand theoretically important evaluate dialoguesystem entirely clear whether independent necessaryto measure empirically investigate association randomly choose test dataset construct theevaluation data four metric five humanevaluators rate four aspect dialogue.we examine pairwise correlation human rating four metric response fluencycorrelates context coherence =0.003 mainly fact inarticulate response often consider incoherent withthe context pair-wise correlation arenon-significant thus thefour metric relatively independent eachother critical take account ofthem obtain holistic evaluation dialoguemodel.3we observe obvious non-linear dependencyeither.3627context conversationspeaker leader follower speaker lead people rathercooperate everybody thejob work together.generated utterancespeaker follower leader model responsespeaker like keep personwho want follower.our score score case study logical self-consistency generated utterance generate blue italic wordshighlights logic contradiction automatic scoreis indicating logic contradiction detected.pearson spearmanbaseline dataset dataset dataset comparison logical self-consistency metric paraphrasing-augmented data andctg data baseline data without augmentationusing spearman pearson correlation humanratings.inter-rater pearson inter-rater spearmanmean mean maxbaseline comparison logical self-consistency metric paraphrasing-augmented data andctg data baseline data without augmentationusing inter-rater spearman pearson correlations.6 conclusionthis paper provide holistic automatic evaluation method open-domain dialogue model incontrast prior mean evaluation capture quality generation also thediversity logical consistency response werecruit gpt-2 strong language model evaluate context coherency response fluency.for response diversity logical self-consistency propose measure aspect underaugmented utterance controlled paraphrasing leverage effective approach generate augmented utterance word substitution andtext generator k-best decoder moreover weutilize n-gram base entropy capture responsediversity entailment base approach measure logical self-consistency propose metricsshow strong correlation human judgment itis hope propose holistic metric pavethe towards comparability open-domaindialogue models.acknowledgmentswenjuan yixian kewei support national natural science foundationof china
paper present tree-structured neuraltopic model topic distributionover tree infinite number branches.our model parameterizes unbounded ancestral fraternal topic distribution applyingdoubly-recurrent neural network thehelp autoencoding variational bayes ourmodel improves data scalability achievescompetitive performance induce latenttopics tree structure compare aprior tree-structured topic model blei al.,2010 work extend tree-structuredtopic model incorporatedwith neural model downstream tasks.1 introductionprobabilistic topic model latent dirichlet allocation blei appliedto numerous task include document modelingand information retrieval recently srivastava andsutton miao apply theautoencoding variational bayes aevb kingmaand welling rezende framework basic topic model aevbimproves data scalability conventional models.the limitation basic topic model thatthey induce topic flat structure organizingthem coherent group hierarchy treestructured topic model griffiths detect latent tree structure topic canovercome limitation model induce atree infinite number node assign ageneric topic root detailed topicsto leaf node figure show example topic induce model characteristic preferable several downstreamtasks document retrieval weninger al.,2012 aspect-based sentiment analysis al.,2013 extractive summarization celikyilmazrootcarrypurchasecover1 qualitymonths zippertime back11 sleeveinside inchprotection nice111 bottomcover topplasticscratches112 colorcover mackeyboard love12 perfectquality pricebought size121 itemreturn receiveamazon money122 pricerecommendbuy perfectlove13 pocketscarry strapshouldercompartment131 biglaptops tabletdescription hp132 booksschool carrybags backfigure topics infer tree-structured topicmodel amazon review laptop fivemost frequent word show manually labeled.and hakkani-tur provide succinct information multiple viewpoint forinstance case document retrieval product review user interested generalopinions cover others moreattention specific topic hardness orcolor cover tree structure navigateusers document desirable granularity.however difficult tree-structuredtopic model neural model downstreamtasks neural model require large amountof data training conventional inference algorithm collapsed gibbs sampling bleiet mean-field approximation wangand blei data scalability issue itis also desirable optimize tree structure fordownstream task jointly update neuralmodel parameter posterior topic model.to overcome challenge propose treestructured neural topic model tsntm isparameterized neural network train aevb prior work apply aevbto flat topic model straightforward toparameterize unbounded ancestral fraternal topic distribution paper provide asolution apply doubly-recurrent neural network drnn alvarez-melis jaakkola,2017 recurrent structure overrespectively ancestor siblings.801experimental result show tsntmachieves competitive performance priorwork blei induce latent topicsand tree structure tsntm scale largerdatasets allows end-to-end training withneural model several task aspect-basedsentiment analysis esmaeili abstractive summarization wang related worksfollowing pioneering work tree-structuredtopic model griffiths several extend model propose ghahramaniet zavitsanos al.,2012 ahmed paisley model base modeling assumptionof wang blei blei whileparameterizing topic distribution aevb.in context apply aevb flat document topic modeling miao srivastava sutton ding miaoet propose model closelyrelated apply recurrent neural network parameterize unbounded flattopic distribution work infer topic distribution infinite tree drnn whichenables induce latent tree structures.goyal tree-structured topicmodel wang blei variationalautoencoder represent video frame atree however approach limit smallerdatasets fact video correspond document training separatelyupdated parameter posterior ofthe topic model mean-field approximation thismotivates propose tsntm scalesto large datasets allows end-to-end trainingwith neural model downstream tasks.3 tree-structured neural topic modelwe present generative process documentsand posterior inference model shownin figure draw path root leafnode level word word drawnfrom multinomial distribution assign thetopic specify path level.1 document index draw gaussian vector xd∼n obtain path distribution obtain level distribution β1β11 β12β111 β112 β121cd,1 cd,2 cd,4cd,3zd,1zd,3zd,2zd,4sampling pathsampling levelwd,1wd,3wd,2wd,4figure sampling process topic word.2 word index draw path mult draw level mult draw word mult ∆v−1 word distributionassigned topic wang andblei blei draw path eachdocument constrain document generate topic path hence wedraw path word enable document tobe generate topic tree.wang blei draw path leveldistribution tree-based stick-breaking construction give νk∼beta πk=πpar νkk−1∏j=1 ηl∼beta θl=ηll−1∏j=1 denote k-thtopic parent respectively denote l-th level appendix moredetails.in contrast introduce neural architecture transform gaussian sample atopic distribution allow posterior inferencewith aevb specifically apply drnn toparameterize path distribution tree.3.1 parameterizing topic distributiona drnn neural network decoder generate tree-structured object encode representation alvarez-melis jaakkola adrnn consist rnns respectively theancestors sibling appendix weassume recurrent structure parameterize unbounded ancestral fraternalpath distribution condition gaussian samplex finite number parameters.802the hidden state topic give tanh wphpar +wshk−1 hpar hk−1 hidden state parent previous sibling k-th topic respectively alternate breaking proportion obtain path distribution sigmoid moreover parameterize unbounded leveldistribution pass gaussian vectorthrough alternate breaking proportion tanh whl−1 sigmoid parameterizing word distributionnext explain word distribution assign toeach topic1 introduce embeddings thek-th topic word rv×h toobtain word distribution ∆v−1 softmax kτ1l temperature value produce moresparse probability distribution word thelevel deep hinton number topic unbounded theword distribution must generate dynamically.hence introduce another drnn generatetopic embeddings drnn tpar tk−1 neural topic model miao introduceddiversity regularizer eliminate redundancy thetopics force topic orthogonal suitable tree-structured topic model admit correlation parent andits child hence introduce tree-specificdiversity regularizer t̄ki ∑k/∈leaf∑i j∈chi t̄kj‖t̄ki‖‖t̄kj‖− leaf denote topic child child k-thtopic respectively regularizer thevariational objective child topic become orthogonal viewpoint parent whileallowing parent–children correlations.1βk draw another distribution weset model parameter follow miao variational inference aevbunder propose probabilistic model likelihood document give wd|µ0 ∏n∑cn wn|βcn cn|π zn|θ θ|µ0 dπdθ=∫π θ|µ0 dπdθ ∆k−1 topic distribution isderived =∑ll=1 cl=kπc latent variable integratedout evidence bound document log-likelihood derive θ|wd ∑nlog θ|wd θ|µ0 θ|wd variational distribution approximate posteriors.following aevb framework introducemulti-layer perceptrons fortransforming bag-of-words vectorwd variational gaussian distribution variational distribution posterior re-written θ|wd x|fµ sample θ|wd sampling̂ compute ·fσ2 prior θ|µ0 also rewrite x|µ0 evidence bound approximate sampled topic distribution ld≈∑nlog wn−kl x|fµ x|µ0 dynamically updating tree structureto allow unbounded tree structure introduce heuristic rule prune thebranches compute proportion wordsin topic ∑dd=1nd ∑dd=1nd foreach non-leaf topic threshold child refine topic topic cumulative proportion topic descendant ∑j∈des less threshold thek-th topic descendant remove denote topic descendant wealso remove topic child bottom.8034 experiments4.1 datasetsin experiment andthe amazon product review collection different news group containing11 training test documents2 forthe amazon product review domainof laptop bags provide angelidis lapata training validation and416 test documents3 provided testdocuments evaluation randomly split remainder document trainingand validation sets.4.2 baseline methodsas baseline tree-structured topic modelbased nested chinese restaurant process ncrp collapsed gibbs sampling blei al.,2010 addition flat neural topic model recurrent stick-breaking process construct unbounded flat topic distribution miao implementation detailsfor tsntm embeddings one-hidden-layer with256 hide unit one-layer unit construct variational parameters.we hyper-parameters gaussian prior distribution zero mean vector aunit variance vector dimension respectively train model adagrad duchiet learn rate initialaccumulator value batch size grow prune tree threshold section temperature insection ncrp-based model thencrp parameter parameteras dirichlet parameteras hyperparameters model tunedbased perplexity validation theamazon product review number oflevels tree initial number ofbranches second third levels.2for direct comparison miao training/testing split vocabularyprovided http //github.com/akashgit/autoencoding_vi_for_topic_models.3https //github.com/stangelid/oposum4the code reproduce result available http //github.com/misonuma/tsntm.npmi amazonrsb miao blei model average npmi induced topics.perplexity amazonrsb miao blei model average perplexity model.4.4 evaluating topic interpretabilityseveral work chang newman al.,2010 point perplexity suitable forevaluating topic interpretability meanwhile lauet show normalized pointwisemutual information npmi pair ofwords topic closely correspond ranking topic interpretability human annotators.thus npmi instead perplexity theprimary evaluation measure follow srivastavaand sutton ding show average npmi topicsinduced model model competitivewith ncrp-based model eachdataset indicate model induceinterpretable topic similar models.as note also show average perplexityover document model table forthe aevb-based model tsntm wecalculate upper bound perplexity usingelbo follow miao srivastavaand sutton contrast estimate bysampling posterior ncrp-based modelwith collapse gibbs sampling.even though difficult compare directly perplexity ncrp-based model islower aevb-based model thistendency correspond result srivastavaand sutton ding report model collapsed gibbs samplingachieves perplexity comparison withthe aevb-based model addition ding also report trade-off betweenperplexity npmi therefore natural thatour model competitive modelsregarding npmi significant difference achieved perplexity.8041 amazon product review tsntm ncrpfigure topic specialization score level.tsntm ncrp0.00.20.40.60.8hierarchical affinity ncrp0.00.20.40.60.8 amazon product review child non-childfigure hierarchical affinity scores.4.5 evaluating tree-structurefor evaluate characteristic tree structure adopt metric topic specialization andhierarchical affinity follow specialization important characteristic tree-structure generaltopic assign root topic become specific toward leaf quantifythis characteristic measure specializationscore cosine similarity word distribution topic entire corpus asthe entire corpus regard generaltopic specific topic similarityscores figure present average topic specialization score level root thencrp general model thetendency roughly similar models.hierarchical affinity preferable aparent topic similar child thanthe topic descend parent toverify property parent secondlevel calculate average cosine similarity ofthe word distribution child non-childrenrespectively figure show average cosinesimilarity topic ncrp-basedmodel induces child topic slightly similar theirparents model infers child topic moresimilarity parent topic moreover lowerscores tsntm also indicate inducesmore diverse topic ncrp-based model.example section example induced topic latent tree laptop bagreviews show figure evaluating data scalabilityto evaluate model scale size ofthe datasets measure training time theconvergence various number documents.0 documents01,0002,0003,0004,0005,000time amazon product reviewstsntmncrpfigure training time various number docs.we randomly sample several number document training amazon product reviewsand measure training time number ofdocuments training stop perplexity validation improve iteration entire batch wemeasure time sample posterior update model parameter except time tocompute perplexity show figure number document increase training time modeldoes change considerably whereas ofthe ncrp increase significantly model canbe train approximately time faster thencrp-based model documents.5 conclusionwe propose novel tree-structured topic model tsntm parameterizes topic distribution infinite tree drnn.experimental result demonstrate thetsntm achieves competitive performance wheninducing latent topic tree structure ascompared prior tree-structured topic model blei help aevb thetsntm train approximately timesfaster scale large datasets ncrpbased model.this allow tree-structured topic model beincorporated recent neural model downstream task aspect-based sentiment analysis esmaeili abstractive summarization wang incorporate ourmodel instead flat topic model providemultiple information desirable granularity.acknowledgmentswe would like thank anonymous reviewer fortheir valuable feedback work supportedby act-x grant number jpmjax1904 andcrest grant number jpmjcr1513 japan.5all computational time measure machine xeon e5-2683-v4 core anda single geforce gpu.805
focus task frequently askedquestions retrieval give user querycan match question and/orthe answer present fully unsupervised method exploit pairsto train bert model modelsmatch user query answer question respectively alleviate missinglabeled data latter automatically generate high-quality question paraphrase weshow model even outperforms supervise model exist datasets.1 introductionmany website online community publishfaq help user find relevant answer tocommon question consist pair ofquestions answer retrievaltask involve rank pair give userquery searching leverage multifield indexing retrieval karan šnajder,2016 hence user query matchedwith either question field answer field concatenated field karan šnajder,2016 association question answer thefaq pair utilize weak supervision train neural model predict similarity user query answer i.e. q-to-amatching gupta carvalho karan andšnajder sakata however faqpairs provide requiredlabeled data train model predict association user query question i.e. q-to-q matching thus label datasetwith user query matching paper term question todenote question within give pair query denote issue user query.pairs require supervised learning gupta andcarvalho karan šnajder sakataet dataset usually manuallygenerated obtain query-log mining construction dataset either requiresdomain expertise e.g. enrich dataset withmanually generate question paraphrase karanand šnajder assume availability ofquery-logs dataset unavailable mustresort utilize unsupervised retrieval modelsfor q-to-q matching previous unsupervised faqretrieval model burke brill al.,2002 karan karan šnajder utilize traditional information retrieval technique lexicaland semantic text matching query expansion etc.in paper overcome aforementionedunsupervised distant supervision totrain neural model method composedof combination three unsupervised methods.each method utilize re-ranking initialpool pair obtain simple bm25 retrieval robertson zaragoza firstmethod apply focused-retrieval approach utilizing passage answer re-ranking bendersky andkurland methodsfine-tunes bert model devlin onefor match q-to-a match q-to-q.to overcome lack train data thelatter case implement novel weaksupervision approach automatically generate question paraphrase couple smart filter ensure high-quality paraphrase thencombine outcome three method usingan unsupervised late-fusion method overall weshow unsupervised retrieval approachis sometimes even outperform state-ofthe-art supervise models.8082 related workseveral previous work also utilized deepneural networks retrieval karanand šnajder convolution neural networks match user query gupta carvalho combination oflong short-term memory lstm capture qto-q q-to-a similarity work aresupervised user query training.following success bert devlin al.,2019 task sakata recently search engine q-to-q matchingand combine result supervisedbert model q-to-a matching similar bert model q-to-a matching differently sakata unsupervised introduce secondunsupervised bert model q-to-q matching.a somewhat related area research community question answering patra zhou related trec tracks.23while share common feature faqretrieval additional signal suchas vote question answer association user-answer user-question clearly pure retrieval auxiliary datais unavailable hence refrain comparingwith works.3 unsupervised retrieval approachour propose retrieval approach distantsupervision train neural model basedon initial candidate retrieval follow reranking step.recall dataset compose pair initial candidate retrieval isbased index pair search engine index section search theindex re-ranking step combine three unsupervised re-rankers first section base focused-retrieval approach utilizingpassages answer re-scoring rerankers fine-tune independent bert models.the first bert model section inspiredby sakata fine-tuned matchquestions answer time give auser query model re-ranks top-k candidate pair match user query tothe answer only.2http //alt.qcri.org/semeval2016/task3/3http //alt.qcri.org/semeval2017/task3/the second bert model section design match user query questions.here utilize weak-supervision generatinghigh quality question paraphrase faqpairs bert model fine-tuned question generated paraphrase time give user query model topk candidate pair re-ranks bymatching user query question only.the final re-ranking obtain combiningthe three re-rankers unsupervised latefusion step section component ourmethod describe rest section.3.1 indexing initial candidate retrievalwe index pair elasticsearch4search engine represent faqpair multifield document threemain field namely question answer theconcatenated field given user query wematch bm25 similarity robertson andzaragoza field5 retrievean initial pool top-k candidates.3.2 passage-based re-rankingour first unsupervised re-ranker applies focusedretrieval approach follow bendersky kurland re-rank candidatesusing maximum-passage approach approach simply implement slidingwindow i.e. passage candidate q+afield text score candidate accord tothe passage high bm25 similarity largeron hereinafter termthis first re-ranking method bm25-maxpsg.3.3 bert model q-to-a similarityamong bert devlin rerankers first bert-q-a reranking candidate pair accordingto similarity give user query andeach pair answer a.to fine-tune bert modelfrom pair triplet network hoffer ailon network isadopted bert fine-tuning mass triplet constitute anfaq pair negative sample answer as4https //www.elastic.co/5searching field obtain inferior results809follows question positive answer pair negativeexamples randomly select faqthat question furtherchallenge model learn small nuancesbetween close answer instead sample thenegative example pair qagainst field search index section sample among thetop-k e.g. retrieve pair nothave question.our bert-q-a different sakataet aspect first sakata al.,2019 fine tune bert model q-to-a match pair well userqueries matched answer therefore supervised setting since user queriesare part thus require label effort compared fine tune thebert-q-a pair second unlike sakata fine-tune bertfor classification task i.e. point-wise training train triplet network hoffer ailon learn relative preference question pair answer network thus implement pair-wise learning-to-rank approach li,2011 inference time give user query thetop-k retrieved pair re-rank pair score pair assignedby fine-tuned bert-q-a model mass al.,2019 bert model q-to-q similaritythe second bert model bert-q-q independent first bert-q-a model section train match user query tofaq question fine-tune model generate weakly-supervised dataset faqpairs inspired anaby-tavor wefine-tune generative pre-training gpt-2 neural network model radford generate question paraphrase gpt-2 pre-trained onhuge body text capture natural languagestructure produce deeply coherent text paragraphs.intuitively would like answer generate paraphrase question unlikethe work anaby-tavor fine6usually i.e. single answer faqquestion possible gpt-2 model give class eachclass title several example consider answer class example question q.we thus concatenate pair longtext whereanswers precede questions,7 andsep special token former separate pair latter separate answersfrom question inside pairs.the gpt-2 fine-tuning sample sequenceof consecutive token wj−l fromu maximize conditional probability |wj−l wj−1 appear next inthe sequence repeat process several times.once model fine-tuned withthe text answer pair generate token wetake generate token paraphraseto question repeat generationprocess generate number questionparaphrases example paraphrase therea deactivate account facebook wasgenerated question delete myfacebook account obstacle generated text noiseit introduce overcome problem weapply filtering step follow idea keeponly paraphrase semantically similar totheir original question i.e. similar answer pair questionq i.e. ground truth answer eachgenerated paraphrase queryagainst index section checkthat among return top-k result atleast pair soman experiment section weused k=10 n=2.to select best paraphrase questionq sort paraphrase pass theabove filter score top-1 return pair paraphrase aquery index motivation isthat high score return queryp imply high similarity bert-q-a model finetuned triplet paraphrase randomly select question7faq question answer treatedhere different questions.8the filter paraphrase download fromhttps //github.com/yosimass/faq-retrieval810from question inference time givena user query top-k retrieved pair re-rank answer answer thescore pair assign finetuned bert-q-q model mass re-rankers combinationwe combine three re-ranking method i.e. bm25-maxpsg fined-tuned bertmodels alternative late-fusion methods.the first combsum kurland culpepper,2018 calculate combine score foreach candidate pair score assignedto three re-ranking methods.9following roitman second alternative implement poolrank method.poolrank first rank candidate pair usingcombsum pair introduce unsupervised query expansion step rm1model lavrenko croft usedto re-rank whole candidate pool experiments4.1 datasetswe datasets evaluation namely faqir karan šnajder stackfaq karan šnajder faqirdataset derive maintenance repair domain yahoo answers communityqa website consist pairsand user query stackfaq dataset wasderived apps domain stackexchange website consist faqpairs result thread questionshave answer user queries.4.2 baselineson datasets compare result various method evaluatedin karan šnajder namely anensemble three unsupervised method bm25 vector-space word-embeddings listnetand lambdamart supervise learningto-rank method train diverseset text similarity feature cnn-rank a9each re-ranker score first max-min normalized.10further follow roitman normalized combsum fusion score weak-relevance label forthe model estimation.11http //takelab.fer.hr/data/faqir/12http //takelab.fer.hr/data/stackfaq supervise learning-to-rank approach base aconvolutional neural network stackfaq dataset report theresult sakata serve asthe strong supervised baseline baselinecombines method tsubaki shinzato al.,2008 search engine q-to-q matching anda supervise fine-tuned bert model q-to-amatching result work thatwere available stackfaq dataset justto emphasize approach reach quality supervised approach directlycompare it.4.3 experimental setupwe elasticsearch index pairs.for first ranker section slide window size character overlap fine-tuning bert-q-a model werandomly sample negative example foreach positive example faqir stackfaq datasets respectively.to fine-tune gpt-2 generate questionparaphrases section segment intoconsecutive sequence token each.we openai medium-sized gpt-2 englishmodel fine-tuned modelto generate paraphrase question qand select top-10 pass filtering describe section overall faqir pass filter enrich outof question stackfaq paraphrase pass filter enrich the125 thread question similar bert-q-afine-tuning select negative example paraphrase-question pair onfaqir stackfaq respectively.the bert model pre-trainedbert-base-uncased model parameter fine-tuning wasdone learn rate trainingepochs similar previous work thefollowing metric mean average precision mean reciprocal rank calculate initial candidate list faqs retrieve search engine standard bm25.8114.4 resultstable report result twodatasets.13 compare base bm25 retrieval bm25 three propose unsupervisedre-ranking method bm25-maxpsg bert-q-aand bert-q-q fusion-based combination combsum poolrank thestate-of-the-art unsupervised supervisedbaselines also compare poolrank+ poolrank except twobert model i.e. bert-q-a bert-q-q fine-tune union respectivetraining faqir stackfaqdatasets.we observe among three re-rankers bert-q-q best example faqirit achieve mapand respectively comparison to0.54 obtain bm25-maxpsgfor respectively confirm previous finding karan šnajder q-to-q match give best signal faqretrieval furthermore datasets fusion method achieve well result individual re-rankers good performance thepoolrank variant combosum.an exception faqir bert-q-qachieved result combosum fusion.as mention bert-q-q significantly good performance faqir othertwo individual ranker thus simple fusion methodsuch combsum handle case well.poolrank relevance model betterapproach thus give good fusion results.further compare baseline cansee faqir unsupervised poolrankoutperformed method include supervised method three metric stackfaq poolrank outperform method except supervised tsubaki+bert sakataet note unsupervised result poolrank+ achieve respectively isquite close supervise result and0.94 respectively sakata karan šnajder faqir initialretrieval subset pair arerelevant least user query.faqir mrrbm25 mrrbm25 evaluation results5 summary conclusionswe present fully unsupervised method faqretrieval method base initial retrieval candidate follow three rerankers first base passageretrieval approach others independent bert model fine-tuned predict query-to-answer query-to-question matching show overcome unsupervised generate high-quality questionparaphrases fine-tune query-toquestion bert model experimentally showedthat unsupervised method sometimes even outperform exist supervise method
